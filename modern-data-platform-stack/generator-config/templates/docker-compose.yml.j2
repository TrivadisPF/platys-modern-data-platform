# =======================================================================
# Platform Name            {{platys['platform-name']}}
# Platform Stack:          {{platys['platform-stack']}}
# Platform Stack Version:  {{platys['platform-stack-version']}}
# =======================================================================
networks:
  default:
    name: {{platys['platform-name']}}

{% if data_center_to_use > 0 and data_centers is defined and data_centers %}
  {% set dataCenter = '-' ~ data_centers.split(',')[data_center_to_use-1] -%}
  {% set dataCenterId = data_center_to_use - 1 %}
{% else %}
  {% set dataCenter = '' -%}
  {% set dataCenterId = 0 %}
{% endif %}

{% set __PROVISIONING_DATA_version = PROVISIONING_DATA_version | default('latest') -%}

{% set __CONFLUENT_PLATFORM_version = CONFLUENT_PLATFORM_version | default('latest') -%}
{% set __KAFKA_version = KAFKA_version | default('latest') -%}
{% set __KAFKA_INIT_version = KAFKA_INIT_version | default('latest') -%}
{% set __KAFKA_MM2_version = KAFKA_MM2_version | default('latest') -%}
{% set __APICURIO_SCHEMA_REGISTRY_version = APICURIO_SCHEMA_REGISTRY_version | default('latest') -%}
{% set __ZOONAVIGATOR_version = ZOONAVIGATOR_version | default('latest') -%}
{% set __ZOONAVIGATOR_API_version = ZOONAVIGATOR_API_version | default('latest') -%}
{% set __KSQLDB_version = KSQLDB_version | default('latest') -%}
{% set __MATERIALIZE_version = MATERIALIZE_version | default('latest') -%}
{% set __HSTREAMDB_version = HSTREAMDB_version | default('latest') -%}
{% set __BENTHOS_version = BENTHOS_version | default('latest') -%}
{% set __RISINGWAVE_version = RISINGWAVE_version | default('latest') -%}
{% set __TIMEPLUS_ENTERPRISE_version = TIMEPLUS_ENTERPRISE_version | default('latest') -%}
{% set __TIMEPLUS_PROTON_version = TIMEPLUS_PROTON_version | default('latest') -%}
{% set __ARROYO_version = ARROYO_version | default('latest') -%}
{% set __SQLFLOW_version = SQLFLOW_version | default('latest') -%}

{% set __SCHEMA_REGISTRY_UI_version = SCHEMA_REGISTRY_UI_version | default('latest') -%}

{% set __LENSES_BOX_version = LENSES_BOX_version | default('latest') -%}

{% set __KCAT_version = KCAT_version | default('latest') -%}

{% set __KASKADE_version = KASKADE_version | default('latest') -%}

{% set __KAFKACTL_version = KAFKACTL_version | default('latest') -%}

{% set __JIKKOU_version = JIKKOU_version | default('latest') -%}
{% set __JIKKOU_SERVER_version = JIKKOU_SERVER_version | default('latest') -%}

{% set __KAFKA_TOPICS_UI_version = KAFKA_TOPICS_UI_version | default('latest') -%}
{% set __KAFKA_CONNECT_UI_version = KAFKA_CONNECT_UI_version | default('latest') -%}
{% set __CMAK_version = CMAK_version | default('latest') -%}
{% set __KAFDROP_version = KAFDROP_version | default('latest') -%}
{% set __KADMIN_version = KADMIN_version | default('latest') -%}
{% set __AKHQ_version = AKHQ_version | default('latest') -%}
{% set __KAFKA_UI_version = KAFKA_UI_version | default('latest') -%}
{% set __EFAK_version = EFAK_version | default('latest') -%}
{% set __KOWL_version = KOWL_version | default('latest') -%}
{% set __REDPANDA_CONSOLE_version = REDPANDA_CONSOLE_version | default('latest') -%}
{% set __KOUNCIL_version = KOUNCIL_version | default('latest') -%}
{% set __KAFKA_MAGIC_version = KAFKA_MAGIC_version | default('latest') -%}
{% set __KAFKA_WEBVIEW_version = KAFKA_WEBVIEW_version | default('latest') -%}
{% set __KPOW_version = KPOW_version | default('latest') -%}
{% set __CONDUKTOR_PLATFORM_version = CONDUKTOR_PLATFORM_version | default('latest') -%}
{% set __KADECK_version = KADECK_version | default('latest') -%}
{% set __KAFKISTRY_version = KAFKISTRY_version | default('latest') -%}
{% set __KLAW_version = KLAW_version | default('latest') -%}
{% set __KAFKA_CONNECTOR_BOARD_version = KAFKA_CONNECTOR_BOARD_version | default('latest') -%}
{% set __STREAMS_EXPLORER_version = STREAMS_EXPLORER_version | default('latest') -%}
{% set __KAFKA_LAG_EXPORTER_version = KAFKA_LAG_EXPORTER_version | default('latest') -%}
{% set __ZILLA_version = ZILLA_version | default('latest') -%}

{% set __REMORA_version = REMORA_version | default('latest') -%}

{% set __BURROW_version = BURROW_version | default('latest') -%}
{% set __BURROW_UI_version = BURROW_UI_version | default('latest') -%}
{% set __BURROW_DASHBOARD_version = BURROW_DASHBOARD_version | default('latest') -%}

{% set __DEBEZIUM_SERVER_version = DEBEZIUM_SERVER_version | default('latest') -%}
{% set __DEBEZIUM_UI_version = DEBEZIUM_UI_version | default('latest') -%}

{% set __HADOOP_image_version = HADOOP_image_version | default('latest') -%}
{% set __HADOOP_version = HADOOP_version | default('latest') -%}
{% set __SQOOP_version = SQOOP_version | default('latest') -%}

{% if (SPARK_base_version | lower) == '3.3'  %}
  {% set __SPARK_version = SPARK_3_3_version -%}
  {% set __LIVY_version = LIVY_version ~ "-spark" ~ SPARK_3_3_version.split(".")[0]  ~ "." ~ SPARK_3_3_version.split(".")[1] -%}
{% elif (SPARK_base_version | lower) == '3.4'  %}
  {% set __SPARK_version = SPARK_3_4_version -%}
  {% set __LIVY_version = LIVY_version ~ "-spark" ~ SPARK_3_4_version.split(".")[0]  ~ "." ~ SPARK_3_4_version.split(".")[1] -%}
{% elif (SPARK_base_version | lower) == '3.5'  %}
  {% set __SPARK_version = SPARK_3_5_version -%}
  {% set __LIVY_version = LIVY_version ~ "-spark" ~ SPARK_3_5_version.split(".")[0]  ~ "." ~ SPARK_3_5_version.split(".")[1] -%}
{% endif -%}

{% set __KYUUBI_version = KYUUBI_version | default('latest') -%}

{% set __ZEPPELIN_version = ZEPPELIN_version | default('latest') -%}

{% set __FLINK_version = FLINK_version | default('latest') -%}
{% set __NUSSKNACKER_version = NUSSKNACKER_version | default('latest') -%}

{% set __TIKA_version = TIKA_version | default('latest') -%}

{% set __NLM_INGESTOR_version = NLM_INGESTOR_version | default('latest') -%}

{% set __UNSTRUCTURED_version = UNSTRUCTURED_version | default('latest') -%}

{% set __SEARXNG_version = SEARXNG_version | default('latest') -%}

{% set __FIRECRAWL_version = FIRECRAWL_version | default('latest') -%}

{% set __CRAWL4AI_version = CRAWL4AI_version | default('latest') -%}

{% set __WHISPER_version = WHISPER_version | default('latest') -%}

{% set __NLLB_version = NLLB_version | default('latest') -%}

{% set __HIVE_version = HIVE_version | default('latest') -%}
{% set __HIVE_version_suffix = HIVE_version_suffix | default('postgresql-metastore-s3') -%}
{% set __HIVE_METASTORE_DB_version = HIVE_METASTORE_DB_version | default('latest') -%}

{% set __AVRO_TOOLS_version = AVRO_TOOLS_version | default('latest') -%}
{% set __PARQUET_TOOLS_version = PARQUET_TOOLS_version | default('latest') -%}

{% set __OPENLDAP_version = OPENLDAP_version | default('latest') -%}
{% set __PHP_LDAP_ADMIN_version = PHP_LDAP_ADMIN_version | default('latest') -%}
{% set __LDAP_USER_MANAGER_version = LDAP_USER_MANAGER_version | default('latest') -%}

{% set __RANGER_version = RANGER_version | default('latest') -%}
{% set __RANGER_postgresql_version = RANGER_postgresql_version | default('latest') -%}

{% set __OPA_version = OPA_version | default('latest') -%}
{% set __STYRA_EOPA_version = STYRA_EOPA_version | default('latest') -%}
{% set __CEDAR_version = CEDAR_version | default('latest') -%}
{% set __OPAL_version = OPAL_version | default('latest') -%}

{% set __ATLAS_version = ATLAS_version | default('latest') -%}

{% set __DATAHUB_version = DATAHUB_version | default('latest') -%}
{% set __DATAHUB_ACTIONS_version = DATAHUB_ACTIONS_version | default('latest') -%}

{% set __OPENMETADATA_version = OPENMETADATA_version | default('latest') -%}

{% set __AMUNDSEN_FRONTEND_version = AMUNDSEN_FRONTEND_version | default('latest') -%}
{% set __AMUNDSEN_SEARCH_version = AMUNDSEN_SEARCH_version | default('latest') -%}
{% set __AMUNDSEN_METADATA_version = AMUNDSEN_METADATA_version | default('latest') -%}

{% set __DATA_PRODUCT_PORTAL_version = DATA_PRODUCT_PORTAL_version | default('latest') -%}

{% set __MARQUEZ_version = MARQUEZ_version | default('latest') -%}
{% set __MARQUEZ_WEB_version = MARQUEZ_WEB_version | default('latest') -%}

{% set __CKAN_version = CKAN_version | default('latest') -%}
{% set __CKAN_DATAPUSHER_version = CKAN_DATAPUSHER_version | default('latest') -%}

{% set __DATAVERSE_version = DATAVERSE_version | default('latest') -%}

{% if (CASSANDRA_major_version | lower) == '3'  %}
  {% set __CASSANDRA_version = CASSANDRA_3_version  -%}
{% elif (CASSANDRA_major_version | lower) == '4'  %}
  {% set __CASSANDRA_version = CASSANDRA_4_version  -%}
{% elif (CASSANDRA_major_version | lower) == '5'  %}
  {% set __CASSANDRA_version = CASSANDRA_5_version  -%}
{% endif -%}

{% set __REAPER_version = REAPER_version | default('latest') -%}

{% set __DATASTAX_version = DATASTAX_version | default('latest') -%}
{% set __DATASTAX_OPSCENTER_version = DATASTAX_OPSCENTER_version | default('latest') -%}

{% if (ELASTICSEARCH_major_version | lower) == '7'  %}
  {% set __ELASTICSEARCH_version = ELASTICSEARCH_7_version  -%}
  {% set __KIBANA_version = KIBANA_7_version | default('latest') -%}
{% elif (ELASTICSEARCH_major_version | lower) == '8'  %}
  {% set __ELASTICSEARCH_version = ELASTICSEARCH_8_version  -%}
  {% set __KIBANA_version = KIBANA_8_version | default('latest') -%}
{% endif -%}

{% set __DEJAVU_version = DEJAVU_version | default('latest') -%}
{% set __CEREBRO_version = CEREBRO_version | default('latest') -%}
{% set __ELASTICSEARCHHQ_version = ELASTICSEARCHHQ_version | default('latest') -%}
{% set __ELASTICVUE_version = ELASTICVUE_version | default('latest') -%}

{% set __OPENSEARCH_version = OPENSEARCH_version | default('latest') -%}
{% set __OPENSEARCH_DASHBOARDS_version = OPENSEARCH_DASHBOARDS_version | default('latest') -%}

{% set __SPLUNK_version = SPLUNK_version | default('latest') -%}

{% set __HUE_version = HUE_version | default('latest') -%}
{% set __HUE_POSTGRES_version = HUE_POSTGRES_version | default('latest') -%}

{% set __STREAMSETS_version = STREAMSETS_version | default('latest') -%}
{% set __STREAMSETS_TRANSFORMER_version = STREAMSETS_TRANSFORMER_version | default('latest') -%}
{% set __STREAMSETS_EDGE_version = STREAMSETS_EDGE_version | default('3.12.0') -%}
{% set __STREAMSETS_DATAOPS_version = STREAMSETS_DATAOPS_version | default('latest') -%}

{% set __NIFI_version = NIFI_version -%}
{% set __NIFI2_version = NIFI2_version -%}

{% if (NIFI_REGISTRY_major_version | lower) == '1'  %}
  {% set __NIFI_REGISTRY_version = NIFI_REGISTRY_version -%}
{% elif (NIFI_REGISTRY_major_version | lower) == '2'  %}
  {% set __NIFI_REGISTRY_version = NIFI_REGISTRY2_version -%}
{% endif -%}
{% if (NIFI_TOOLKIT_major_version | lower) == '1'  %}
  {% set __NIFI_TOOLKIT_version = NIFI_TOOLKIT_version -%}
{% elif (NIFI_TOOLKIT_major_version | lower) == '2'  %}
  {% set __NIFI_TOOLKIT_version = NIFI_TOOLKIT2_version -%}
{% endif -%}
{% set __MONITOFI_version = MONITOFI_version | default('latest') -%}

{% set __N8N_version = N8N_version | default('latest') -%}

{% set __STREAMPIPES_version = STREAMPIPES_version | default('latest') -%}

{% set __CRIBL_version = CRIBL_version | default('latest') -%}

{% set __FLUENTD_version = FLUENTD_version | default('latest') -%}
{% set __FLUENT_BIT_version = FLUENT_BIT_version | default('latest') -%}
{% set __FILEBEAT_version = FILEBEAT_version | default('latest') -%}

{% set __CONDUIT_version = CONDUIT_version | default('latest') -%}

{% set __NODERED_version = NODERED_version | default('latest') -%}

{% set __STREAMSHEETS_version = STREAMSHEETS_version | default('latest') -%}

{% set __SPRING_DATAFLOW_version = SPRING_DATAFLOW_version | default('latest') -%}
{% set __SPRING_SKIPPER_version = SPRING_SKIPPER_version | default('latest') -%}

{% set __ASPIRE_version = ASPIRE_version | default('latest') -%}

{% set __AIRBYTE_version = AIRBYTE_version | default('latest') -%}

{% if AIRFLOW_python_version and AIRFLOW_python_version is defined %}
  {% set __AIRFLOW_version = ("slim-" if AIRFLOW_use_slim_image else "") ~ AIRFLOW_version ~ "-python" ~ AIRFLOW_python_version | default('latest') -%}
{% else -%}
  {% set __AIRFLOW_version = ("slim-" if AIRFLOW_use_slim_image else "") ~ AIRFLOW_version  | default('latest') -%}
{% endif -%}

{% set __MAGE_AI_version = MAGE_AI_version | default('latest') -%}

{% set __KESTRA_version = KESTRA_version | default('latest') -%}

{% set __OFELIA_version = OFELIA_version | default('latest') -%}

{%if (JUPYTER_edition | lower) == 'minimal' | default(false) %}
  {% set __JUPYTER_version = JUPYTER_MINIMAL_version | default('latest') -%}
{%elif (JUPYTER_edition | lower) == 'r' | default(false) %}
  {% set __JUPYTER_version = JUPYTER_R_version | default('latest') -%}
{%elif (JUPYTER_edition | lower) == 'scipy' | default(false) %}
  {% set __JUPYTER_version = JUPYTER_SCIPY_version | default('latest') -%}
{%elif (JUPYTER_edition | lower) == 'datascience' | default(false) %}
  {% set __JUPYTER_version = JUPYTER_DATASCIENCE_version | default('latest') -%}
{%elif (JUPYTER_edition | lower) == 'tensorflow' | default(false) %}
  {% set __JUPYTER_version = JUPYTER_TENSORFLOW_version | default('latest') -%}
{%elif (JUPYTER_edition | lower) == 'pyspark' or (JUPYTER_edition | lower) == 'all-spark' | default(false) %}
  {% if (SPARK_base_version | lower) == '3.3'  %}
    {% set __JUPYTER_version = JUPYTER_SPARK_3_3_version | default('latest') -%}
  {% elif (SPARK_base_version | lower) == '3.4'  %}
    {% set __JUPYTER_version = JUPYTER_SPARK_3_4_version | default('latest') -%}
  {% elif (SPARK_base_version | lower) == '3.5'  %}
    {% set __JUPYTER_version = JUPYTER_SPARK_3_5_version | default('latest') -%}
  {% else -%}
    {% set __JUPYTER_version = JUPYTER_SPARK_3_3_version | default('latest') -%}
  {% endif -%}
{% endif -%}

{% set __JUPYTERHUB_version = JUPYTERHUB_version | default('latest') -%}

{% set __ANACONDA_version = ANACONDA_version | default('latest') -%}

{% set __RSTUDIO_version = RSTUDIO_version | default('latest') -%}
{% set __SHINY_SERVER_version = SHINY_SERVER_version | default('latest') -%}

{% set __DATAIKU_DSS_version = DATAIKU_DSS_version | default('latest') -%}

{% set __MLFLOW_SERVER_version = MLFLOW_SERVER_version | default('latest') -%}

{% set __OPTUNA_version = OPTUNA_version | default('latest') -%}
{% set __OPTUNA_DASHBOARD_version = OPTUNA_DASHBOARD_version | default('latest') -%}

{% set __MINDSDB_version = MINDSDB_version | default('latest') -%}

{% set __OLLAMA_version = OLLAMA_version | default('latest') -%}

{% set __LOCAL_AI_version = LOCAL_AI_version | default('latest') -%}

{% set __OPEN_WEBUI_version = OPEN_WEBUI_version | default('latest') -%}

{% set __ALPACA_WEBUI_version = ALPACA_WEBUI_version | default('latest') -%}

{% set __ANYTHING_LLM_version = ANYTHING_LLM_version | default('latest') -%}

{% set __PERPLEXICA_version = PERPLEXICA_version | default('latest') -%}

{% set __BIG_AGI_version = BIG_AGI_version | default('latest') -%}

{% set __AUTOGEN_STUDIO_version = AUTOGEN_STUDIO_version | default('latest') -%}

{% set __CREWAI_STUDIO_version = CREWAI_STUDIO_version | default('latest') -%}

{% set __AGENT_ZERO_version = AGENT_ZERO_STUDIO_version | default('latest') -%}

{% set __LITELLM_version = LITELLM_version | default('latest') -%}

{% set __FLOWISE_version = FLOWISE_version | default('latest') -%}
{% set __LANGFLOW_version = LANGFLOW_version | default('latest') -%}
{% set __LANGFUSE_version = LANGFUSE_version | default('latest') -%}
{% set __OPENLIT_version = OPENLIT_version | default('latest') -%}
{% set __LANGWATCH_version = LANGWATCH_version | default('latest') -%}
{% set __LANGEVALS_version = LANGEVALS_version | default('latest') -%}
{% set __ARIZE_PHOENIX_version = ARIZE_PHOENIX_version | default('latest') -%}

{% set __GPT_RESEARCHER_version = GPT_RESEARCHER_version | default('latest') -%}

{% set __LOCAL_DEEP_RESEARCHER_version = LOCAL_DEEP_RESEARCHER_version | default('latest') -%}

{% set __VERBA_version = VERBA_version | default('latest') -%}

{% set __RAGFLOW_version = RAGFLOW_version | default('latest') -%}

{% set __NEO4j_KG_BUILDER_version = NEO4j_KG_BUILDER_version | default('latest') -%}

{% set __KIE_SERVER_version = KIE_SERVER_version | default('latest') -%}

{% set __PROMTAIL_version = PROMTAIL_version | default('latest') -%}
{% set __LOKI_version = LOKI_version | default('latest') -%}
{% set __TEMPO_version = TEMPO_version | default('latest') -%}

{% set __GRAFANA_version = GRAFANA_version | default('latest') -%}

{% set __METABASE_version = METABASE_version | default('latest') -%}
{% set __SUPERSET_version = SUPERSET_version | default('latest') -%}
{% set __REDASH_version = REDASH_version | default('latest') -%}
{% set __SMASHING_version = SMASHING_version | default('latest') -%}
{% set __TIPBOARD_version = TIPBOARD_version | default('latest') -%}
{% set __CHARTBOARD_version = CHARTBOARD_version | default('latest') -%}

{% set __RETOOL_version = RETOOL_version | default('latest') -%}
{% set __TOOLJET_version = TOOLJET_version | default('latest') -%}
{% set __STREAMLIT_python_version = STREAMLIT_python_version | default('latest') -%}
{% set __BASEROW_version = BASEROW_version | default('latest') -%}

{% set __MEMCACHED_version = MEMCACHED_version | default('latest') -%}

{% set __REDIS_version = REDIS_version | default('latest') -%}
{% set __REDIS_STACK_version = REDIS_STACK_version | default('latest') -%}
{% set __REDIS_EXPORTER_version = REDIS_EXPORTER_version | default('latest') -%}
{% set __REDIS_INSIGHT_version = REDIS_INSIGHT_version | default('latest') -%}
{% set __REDIS_COMMANDER_version = REDIS_COMMANDER_version | default('latest') -%}

{% set __VALKEY_version = VALKEY_version | default('latest') -%}

{% set __MONGO_version = MONGO_version | default('latest') -%}
{% set __MONGO_EXPRESS_version = MONGO_EXPRESS_version | default('latest') -%}
{% set __MONGO_ADMIN_version = MONGO_ADMIN_version | default('latest') -%}

{% set __SOLR_version = SOLR_version | default('latest') -%}

{% set __JANUSGRAPH_version = JANUSGRAPH_version | default('latest') -%}

{% set __INVANA_ENGINE_version = INVANA_ENGINE_version | default('latest') -%}
{% set __INVANA_STUDIO_version = INVANA_STUDIO_version | default('latest') -%}

{% if (NEO4J_major_version | lower) == '4'  %}
  {% set __NEO4J_version = NEO4J_4_version -%}
{% elif (NEO4J_major_version | lower) == '5'  %}
  {% set __NEO4J_version = NEO4J_5_version -%}
{% endif -%}
{% set __NEODASH_version = NEODASH_version | default('latest') -%}

{% set __QUINE_version = QUINE_version | default('latest') -%}

{% set __MEMGRAPH_version = MEMGRAPH_version | default('latest') -%}
{% set __MEMGRAPH_MAGE_version = MEMGRAPH_MAGE_version | default('latest') -%}
{% set __MEMGRAPH_PLATFORM_version = MEMGRAPH_PLATFORM_version | default('latest') -%}

{% set __ARCADEDB_version = ARCADEDB_version | default('latest') -%}

{% set __DGRAPH_version = DGRAPH_version | default('latest') -%}

{% set __STARDOG_version = STARDOG_version | default('latest') -%}
{% set __STARDOG_STUDIO_version = STARDOG_STUDIO_version | default('latest') -%}

{% set __GRAPHDB_FREE_version = GRAPHDB_FREE_version | default('latest') -%}
{% set __GRAPHDB_version = GRAPHDB_version | default('latest') -%}

{% set __JENA_FUSEKI_version = JENA_FUSEKI_version | default('latest') -%}

{% set __INFLUXDB_version = INFLUXDB_version | default('latest') -%}
{% set __TELEGRAF_version = TELEGRAF_version | default('latest') -%}
{% set __CHRONOGRAF_version = CHRONOGRAF_version | default('latest') -%}
{% set __KAPACITOR_version = KAPACITOR_version | default('latest') -%}
{% set __INFLUXDB2_version = INFLUXDB2_version | default('2.2.0-alpha') -%}
{% set __INFLUXDB_UI_version = INFLUXDB_UI_version | default('latest') -%}

{% set __QUESTDB_version = QUESTDB_version | default('latest') -%}

{% set __CHROMA_version = CHROMA_version | default('latest') -%}

{% set __QDRANT_version = QDRANT_version | default('latest') -%}

{% set __WEAVIATE_version = WEAVIATE_version | default('latest') -%}

{% set __MILVUS_version = MILVUS_version | default('latest') -%}
{% set __ATTU_version = ATTU_version | default('latest') -%}

{% set __INFINITY_version = INFINITY_version | default('latest') -%}

{% set __VECTOR_ADMIN_version = VECTOR_ADMIN_version | default('latest') -%}

{% set __KUDU_version = KUDU_version | default('latest') -%}

{% set __DUCKDB_version = DUCKDB_version | default('latest') -%}
{% set __QUACKFLIGHT_version = QUACKFLIGHT_version | default('latest') -%}

{% set __DRUID_version = DRUID_version | default('latest') -%}

{% set __PINOT_version = PINOT_version | default('latest') -%}

{% set __STARROCKS_version = STARROCKS_version | default('latest') -%}

{% set __CLICKHOUSE_version = CLICKHOUSE_version | default('latest') -%}
{% set __CLICKHOUSE_UI_version = CLICKHOUSE_UI_version | default('latest') -%}

{% set __IGNITE_version = IGNITE_version | default('latest') -%}
{% set __GRIDGAIN_CC_version = GRIDGAIN_CC_version | default('latest') -%}

{% set __PROMETHEUS_version = PROMETHEUS_version | default('latest') -%}
{% set __PROMETHEUS_PUSHGATEWAY_version = PROMETHEUS_PUSHGATEWAY_version | default('latest') -%}
{% set __PROMETHEUS_NODEEXPORTER_version = PROMETHEUS_NODEEXPORTER_version | default('latest') -%}
{% set __PROMETHEUS_ALERTMANAGER_version = PROMETHEUS_ALERTMANAGER_version | default('latest') -%}

{% set __TILE38_version = TILE38_version | default('latest') -%}

{% set __ORACLE_XE_version = ORACLE_XE_version | default('latest') -%}
{% set __ORACLE_FREE_version = ORACLE_FREE_version | default('latest') -%}
{% set __ORACLE_OCI_FREE_version = ORACLE_OCI_FREE_version | default('latest') -%}
{% set __ORACLE_ADB_FREE_version = ORACLE_ADB_FREE_version | default('latest') -%}
{% set __ORACLE_EE_version = ORACLE_EE_version | default('latest') -%}
{% set __ORACLE_SQLCL_version = ORACLE_SQLCL_version | default('latest') -%}
{% set __ORACLE_REST_DATA_SERVICE_version = ORACLE_REST_DATA_SERVICE_version | default('latest') -%}

{% set __MYSQL_version = MYSQL_version | default('latest') -%}
{% set __MARIADB_version = MARIADB_version | default('latest') -%}
{% set __POSTGRESQL_version = POSTGRESQL_version | default('latest') -%}
{% set __POSTGRESQL_POSTGIS_version = POSTGRESQL_POSTGIS_version | default('latest') -%}
{% set __POSTGRESQL_PGVECTOR_version = POSTGRESQL_PGVECTOR_version | default('latest') -%}
{% set __POSTGRESQL_PGVECTO_RS_version = POSTGRESQL_PGVECTO_RS_version | default('latest') -%}
{% set __POSTGRESQL_AGE_version = POSTGRESQL_AGE_version | default('latest') -%}
{% set __POSTGRESQL_POSTGIS_WE_version = POSTGRESQL_POSTGIS_WE_version | default('latest') -%}
{% set __POSTGRESQL_PGDUCKDB_version = POSTGRESQL_PGDUCKDB_version | default('latest') -%}

{% set __POSTGREST_version = POSTGREST_version | default('latest') -%}

{% set __SUPABASE_STUDIO_version = SUPABASE_STUDIO_version | default('latest') -%}
{% set __SUPABASE_AUTH_version = SUPABASE_AUTH_version | default('latest') -%}
{% set __SUPABASE_REST_version = SUPABASE_REST_version | default('latest') -%}
{% set __SUPABASE_REALTIME_version = SUPABASE_REALTIME_version | default('latest') -%}
{% set __SUPABASE_STORAGE_version = SUPABASE_STORAGE_version | default('latest') -%}
{% set __SUPABASE_IMGPROXY_version = SUPABASE_IMGPROXY_version | default('latest') -%}
{% set __SUPABASE_META_version = SUPABASE_META_version | default('latest') -%}
{% set __SUPABASE_EDGE_RUNTIME_version = SUPABASE_EDGE_RUNTIME_version | default('latest') -%}
{% set __SUPABASE_ANALYTICS_version = SUPABASE_ANALYTICS_version | default('latest') -%}
{% set __SUPABASE_POSTGRESQL_version = SUPABASE_POSTGRESQL_version | default('latest') -%}
{% set __SUPABASE_VECTOR_version = SUPABASE_VECTOR_version | default('latest') -%}
{% set __SUPABASE_SUPAVISOR_version = SUPABASE_SUPAVISOR_version | default('latest') -%}

{% set __TIMESCALEDB_version = TIMESCALEDB_version | default('latest') -%}
{% set __SQLSERVER_version = SQLSERVER_version | default('latest') -%}
{% set __YUGABYTE_version = YUGABYTE_version | default('latest') -%}
{% set __SINGLE_STORE_version = SINGLE_STORE_version | default('latest') -%}

{% set __PGADMIN_version = PGADMIN_version | default('latest') -%}
{% set __ADMINER_version = ADMINER_version | default('latest') -%}

{% set __CLOUDBEAVER_version = CLOUDBEAVER_version | default('latest') -%}
{% set __SQLPAD_version = SQLPAD_version | default('latest') -%}
{% set __SQLCHAT_version = SQLCHAT_version | default('latest') -%}
{% set __QUERYBOOK_version = QUERYBOOK_version | default('latest') -%}
{% set __DBGATE_version = DBGATE_version | default('latest') -%}
{% set __NOCODB_version = NOCODB_version | default('latest') -%}
{% set __QUIX_version = QUIX_version | default('latest') -%}

{% set __AXON_version = AXON_version | default('latest') -%}
{% set __EVENTSTORE_version = EVENTSTORE_version | default('latest') -%}

{% set __HAZELCAST_version = HAZELCAST_version | default('latest') -%}
{% set __HAZELCAST_MC_version = HAZELCAST_MC_version | default('latest') -%}

{% set __TRINO_version = TRINO_version | default('latest') -%}
{% set __STARBURSTDATA_version = STARBURSTDATA_version | default('latest') -%}
{% set __TRINO_CLI_version = TRINO_CLI_version | default('latest') -%}
{% set __PRESTO_version = PRESTO_version | default('latest') -%}
{% set __AHANA_version = AHANA_version | default('latest') -%}
{% set __PRESTO_CLI_version = PRESTO_CLI_version | default('latest') -%}
{% set __DREMIO_version = DREMIO_version | default('latest') -%}
{% set __DRILL_version = DRILL_version | default('latest') -%}

{% set __HASURA_version = HASURA_version | default('latest') -%}
{% set __GRAPHQL_MESH_version = GRAPHQL_MESH_version | default('latest') -%}
{% set __DIRECTUS_version = DIRECTUS_version | default('latest') -%}

{% set __TYK_version = TYK_version | default('latest') -%}
{% set __TYK_PUMP_version = TYK_PUMP_version | default('latest') -%}

{% set __KONG_version = KONG_version | default('latest') -%}
{% set __KONG_DECK_version = KONG_DECK_version | default('latest') -%}
{% set __KONGA_version = KONGA_version | default('latest') -%}
{% set __KONG_ADMIN_UI_version = KONG_ADMIN_UI_version | default('latest') -%}
{% set __KONG_MAP_version = KONG_MAP_version | default('latest') -%}

{% set __CURITY_version = CURITY_version | default('latest') -%}

{% set __NUCLIO_version = NUCLIO_version | default('latest') -%}

{% set __MOSQUITTO_version = MOSQUITTO_version | default('latest') -%}
{% set __HIVEMQ3_version = HIVEMQ3_version | default('latest') -%}
{% set __HIVEMQ4_version = HIVEMQ4_version | default('latest') -%}
{% set __EMQX_oss_version = EMQX_oss_version | default('latest') -%}
{% set __EMQX_enterprise_version = EMQX_enterprise_version | default('latest') -%}
{% set __MQTTX_CLI_version = MQTTX_CLI_version | default('latest') -%}
{% set __MQTTX_WEB_version = MQTTX_WEB_version | default('latest') -%}
{% set __HIVEMQ_MQTT_WEB_CLIENT_version = HIVEMQ_MQTT_WEB_CLIENT_version | default('latest') -%}

{% set __CEDALO_MANAGEMENT_CENTER_version = CEDALO_MANAGEMENT_CENTER_version | default('latest') -%}

{% set __THINGSBOARD_version = THINGSBOARD_version | default('latest') -%}

{% set __ACTIVEMQ_CLASSIC_version = ACTIVEMQ_CLASSIC_version | default('latest') -%}
{% set __ACTIVEMQ_ARTEMIS_version = ACTIVEMQ_ARTEMIS_version | default('latest') -%}
{% set __RABBITMQ_version = RABBITMQ_version | default('latest') -%}
{% set __SOLACE_PUBSUB_version = SOLACE_PUBSUB_version | default('latest') -%}
{% set __SOLACE_KAFKA_PROXY_version = SOLACE_KAFKA_PROXY_version | default('latest') -%}
{% set __NATS_version = NATS_version | default('latest') -%}

{% set __PURE_FTPD_version = PURE_FTPD_version | default('latest') -%}
{% set __SFTP_version = SFTP_version | default('latest') -%}
{% set __SFTPGO_version = SFTPGO_version | default('latest') -%}

{% set __FILEZILLA_version = FILEZILLA_version | default('latest') -%}

{% set __MAILDEV_version = MAILDEV_version | default('latest') -%}
{% set __MAILPIT_version = MAILPIT_version | default('latest') -%}
{% set __MAILHOG_version = MAILHOG_version | default('latest') -%}

{% set __CAMUNDA_BPM_PLATFORM_version = CAMUNDA_BPM_PLATFORM_version | default('latest') -%}
{% set __CAMUNDA_OPTIMIZE_version = CAMUNDA_OPTIMIZE_version | default('latest') -%}
{% set __CAMUNDA_ZEEBE_version = CAMUNDA_ZEEBE_version | default('latest') -%}
{% set __CAMUNDA_OPERATE_version = CAMUNDA_OPERATE_version | default('latest') -%}
{% set __CAMUNDA_ZEEQS_version = CAMUNDA_ZEEQS_version | default('latest') -%}

{% set __X4_SERVER_version = X4_SERVER_version | default('latest') -%}

{% set __IOEVENT_COCKPIT_version = IOEVENT_COCKPIT_version | default('latest') -%}

{% set __PENTHAO_version = PENTHAO_version | default('latest') -%}

{% set __MINIO_version = MINIO_version | default('latest') -%}
{% set __MINIO_MC_version = MINIO_MC_version | default('latest') -%}
{% set __MINIO_CONSOLE_version = MINIO_CONSOLE_version | default('latest') -%}
{% set __ADMINIO_UI_version = ADMINIO_UI_version | default('latest') -%}
{% set __ADMINIO_API_version = ADMINIO_API_version | default('latest') -%}
{% set __MINIO_WEB_version = MINIO_WEB_version | default('latest') -%}
{% set __MINIO_KES_version = MINIO_KES_version | default('latest') -%}
{% set __S3MANAGER_version = S3MANAGER_version | default('latest') -%}
{% set __FILESTASH_version = FILESTASH_version | default('latest') -%}
{% set __AWSCLI_version = AWSCLI_version | default('latest') -%}
{% set __AZURECLI_version = AZURECLI_version | default('latest') -%}
{% set __AZURE_STORAGE_EXPLORER_version = AZURE_STORAGE_EXPLORER_version | default('latest') -%}
{% set __ICEBERG_REST_CATALOG_version = ICEBERG_REST_CATALOG_version | default('latest') -%}

{% set __LAKEFS_version = LAKEFS_version | default('latest') -%}
{% set __LAKEFS_HOOKS_version = LAKEFS_HOOKS_version | default('latest') -%}

{% set __NESSIE_version = NESSIE_version | default('latest') -%}

{% set __UNITY_CATALOG_version = UNITY_CATALOG_version | default('latest') -%}
{% set __UNITY_CATALOG_UI_version = UNITY_CATALOG_UI_version | default('latest') -%}

{% set __DBT_version = DBT_version | default('latest') -%}

{% set __TAIGA_version = TAIGA_version | default('latest') -%}
{% set __TASKCAFE_version = TASKCAFE_version | default('latest') -%}
{% set __FOCALBOARD_version = FOCALBOARD_version | default('latest') -%}

{% set __CODE_SERVER_version = CODE_SERVER_version | default('latest') -%}
{% set __EXCALIDRAW_version = EXCALIDRAW_version | default('latest') -%}
{% set __DRAWIO_version = DRAWIO_version | default('latest') -%}
{% set __FIREFOX_version = FIREFOX_version | default('latest') -%}
{% set __FILE_BROWSER_version = FILE_BROWSER_version | default('latest') -%}
{% set __VAULT_version = VAULT_version | default('latest') -%}
{% set __KEYCLOAK_version = KEYCLOAK_version | default('latest') -%}
{% set __AUTHELIA_version = AUTHELIA_version | default('latest') -%}
{% set __MOCK_SERVER_version = MOCK_SERVER_version | default('latest') -%}
{% set __ETCD_version = ETCD_version | default('latest') -%}
{% set __ETCD_BROWSER_version = ETCD_BROWSER_version | default('latest') -%}
{% set __PORTAINER_version = PORTAINER_version | default('latest') -%}
{% set __DOCKER_EXEC_WEBCONSOLE_version = DOCKER_EXEC_WEBCONSOLE_version | default('latest') -%}
{% set __RANCHER_version = RANCHER_version | default('latest') -%}
{% set __CETUSGUARD_version = CETUSGUARD_version | default('latest') -%}
{% set __NGROK_version = NGROK_version | default('latest') -%}
{% set __CADVISOR_version = CADVISOR_version | default('latest') -%}
{% set __GLANCES_version = GLANCES_version | default('latest') -%}
{% set __DOCKER_REGISTRY_version = DOCKER_REGISTRY_version | default('latest') -%}
{% set __DOCKER_REGISTRY_UI_version = DOCKER_REGISTRY_UI_version | default('latest') -%}
{% set __GITWEB_version = GITWEB_version | default('latest') -%}
{% set __HAWTIO_version = HAWTIO_version | default('latest') -%}
{% set __SPRING_BOOT_ADMIN_version = SPRING_BOOT_ADMIN_version | default('latest') -%}
{% set __WETTY_version = WETTY_version | default('latest') -%}

{% set __RANETO_version = RANETO_version | default('latest') -%}
{% set __MARKDOWN_MADNESS_version = MARKDOWN_MADNESS_version | default('latest') -%}

{% set __MARKDOWN_VIEWER_version = MARKDOWN_VIEWER_version | default('latest') -%}

{% set __LOG4BRAINS_version = LOG4BRAINS_VIEWER_version | default('latest') -%}

{% set __WATCHTOWER_version = WATCHTOWER_version | default('latest') -%}

{% set __POSTMAN_version = POSTMAN_version | default('latest') -%}

{% set __PACT_BROKER_version = PACT_BROKER_version | default('latest') -%}

{% set __SWAGGER_EDITOR_version = SWAGGER_EDITOR_version | default('latest') -%}
{% set __SWAGGER_UI_version = SWAGGER_UI_version | default('latest') -%}

{% set __ASYNCAPI_STUDIO_version = ASYNCAPI_STUDIO_version | default('latest') -%}

{% set __ASYNCAPI_STUDIO_version = ASYNCAPI_STUDIO_version | default('latest') -%}

{% set __DATA_MESH_MANAGER_version = DATA_MESH_MANAGER_version | default('latest') -%}
{% set __DATA_CONTRACT_MANAGER_version = DATA_CONTRACT_MANAGER_version | default('latest') -%}
{% set __DATA_CONTRACT_CLI_version = DATA_CONTRACT_CLI_version | default('latest') -%}

{% set __OTEL_COLLECTOR_version = OTEL_COLLECTOR_version | default('latest') -%}
{% set __ZIPKIN_version = ZIPKIN_version | default('latest') -%}
{% set __JAEGER_version = JAEGER_version | default('latest') -%}
{% set __PITCHFORK_version = PITCHFORK_version | default('latest') -%}

{% set __hw_arch = hw_arch | default('x86-64') -%}
{% set __PYTHON_version = PYTHON_version | default('3') -%}

{% set __S3FS_version = S3FS_version | default('3') -%}

{% set __WEB_PROTEGE_version = WEB_PROTEGE_version | default('3') -%}

{% set __HAPI_FHIR_version = HAPI_FHIR_version | default('3') -%}
{% set __BLAZE_FHIR_version = BLAZE_FHIR_version | default('3') -%}
{% set __LFH_FHIR_version = LFH_FHIR_version | default('3') -%}
{% set __FHIR_GATEWAY_version = FHIR_GATEWAY_version | default('3') -%}

{% if DATAHUB_enable -%}
# enforce some dependencies
  {% set KAFKA_enable = true -%}
  {% set SCHEMA_REGISTRY_enable = true -%}
{% endif -%}   {# DATAHUB_enable  #}

{% if (KAFKA_enable and not KAFKA_use_kraft_mode) or ATLAS_enable or DRILL_enable or PINOT_enable -%}
# enforce some dependencies
  {% set ZOOKEEPER_enable = true -%}
{% endif -%}   {# KAFKA_enable or ATLAS_enable or DRILL_enable or PINOT_enable #}

{% if (NIFI_enable and NIFI_create_cluster) -%}
# enforce some dependencies
  {% set ZOOKEEPER_enable = true -%}
{% endif -%}   {# NIFI_enable and NIFI_create_cluster #}

{% if (NIFI2_enable and NIFI2_create_cluster) -%}
# enforce some dependencies
  {% set ZOOKEEPER_enable = true -%}
{% endif -%}   {# NIFI2_enable and NIFI2_create_cluster #}

{% if (VECTOR_ADMIN_enable) -%}
# enforce some dependencies
  {% set POSTGRESQL_enable = true -%}
{% endif -%}

{% if jmx_monitoring_with_prometheus_enable -%}
# enforce some dependencies
  {% set PROMETHEUS_enable = true -%}
  {% set GRAFANA_enable = true -%}
{% endif -%}   {# jmx_monitoring_with_prometheus_enable #}

{% if KAFKA_SCHEMA_REGISTRY_enable is defined and KAFKA_SCHEMA_REGISTRY_enable -%}
# backward compatiblity to platform < 1.14.0
# enforce some dependencies
  {% set SCHEMA_REGISTRY_enable = true -%}
{% endif -%}   {# KAFKA_SCHEMA_REGISTRY_enable #}

{% if KAFKA_TOPICS_UI_enable -%}
# enforce some dependencies
  {% set KAFKA_RESTPROXY_enable = true -%}
{% endif -%}   {# KAFKA_TOPICS_UI_enable #}

{% if KAFKA_CONNECT_enable -%}
# enforce some dependencies
  {% set KSQLDB_use_embedded_connect = false -%}
{% endif -%}   {# KAFKA_CONNECT_enable #}

{% if KSQLDB_enable and not external['SCHEMA_REGISTRY_url'] -%}
# enforce some dependencies
  {% set SCHEMA_REGISTRY_enable = true -%}
{% endif -%}   {# KSQLDB_enable #}

{% if (SCHEMA_REGISTRY_enable and SCHEMA_REGISTRY_flavour | lower) == 'apicurio' and APICURIO_auth_enabled | default(false) -%}
# enforce keycloak dependencies
  {% set KEYCLOAK_enable = true -%}
{% endif -%}   {# PLATFORM_ARM_enable #}

{% if HUE_enable -%}
# enforce some dependencies
  {% set SOLR_enable = true -%}
{% endif -%}   {# PLATFORM_ARM_enable #}

{% if TIPBOARD_enable -%}
# enforce some dependencies
  {% set REDIS_enable = true -%}
{% endif -%}   {# TIPBOARD_enable #}

{% if HASURA_enable -%}
# enforce some dependencies
  {% set POSTGRESQL_enable = true -%}
{% endif -%}   {# HASURA_enable #}

{% if CAMUNDA_ZEEBE_enable -%}
# enforce some dependencies
  {% set ELASTICSEARCH_enable = true -%}
{% endif -%}   {# CAMUNDA_ZEEBE_enable #}

{% if MICROCKS_enable -%}
# enforce some dependencies
  {% set KEYCLOAK_enable = true -%}
  {% set POSTMAN_enable = true -%}
{% endif -%}   {# MICROCKS_enable #}

{% if FIREFOX_use_port_80 -%}
  # if FIREFOX runs on port 80, then markdown viewer can not run on 80
  {% set MARKDOWN_VIEWER_use_port_80 = false -%}
{% endif -%}   {# FIREFOX_use_port_80 #}

{% if MLFLOW_SERVER_enable -%}
# Enable PostgreSQL or MySQL for MLflow server
  {%if MLFLOW_SERVER_backend | default('file') == 'postgresql' %}
    {% set POSTGRESQL_enable = true -%}
  {%elif MLFLOW_SERVER_backend | default('file') == 'mysql' %}
    {% set MYSQL_enable = true -%}
  {% endif -%}
{% endif -%}   {# MLFLOW_SERVER_enable #}

{% if (__hw_arch | upper) == 'ARM' -%}
# if we use stack type ARM, then disable some containers which do not support ARM
  {% set ZOOKEEPER_enable = false -%}
  {% set KAFKA_enable = false -%}
  {% set APICURIO_SCHEMA_REGISTRY_enable = false -%}
  {% set MATERIALIZE_enable = false -%}
  {% set HSTREAMDB_enable = false -%}
  {% set BENTHOS_enable = false -%}
  {% set RISINGWAVE_enable = false -%}
  {% set TIMEPLUS_enable = false -%}
  {% set ARROYO_enable = false -%}
  {% set SQLFLOW_enable = false -%}
  {% set KAFKA_REPLICATOR_enable = false -%}
  {% set KAFKA_MM2_enable = false -%}
  {% set LENSES_BOX_enable = false -%}
  {% set KCAT_enable = false -%}
  {% set KASKADE_enable = false -%}
  {% set JIKKOU_enable = false -%}
  {% set JIKKOU_SERVER_enable = false -%}  
  {% set KAFKA_UI_enable = false -%}
  {% set EFAK_enable = false -%}
  {% set KOWL_enable = false -%}
  {% set REDPANDA_CONSOLE_enable = false -%}
  {% set KOUNCIL_enable = false -%}
  {% set KAFKA_MAGIC_enable = false -%}
  {% set KAFKA_LAG_EXPORTER_enable = false -%}
  {% set KAFKA_WEBVIEW_enable = false -%}
  {% set KPOW_enable = false -%}
  {% set CONDUKTOR_PLATFORM_enable = false -%}
  {% set KADECK_enable = false -%}
  {% set KAFKISTRY_enable = false -%}
  {% set KLAW_enable = false -%}
  {% set KAFKA_CONNECTOR_BOARD_enable = false -%}
  {% set ZILLA_enable = false -%}
  {% set REMORA_enable = false -%}
  {% set DEBEZIUM_SERVER_enable = false -%}
  {% set DEBEZIUM_UI_enable = false -%}
  {% set HADOOP_enable = false -%}
  {% set SPARK_enable = false -%}
  {% set TIKA_enable = false -%}
  {% set NLM_INGESTOR_enable = false -%}
  {% set UNSTRUCTURED_enable = false -%}
  {% set FIRECRAWL_enable = false -%}
  {% set CRAWL4AI_enable = false -%}
  {% set WHISPER_enable = false -%}
  {% set NLLB_enable = false -%}
  {% set HIVE_enable = false -%}
  {% set AVRO_TOOLS_enable = false -%}
  {% set OPENLDAP_enable = false -%}
  {% set PHP_LDAP_ADMIN_enable = false -%}
  {% set LDAP_USER_MANAGER_enable = false -%}
  {% set RANGER_enable = false -%}
  {% set RANGER_POSTGRES_enable = false -%}
  {% set OPA_enable = false -%}
  {% set STYRA_EOPA_enable = false -%}
  {% set CEDAR_enable = false -%}
  {% set OPAL_enable = false -%}
  {% set ATLAS_enable = false -%}
  {% set DATAHUB_enable = false -%}
  {% set AMUNDSEN_enable = false -%}
  {% set DATA_PRODUCT_PORTAL_enable = false -%}
  {% set MARQUEZ_enable = false -%}
  {% set CKAN_enable = false -%}
  {% set DATAVERSE_enable = false -%}
  {% set HUE_enable = false -%}
  {% set STREAMSETS_enable = false -%}
  {% set STREAMSETS_TRANSFORMER_enable = false -%}
  {% set STREAMSETS_DATAOPS_enable = false -%}
  {% set NIFI_enable = false -%}
  {% set NIFI2_enable = false -%}
  {% set NIFI_REGISTRY_enable = false -%}
  {% set NIFI_TOOLKIT_enable = false -%}
  {% set N8N_enable = false -%}
  {% set STREAMSHEETS_enable = false -%}
  {% set CONDUIT_enable = false -%}
  {% set SPRING_DATAFLOW_enable = false -%}
  {% set ASPIRE_enable = false -%}
  {% set AIRBYTE_enable = false -%}
  {% set AIRFLOW_enable = false -%}
  {% set MAGE_AI_enable = false -%}
  {% set KESTRA_enable = false -%}
  {% set ZEPPELIN_enable = false -%}
  {% set JUPYTER_enable = false -%}
  {% set JUPYTERHUB_enable = false -%}
  {% set RSTUDIO_enable = false -%}
  {% set SHINY_SERVER_enable = false -%}
  {% set MLFLOW_SERVER_enable = false -%}
  {% set OPTUNA_enable = false -%}
  {% set MINDSDB_enable = false -%}
  {% set OLLAMA_enable = false -%}
  {% set NVIDIA_NIM_enable = false -%}
  {% set OPEN_WEBUI_enable = false -%}
  {% set ALPACA_WEBUI_enable = false -%}
  {% set ANYTHING_LLM_enable = false -%}
  {% set PERPLEXICA_enable = false -%}
  {% set BIG_AGI_enable = false -%}
  {% set AUTOGEN_STUDIO_enable = false -%}
  {% set CREWAI_STUDIO_enable = false -%}  
  {% set AGENT_ZERO_enable = false -%}  
  {% set LITELLM_enable = false -%}
  {% set FLOWISE_enable = false -%}
  {% set LANGFLOW_enable = false -%}
  {% set LANGFUSE_enable = false -%}  
  {% set OPENLIT_enable = false -%}  
  {% set LANGWATCH_enable = false -%}   
  {% set LANGEVALS_enable = false -%} 
  {% set ARIZE_PHOENIX_enable = false -%}  
  {% set GPT_RESEARCHER_enable = false -%}
  {% set LOCAL_DEEP_RESEARCHER_enable = false -%}
  {% set VERBA_enable = false -%}
  {% set RAGFLOW_enable = false -%}
  {% set NEO4j_KG_BUILDER_enable = false -%}
  {% set DATAIKU_DSS_enable = false -%}
  {% set KIE_SERVER_enable = false -%}
  {% set METABASE_enable = false -%}
  {% set SUPERSET_enable = false -%}
  {% set REDASH_enable = false -%}
  {% set SMASHING_enable = false -%}
  {% set TIPBOARD_enable = false -%}
  {% set CHARTBOARD_enable = false -%}
  {% set BASEROW_enable = false -%}
  {% set REDIS_enable = false -%}
  {% set REDIS_INSIGHT_enable = false -%}
  {% set REDIS_COMMANDER_enable = false -%}
  {% set VALKEY_enable = false -%}  
  {% set MONGO_enable = false -%}
  {% set ELASTICSEARCH_enable = false -%}
  {% set OPENSEARCH_enable = false -%}
  {% set OPENSEARCH_DASHBOARDS_enable = false -%}
  {% set DEJAVU_enable = false -%}
  {% set CEREBRO_enable = false -%}
  {% set ELASTICHQ_enable = false -%}
  {% set ELASTICVUE_enable = false -%}
  {% set KIBANA_enable = false -%}
  {% set SPLUNK_enable = false -%}
  {% set INFLUXDB2_enable = false -%}
  {% set QUESTDB_enable = false -%}
  {% set KUDU_enable = false -%}
  {% set CHROMA_enable = false -%}
  {% set QDRANT_enable = false -%}
  {% set WEAVIATE_enable = false -%}
  {% set MILVUS_enable = false -%}
  {% set ATTU_enable = false -%}
  {% set INFINITY_enable = false -%}
  {% set VECTOR_ADMIN_enable = false -%}
  {% set DUCKDB_enable = false -%}
  {% set QUACKFLIGHT_enable = false -%}
  {% set DRUID_enable = false -%}
  {% set PINOT_enable = false -%}
  {% set STARROCKS_enable = false -%}
  {% set CLICKHOUSE_enable = false -%}
  {% set CLICKHOUSE_UI_enable = false -%}
  {% set IGNITE_enable = false -%}
  {% set JANUSGRAPH_enable = false -%}
  {% set INVANA_ENGINE_enable = false -%}
  {% set INVANA_STUDIO_enable = false -%}
  {% set NEO4J_enable = false -%}
  {% set QUINE_enable = false -%}
  {% set MEMGRAPH_enable = false -%}
  {% set ARCADEDB_enable = false -%}
  {% set DGRAPH_enable = false -%}
  {% set STARDOG_enable = false -%}
  {% set STARDOG_STUDIO_enable = false -%}
  {% set GRAPHDB_enable = false -%}
  {% set TILE38_enable = false -%}
  {% set POSTGREST_enable = false -%}
  {% set SUPABASE_enable = false -%}
  {% set ORACLE_XE_enable = false -%}
  {% set ORACLE_FREE_enable = false -%}
  {% set ORACLE_OCI_FREE_enable = false -%}
  {% set ORACLE_ADB_FREE_enable = false -%}
  {% set ORACLE_EE_enable = false -%}
  {% set ORACLE_SQLCL_enable = false -%}
  {% set ORACLE_REST_DATA_SERVICE_enable = false -%}
  {% set MYSQL_enable = false -%}
  {% set SQLSERVER_enable = false -%}
  {% set TIMESCALEDB_enable = false -%}
  {% set PGADMIN_enable = false -%}
  {% set ADMINER_enable = false -%}
  {% set CLOUDBEAVER_enable = false -%}
  {% set SQLPAD_enable = false -%}
  {% set SQLCHAT_enable = false -%}
  {% set QUERYBOOK_enable = false -%}
  {% set DBGATE_enable = false -%}
  {% set NOCODB_enable = false -%}
  {% set QUIX_enable = false -%}
  {% set AXON_enable = false -%}
  {% set EVENTSTORE_enable = false -%}
  {% set TRINO_enable = false -%}
  {% set TRINO_CLI_enable = false -%}
  {% set PRESTO_enable = false -%}
  {% set PRESTO_CLI_enable = false -%}
  {% set DREMIO_enable = false -%}
  {% set DRILL_enable = false -%}
  {% set HASURA_enable = false -%}
  {% set GRAPHQL_MESH_enable = false -%}
  {% set DIRECTUS_enable = false -%}
  {% set TYK_enable = false -%}
  {% set TYK_PUMP_enable = false -%}
  {% set TYK_DASHBOARD_enable = false -%}
  {% set KONG_enable = false -%}
  {% set KONG_DECK_enable = false -%}
  {% set KONGA_enable = false -%}
  {% set KONG_ADMIN_UI_enable = false -%}
  {% set KONG_MAP_enable = false -%}
  {% set NUCLIO_enable = false -%}
  {% set HIVEMQ3_enable = false -%}
  {% set HIVEMQ4_enable = false -%}
  {% set EMQX_enable = false -%}
  {% set MQTTX_CLI_enable = false -%}
  {% set MQTTX_WEB_enable = false -%}
  {% set MQTT_UI_enable = false -%}
  {% set THINGSBOARD_enable = false -%}
  {% set ACTIVEMQ_enable = false -%}
  {% set SOLACE_PUBSUB_enable = false -%}
  {% set SOLACE_KAFKA_PROXY_enable = false -%}
  {% set FTP_enable = false -%}
  {% set MAILDEV_enable = false -%}
  {% set MAILPIT_enable = false -%}
  {% set MAILHOG_enable = false -%}
  {% set CAMUNDA_BPM_PLATFORM_enable = false -%}
  {% set CAMUNDA_OPTIMIZE_enable = false -%}
  {% set CAMUNDA_ZEEBE_enable = false -%}
  {% set CAMUNDA_OPERATE_enable = false -%}
  {% set CAMUNDA_ZEEQS_enable = false -%}
  {% set X4_SERVER_enable = false -%}
  {% set IOEVENT_COCKPIT_enable = false -%}
  {% set DBT_enable = false -%}
  {% set PENTHAO_enable = false -%}
  {% set MINIO_enable = false -%}
  {% set MINIO_MC_enable = false -%}
  {% set MINIO_CONSOLE_enable = false -%}
  {% set MINIO_WEB_enable = false -%}
  {% set LAKEFS_enable = false -%}
  {% set NESSIE_enable = false -%}
  {% set UNITY_CATALOG_enable = false -%}
  {% set ADMINIO_UI_enable = false -%}
  {% set ADMINIO_API_enable = false -%}
  {% set FILESTASH_enable = false -%}
  {% set S3MANAGER_enable = false -%}
  {% set AWSCLI_enable = false -%}
  {% set AZURE_STORAGE_EXPLORER_enable = false -%}
  {% set KEYCLOAK_enable = false -%}
  {% set POSTMAN_enable = false -%}
  {% set PACT_BROKER_enable = false -%}
  {% set DATA_MESH_MANAGER_enable = false -%}
  {% set DATA_CONTRACT_MANAGER_enable = false -%}
  {% set DATA_CONTRACT_CLI_enable = false -%}
  {% set MICROCKS_enable = false -%}
  {% set MOCK_SERVER_enable = false -%}
  {% set OTEL_COLLECTOR_enable = false %}
  {% set ZIPKIN_enable = false -%}
  {% set JAEGER_enable = false -%}
  {% set PITCHFORK_enable = false -%}
  {% set TEMPO_enable = false -%}
  {% set TAIGA_enable = false -%}
  {% set TASKCAFE_enable = false -%}
  {% set FOCALBOARD_enable = false -%}
  {% set CODE_SERVER_enable = false -%}
  {% set EXCALIDRAW_enable = false -%}
  {% set DRAWIO_enable = false -%}
  {% set FIREFOX_enable = false -%}
  {% set FILE_BROWSER_enable = false -%}
  {% set ETCD_E3W_enable = false -%}
  {% set DOCKER_EXEC_WEBCONSOLE_enable = false -%}
  {% set NGROK_enable = false -%}
  {% set CADVISOR_enable = false -%}
  {% set GLANCES_enable = false -%}  
  {% set DOCKER_REGISTRY_enable = false -%}
  {% set DOCKER_REGISTRY_UI_enable = false -%}
  {% set GITWEB_enable = false -%}
  {% set HAWTIO_enable = false -%}
  {% set SPRING_BOOT_ADMIN_enable = false -%}
  {% set WETTY_enable = false -%}
  {% set RANETO_enable = false -%}
  {% set MARKDOWN_MADNESS_enable = false -%}
  {% set MARKDOWN_VIEWER_enable = false -%}
  {% set LOG4BRAINS_enable = false -%}
  {% set SWAGGER_EDITOR_enable = false -%}
  {% set SWAGGER_UI_enable = false -%}
  {% set ASYNCAPI_STUDIO_enable = false -%}
{% endif %}   {# PLATFORM_ARM_enable #}

{#  ================================== Kafka Settings ========================================== #}
{% macro login_module(listener_sasl_mechanism, default_sasl_mechanism) -%}
  {%- set sasl_mechanism = (listener_sasl_mechanism if listener_sasl_mechanism is defined and listener_sasl_mechanism != None else default_sasl_mechanism) -%}
  {%- if sasl_mechanism == "SCRAM-SHA-256" or sasl_mechanism == "SCRAM_SHA-512" -%}
    org.apache.kafka.common.security.scram.ScramLoginModule
  {%- elif sasl_mechanism == "OAUTHBEARER" -%}
    org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule
  {%- elif sasl_mechanism == "PLAIN" -%}
    org.apache.kafka.common.security.plain.PlainLoginModule
  {%- else -%}
    org.apache.kafka.common.security.plain.PlainLoginModule
  {%- endif -%}
{%- endmacro -%}

{% macro users_list(admin_username, admin_password, connect_username, connect_password,
                schemaregistry_username, schemaregistry_password, ksqldb_username, ksqldb_password,
                tools_username, tools_password, other_users) -%}
  {{admin_username}}:{{admin_password}}
  {%- if connect_username is defined and connect_username and connect_username | length -%}
     ,{{connect_username}}:{{connect_password}}
  {%- endif -%}
  {%- if schemaregistry_username is defined and schemaregistry_username and schemaregistry_username | length -%}
     ,{{schemaregistry_username}}:{{schemaregistry_password}}
  {%- endif -%}
  {%- if schemaregistry_username is defined and schemaregistry_username and schemaregistry_username | length -%}
     ,{{ksqldb_username}}:{{ksqldb_password}}
  {%- endif -%}
  {%- if schemaregistry_username is defined and schemaregistry_username and schemaregistry_username | length -%}
     ,{{tools_username}}:{{tools_password}}
  {%- endif -%}
  {%- if other_users is defined and other_users and other_users | length -%}
     ,{{other_users}}
  {%- endif -%}
{%- endmacro -%}

{% set ns = namespace(bootstrapServers='') %}
{% set ns = namespace(schemaRegistryUrl='') %}
{% set ns = namespace(secureKafkaEnabled='') %}
{% set ns = namespace(securityProtocol='') %}
{% set ns = namespace(saslMechanism='') %}
{% set ns = namespace(serviceNames='') %}
{% set ns = namespace(ports='') %}
{% set ns = namespace(portsJMX='') %}
{% set ns = namespace(portsProm='') %}
{% set ns = namespace(controllerQuorumVoters='') %}
{% if (external['KAFKA_enable']) | default(false) %}
    {% set ns.bootstrapServers = external['KAFKA_bootstrap_servers'] %}
    {% set ns.bootstrapServer = external['KAFKA_bootstrap_servers'].split(',')[0] %}
    {% set ns.secureKafkaEnabled = (external['KAFKA_security_protocol'] is defined and external['KAFKA_security_protocol'] and external['KAFKA_security_protocol'] | length and external['KAFKA_security_protocol'] != 'PLAINTEXT') %}
    {% set ns.securityProtocol = external['KAFKA_security_protocol'] %}
    {% set ns.securityProtocolBROKER = external['KAFKA_security_protocol'] %}
    {% set ns.saslMechanism = external['KAFKA_sasl_mechanism'] %}
    {% set ns.saslMechanismBROKER = external['KAFKA_sasl_mechanism'] %}
    {% set ns.loginModule = external['KAFKA_login_module'] %}
    {% set ns.loginModuleBROKER = external['KAFKA_login_module'] %}
  {% if external['KAFKA_sasl_username'] is defined and external['KAFKA_sasl_username'] | default(false) %}
    {% set ns.kafkaToolsDefaultUsername = external['KAFKA_sasl_username'] %}
  {% else %}
    {% set ns.kafkaToolsDefaultUsername = '${PLATYS_EXTERNAL_KAFKA_USERNAME:?PLATYS_EXTERNAL_KAFKA_USERNAME must be set either in .env or as an environment variable}' %}
  {% endif %}  {# KAFKA_sasl_username #}
  {% if external['KAFKA_sasl_password'] is defined and external['KAFKA_sasl_password'] | default(false) %}
    {% set ns.kafkaToolsDefaultUsername = external['KAFKA_sasl_password'] %}
  {% else %}
    {% set ns.kafkaToolsDefaultPassword = '${PLATYS_EXTERNAL_KAFKA_PASSWORD:?PLATYS_EXTERNAL_KAFKA_PASSWORD must be set either in .env or as an environment variable}' %}
  {% endif %}  {# KAFKA_sasl_password #}
{% else %} {# not external['KAFKA_sasl_username'] #}
    {#% for datacenter in (DATA_CENTER.split(',') | default([]) ) %#}
  {% set dcName = dataCenter %}
  {% set basePort = 9092 + dataCenterId * KAFKA_broker_nodes %}
  {% set basePortJMX = 9992 + dataCenterId * KAFKA_broker_nodes %}
  {% set basePortProm = 1234 + dataCenterId * KAFKA_broker_nodes %}
  {% set serviceBaseName = "kafka" ~ dcName ~ "-" %}

  {% for num in range(KAFKA_broker_nodes | default(1) ) %}
    {% set port = basePort + loop.index-1 %}
    {% set portJMX = basePortJMX + loop.index-1 %}
    {% set portProm = basePortProm + loop.index-1 %}
    {% set serviceName = serviceBaseName ~ loop.index %}
    {% if loop.first %}
      {% set ns.bootstrapServers = serviceName ~ ":1" ~ port %}
      {% set ns.controllerQuorumVoters = loop.index ~ ":1" ~ serviceName ~ ":1" ~ port %}
      {% set ns.controllerQuorumVoters = loop.index ~ "@" ~ serviceName ~ ":4" ~ port %}
    {% elif loop.index <= 3 %}
      {% set ns.bootstrapServers = ns.bootstrapServers ~ ',' ~ serviceName ~ ":1" ~ port %}
      {% set ns.controllerQuorumVoters = ns.controllerQuorumVoters ~ ',' ~ loop.index ~ "@" ~ serviceName ~ ":4" ~ port %}
    {% endif %}

    {% if ns.serviceNames is not defined %}
      {% set ns.serviceNames = serviceName %}
      {% set ns.ports = port|string %}
      {% set ns.portsJMX = portJMX|string %}
      {% set ns.portsProm = portProm|string %}
    {% else %}
      {% set ns.serviceNames = ns.serviceNames ~ ',' ~ serviceName %}
      {% set ns.ports = ns.ports ~ ',' ~ port %}
      {% set ns.portsJMX = ns.portsJMX ~ ',' ~ portJMX %}
      {% set ns.portsProm = ns.portsProm ~ ',' ~ portProm %}
    {% endif %}
  {% endfor %}
  {% set ns.bootstrapServer = ns.bootstrapServers.split(',')[0] %}

    {#% endfor #}  {# datacenter in (KAFKA_datacenters.split(',') | default([]) ) #}
  {% set ns.secureKafkaEnabled = (KAFKA_security_protocol != 'PLAINTEXT') %}

  {% set ns.securityProtocol = KAFKA_security_protocol %}
  {% set ns.securityProtocolCONTROLLER = KAFKA_controller_security_protocol if KAFKA_controller_security_protocol is defined and KAFKA_controller_security_protocol and KAFKA_controller_security_protocol | length else KAFKA_security_protocol %}
  {% set ns.securityProtocolBROKER = KAFKA_broker_security_protocol if KAFKA_broker_security_protocol is defined and KAFKA_broker_security_protocol and KAFKA_broker_security_protocol | length else KAFKA_security_protocol %}
  {% set ns.securityProtocolLOCAL = KAFKA_local_security_protocol if KAFKA_local_security_protocol is defined and KAFKA_local_security_protocol and KAFKA_local_security_protocol | length else KAFKA_security_protocol %}
  {% set ns.securityProtocolDOCKERHOST = KAFKA_dockerhost_security_protocol if KAFKA_dockerhost_security_protocol is defined and KAFKA_dockerhost_security_protocol and KAFKA_dockerhost_security_protocol | length else KAFKA_security_protocol %}
  {% set ns.securityProtocolEXTERNAL = KAFKA_external_security_protocol if KAFKA_external_security_protocol is defined and KAFKA_external_security_protocol and KAFKA_external_security_protocol | length else KAFKA_security_protocol %}

  {% set ns.saslMechanism = KAFKA_sasl_mechanism %}
  {% set ns.saslMechanismCONTROLLER = KAFKA_controller_sasl_mechanism if KAFKA_controller_sasl_mechanism is defined and KAFKA_controller_sasl_mechanism and KAFKA_controller_sasl_mechanism | length else KAFKA_sasl_mechanism %}
  {% set ns.saslMechanismBROKER = KAFKA_broker_sasl_mechanism if KAFKA_broker_sasl_mechanism is defined and KAFKA_broker_sasl_mechanism and KAFKA_broker_sasl_mechanism | length else KAFKA_sasl_mechanism %}
  {% set ns.saslMechanismLOCAL = KAFKA_local_sasl_mechanism if KAFKA_local_sasl_mechanism is defined and KAFKA_local_sasl_mechanism and KAFKA_local_sasl_mechanism | length else KAFKA_sasl_mechanism %}
  {% set ns.saslMechanismDOCKERHOST = KAFKA_dockerhost_sasl_mechanism if KAFKA_dockerhost_sasl_mechanism is defined and KAFKA_dockerhost_sasl_mechanism and KAFKA_dockerhost_sasl_mechanism | length else KAFKA_sasl_mechanism %}
  {% set ns.saslMechanismEXTERNAL = KAFKA_external_sasl_mechanism if KAFKA_external_sasl_mechanism is defined and KAFKA_external_sasl_mechanism and KAFKA_external_sasl_mechanism | length else KAFKA_sasl_mechanism %}

  {% set ns.loginModule = login_module(KAFKA_sasl_mechanism, KAFKA_sasl_mechanism) %}
  {% set ns.loginModuleCONTROLLER = login_module(KAFKA_controller_sasl_mechanism, KAFKA_sasl_mechanism) %}
  {% set ns.loginModuleBROKER = login_module(KAFKA_broker_sasl_mechanism, KAFKA_sasl_mechanism) %}
  {% set ns.loginModuleLOCAL = login_module(KAFKA_local_sasl_mechanism, KAFKA_sasl_mechanism) %}
  {% set ns.loginModuleDOCKERHOST = login_module(KAFKA_dockerhost_sasl_mechanism, KAFKA_sasl_mechanism) %}
  {% set ns.loginModuleEXTERNAL = login_module(KAFKA_extrnal_sasl_mechanism, KAFKA_sasl_mechanism) %}

  {% set ns.kafkaToolsDefaultUsername = KAFKA_tools_username %}
  {% set ns.kafkaToolsDefaultPassword = KAFKA_tools_password %}

  {% set ns.users = users_list(KAFKA_admin_username, KAFKA_admin_password, KAFKA_CONNECT_kafka_username, KAFKA_CONNECT_kafka_password,
                  KSQLDB_kafka_username,KSQLDB_kafka_password,SCHEMA_REGISTRY_kafka_username,SCHEMA_REGISTRY_kafka_password,
                  KAFKA_tools_username,KAFKA_tools_password,KAFKA_addl_users) %}

{% endif %}   {# external['KAFKA_enable'] #}

{% if ns.secureKafkaEnabled and KAFKA_security_debug | default(false) %}
#  ns.securityProtocolCONTROLLER = {{ns.securityProtocolCONTROLLER}}
#  ns.securityProtocolBROKER = {{ns.securityProtocolBROKER}}
#  ns.securityProtocolLOCAL = {{ns.securityProtocolLOCAL}}
#  ns.securityProtocolDOCKERHOST = {{ns.securityProtocolDOCKERHOST}}
#  ns.securityProtocolEXTERNAL = {{ns.securityProtocolEXTERNAL}}
#
#  ns.saslMechanismCONTROLLER = {{ns.saslMechanismCONTROLLER}}
#  ns.saslMechanismBROKER = {{ns.saslMechanismBROKER}}
#  ns.saslMechanismLOCAL = {{ns.saslMechanismLOCAL}}
#  ns.saslMechanismDOCKERHOST = {{ns.saslMechanismDOCKERHOST}}
#  ns.saslMechanismEXTERNAL = {{ns.saslMechanismEXTERNAL}}

#  ns.loginModuleLOCAL = {{ns.loginModuleLOCAL}}
#  ns.loginModuleDOCKERHOST = {{ns.loginModuleDOCKERHOST}}
#  ns.loginModuleEXTERNAL = {{ns.loginModuleEXTERNAL}}
#  ns.users = {{ns.users}}
{% endif %}

{#  ================================== Schema Registry Settings ========================================== #}
{% if not (external['SCHEMA_REGISTRY_enable']) | default(false) %}
    {% if SCHEMA_REGISTRY_flavour is defined %}
        {%if SCHEMA_REGISTRY_UI_use_public_ip | default(true) %}
          {% set ns.schemaRegistryUrlExternal = 'http://${PUBLIC_IP}:8081' %}
        {% else %}
          {% set ns.schemaRegistryUrlExternal = 'http://${DOCKER_HOST_IP}:8081' %}
        {% endif -%}   {#  SCHEMA_REGISTRY_use_public_ip #}
    {% endif %}

    {% if SCHEMA_REGISTRY_flavour == "confluent" %}
      {% set ns.schemaRegistryUrl = 'http://schema-registry-1:8081' %}
      {% set ns.schemaRegistryServiceName = 'schema-registry-1' %}
    {% elif SCHEMA_REGISTRY_flavour == "apicurio" %}
      {% set ns.schemaRegistryUrl = 'http://apicurio-registry-1:8081/apis/ccompat/v6' %}
      {% set ns.schemaRegistryUrlExternal = ns.schemaRegistryUrlExternal ~ '/apis/ccompat/v6' %}
      {% set ns.schemaRegistryServiceName = 'apicurio-registry-1' %}
    {% else %}
      {% set ns.schemaRegistryUrl = 'http://<schema-registry-not-enabled>' %}
      {% set ns.schemaRegistryUrlExternal = 'http://<schema-registry-not-enabled>' %}
    {% endif %}
{% else -%}
    {% set ns.schemaRegistryUrlExternal = external['SCHEMA_REGISTRY_url'] %}
    {% set ns.schemaRegistryUrl = external['SCHEMA_REGISTRY_url'] %}
    {% set ns.schemaRegistryUsername = '${PLATYS_SCHEMA_REGISTRY_USERNAME:?PLATYS_SCHEMA_REGISTRY_USERNAME must be set either in .env or as an environment variable}' %}
    {% set ns.schemaRegistryPassword = '${PLATYS_SCHEMA_REGISTRY_PASSWORD:?PLATYS_SCHEMA_REGISTRY_PASSWORD must be set either in .env or as an environment variable}' %}
{% endif %}   {# not external['SCHEMA_REGISTRY_enable'] #}

{#  ================================== Kafka Connect Settings ========================================== #}

{% set ns1 = namespace(connectServersInternalUrl='') %}
{% set ns1 = namespace(connectServersUrl='') %}
{% set ns1 = namespace(connectClusterName='') %}
{% if (KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect) | default(false) %}
  {% if KAFKA_CONNECT_enable %}
    {% set connectNodes = KAFKA_CONNECT_nodes %}
    {% set ns1.connectClusterName = 'connect-custer' %}
  {% else %}
    {% set connectNodes = KSQLDB_nodes %}
    {% set ns1.connectClusterName = 'ksqldb-connect-custer' %}
  {% endif %}
  {% for num in range(connectNodes | default(1) ) %}
    {% set port = 8083 + loop.index-1 %}
    {% if loop.first %}
      {% set ns1.connectServersUrl = 'http://${PUBLIC_IP}:' ~ port %}
      {%if KSQLDB_use_embedded_connect | default(true) %}
        {% set ns1.connectServersInternalUrl = 'http://ksqldb-server-' ~ loop.index ~ ':' ~ port %}
      {% else %}
        {% set ns1.connectServersInternalUrl = 'http://kafka-connect-' ~ loop.index ~ ':' ~ port %}
      {% endif -%}   {#  KSQLDB_use_embedded_connect #}
    {% else %}
      {% set ns1.connectServersUrl = ns1.connectServersUrl ~ ',http://${PUBLIC_IP}:' ~ port %}
      {%if KSQLDB_use_embedded_connect | default(true) %}
        {% set ns1.connectServersInternalUrl = ns1.connectServersInternalUrl ~ ',http://ksqldb-server-' ~ loop.index ~ ':' ~ port %}
      {% else %}
        {% set ns1.connectServersInternalUrl = ns1.connectServersInternalUrl ~ ',http://kafka-connect-' ~ loop.index ~ ':' ~ port %}
      {% endif -%}   {#  KSQLDB_use_embedded_connect #}
    {% endif %}
  {% endfor %}
{% endif %}   {# KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect #}

{#  ================================== Spark Settings ========================================== #}
{% if (external['SPARK_enable']) | default(false) %}
  {% set ns.sparkMasterUrl = external['SPARK_master_url']%}
{% else -%} 
  {% set ns.sparkMasterUrl = "spark://spark-master:7077" %}
{% endif -%} 

{% if (SPARK_base_version | lower) == '3.3'  %}
  {% set ns.sparkTableFormatJar = ("/opt/bitnami/spark/jars/delta-core_2.12-2.3.0.jar,/opt/bitnami/spark/jars/delta-storage-2.3.0.jar" if SPARK_table_format_type == 'delta' else "/opt/bitnami/spark/jars/iceberg-spark-runtime-3.3_2.12-1.2.1.jar" if SPARK_table_format_type == 'iceberg' else "/opt/bitnami/spark/jars/hudi-spark3.3-bundle_2.12-0.13.0.jar" if SPARK_table_format_type == 'hudi') -%}
  {% set ns.sparkDeltaDependencies = "io.delta:delta-core_2.12:2.3.0,io.delta:delta-storage:2.3.0" -%}
 {% if NESSIE_enable | default(false) %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.100.0" -%}  
 {% elif LAKEFS_enable | default(false) %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.5.2,io.lakefs:lakefs-iceberg:0.1.4,io.lakefs:lakefs-spark-extensions_2.12:0.0.3" -%}  
 {% else %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.7.0" -%}
 {% endif %}
  {% set ns.sparkHudiDependencies ="org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.0" -%}
{% elif (SPARK_base_version | lower) == '3.4'  %}
  {% set ns.sparkTableFormatJar = ("/opt/bitnami/spark/jars/delta-core_2.12-2.4.0.jar,/opt/bitnami/spark/jars/delta-storage-2.4.0.jar" if SPARK_table_format_type == 'delta' else "/opt/bitnami/spark/jars/iceberg-spark-runtime-3.4_2.12-1.2.1.jar" if SPARK_table_format_type == 'iceberg' else "/opt/bitnami/spark/jars/hudi-spark3.3-bundle_2.12-0.13.0.jar" if SPARK_table_format_type == 'hudi') -%}
  {% set ns.sparkDeltaDependencies = "io.delta:delta-core_2.12:2.4.0,io.delta:delta-storage:2.4.0" -%}
 {% if NESSIE_enable | default(false) %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.100.0" -%}
 {% elif LAKEFS_enable | default(false) %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.2,io.lakefs:lakefs-iceberg:0.1.4,io.lakefs:lakefs-spark-extensions_2.12:0.0.3" -%}
 {% else %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.0" -%}
 {% endif %}
  {% set ns.sparkHudiDependencies ="org.apache.hudi:hudi-spark3.4-bundle_2.12:0.13.0" -%}
{% elif (SPARK_base_version | lower) == '3.5'  %}
  {% set ns.sparkTableFormatJar = ("/opt/bitnami/spark/jars/delta-spark_2.12-3.2.1.jar,/opt/bitnami/spark/jars/delta-storage-3.2.1.jar" if SPARK_table_format_type == 'delta' else "/opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar" if SPARK_table_format_type == 'iceberg' else "/opt/bitnami/spark/jars/hudi-spark3.3-bundle_2.12-0.15.0.jar" if SPARK_table_format_type == 'hudi') -%}
  {% set ns.sparkDeltaDependencies = "io.delta:delta-spark_2.12:3.2.1,io.delta:delta-storage:3.2.1" -%}
 {% if UNITY_CATALOG_enable and SPARK_table_format_type == 'delta' | default(false) %}
  {% set ns.sparkTableFormatJar = ns.sparkTableFormatJar ~ ",/opt/bitnami/spark/jars/unitycatalog-spark_2.12-0.2.1.jar,/opt/bitnami/spark/jars/unitycatalog-client-0.2.1.jar" -%}
  {% set ns.sparkDeltaDependencies = ns.sparkDeltaDependencies ~ ",io.unitycatalog:unitycatalog-spark_2.12:0.2.1,io.unitycatalog:unitycatalog-client:0.2.1" -%}
 {% endif %}
 {% if NESSIE_enable | default(false) %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.100.0" -%}
 {% elif LAKEFS_enable | default(false) %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,io.lakefs:lakefs-iceberg:0.1.4,io.lakefs:lakefs-spark-extensions_2.12:0.0.3" -%}
 {% else %}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0" -%}
 {% endif %}
  {% set ns.sparkHudiDependencies ="org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0" -%}
{% else -%}
  {% set ns.sparkTableFormatJar = ("/opt/bitnami/spark/jars/delta-core_2.12-0.6.1.jar,/opt/bitnami/spark/jars/delta-storage-0.6.1.jar" if SPARK_table_format_type == 'delta' else "/opt/bitnami/spark/jars/iceberg-spark-2-0.13.2.jar" if SPARK_table_format_type == 'iceberg' else "/opt/bitnami/spark/jars/hudi-spark2.4-bundle_2.12-0.13.0" if SPARK_table_format_type == 'hudi') -%}
  {% set ns.sparkDeltaDependencies = "io.delta:delta-core_2.12:0.6.1" -%}
  {% set ns.sparkIcebergDependencies = "org.apache.iceberg:iceberg-spark-2:0.13.2" -%}
  {% set ns.sparkHudiDependencies ="org.apache.hudi:hudi-spark2.4-bundle_2.12:0.13.0" -%}
{% endif -%}    {# SPARK_base_version #}

{% set ns.datahubSparkPluginJar = "/opt/bitnami/spark/jars/datahub-spark-lineage-" ~ __DATAHUB_version | replace("v", "") ~ ".jar" -%}
{% set ns.datahubSparkPluginDependencies = "io.acryl:datahub-spark-lineage:" ~ __DATAHUB_version | replace("v", "") -%}

{#  ================================== S3 Settings ========================================== #}

{% if (external['S3_enable']) | default(false) %}
  {% if (external['S3_endpoint'] != None) | default(false) %}
    {% set s3EndpointStorage = external['S3_endpoint'] %}
  {% endif %}
    {% set s3EndpointStorage = external['S3_endpoint'] %}
    {% set s3PathStyleAccess = external['S3_path_style_access'] %}
    {% set s3AccessKey = '${PLATYS_AWS_ACCESS_KEY:?PLATYS_AWS_ACCESS_KEY must be set either in .env or as an environment variable}' %}
    {% set s3SecretAccessKey = '${PLATYS_AWS_SECRET_ACCESS_KEY:?PLATYS_AWS_SECRET_ACCESS_KEY must be set either in .env or as an environment variable}' %}
    {% set s3DefaultRegion = external['S3_default_region'] %}
    {% set s3AdminBucketName = external['S3_admin_bucket_name'] %}
{% else %}
    {% set s3EndpointStorage = 'http://minio-1:9000' %}
    {% set s3PathStyleAccess = 'true' %}
    {% set s3AccessKey = '${PLATYS_AWS_ACCESS_KEY:-' ~ MINIO_access_key ~'}' %}
    {% set s3SecretAccessKey = '${PLATYS_AWS_SECRET_ACCESS_KEY:-' ~ MINIO_secret_key ~'}' %}
    {% set s3DefaultRegion = 'us-east-1' %}
    {% set s3AdminBucketName = 'admin-bucket' %}
{% endif %}   {# external['s3Endpoint'] #}

{% if LAKEFS_enable and LAKEFS_use_as_drop_in_replacement_for_s3 | default(false) %}
    {% set s3Endpoint = 'http://lakefs:8000' %}
    {% set adminBucketRepo = LAKEFS_admin_bucket_repo if LAKEFS_admin_bucket_repo is defined and LAKEFS_admin_bucket_repo and LAKEFS_admin_bucket_repo | length else 'admin' %}
    {% set adminBucketBranch = LAKEFS_admin_bucket_branch if LAKEFS_admin_bucket_branch is defined and LAKEFS_admin_bucket_branch and LAKEFS_admin_bucket_branch | length else 'main' %}
    {% set s3AdminBucketName = adminBucketRepo ~ '/' ~ adminBucketBranch %}    {# we overwrite the s3AdminBucketName by a lakeFS name #}
{% else %}
    {% set s3Endpoint = s3EndpointStorage %}
{% endif %}   {# LAKEFS_enable #}

{#  ================================== ADLS Settings ========================================== #}

{% if (external['ADLS_enable']) | default(false) %}
  {% if (external['ADLS_storage_account'] != None) | default(false) %}
    {% set adlsStorageAccount = external['ADLS_storage_account'] %}
  {% endif %}
  {% set adlsAccessKey = '${PLATYS_AZURE_ADLS_ACCESS_KEY:?PLATYS_AZURE_ADLS_ACCESS_KEY must be set either in .env or as an environment variable}' %}
{% endif %}   {# external['s3Endpoint'] #}

{#  ================================== Datahub Settings ========================================== #}

{% if (external['DATAHUB_enable']) | default(false) %}
    {% set ns.gmsUrl = external['DATAHUB_gms_url'] %}
{% else %}
    {% set ns.gmsUrl = 'http://datahub-gms:8080' %}
{% endif %}   {# external['DATAHUB_enable'] #}

{#  ================================== Ollama Settings ========================================== #}

{% if (external['OLLAMA_enable']) | default(false) %}
    {% set ns.ollamaUrl = external['OLLAMA_url'] %}
{% else %}
    {% set ns.ollamaUrl = 'http://ollama:11434' %}
{% endif %}   {# external['OLLAMA_enable'] #}

{#  ================================== LMStudio Settings ========================================== #}

{% if (external['LMSTUDIO_enable']) | default(false) %}
    {% set ns.lmstudioUrl = external['LMSTUDIO_url'] %}
{% else %}
    {% set ns.lmstudioUrl = 'http://localhost:1234/v1' %}
{% endif %}   {# external['LMSTUDIO_enable'] #}

{#  ================================== SearXNG Settings ========================================== #}

{% if (external['SEARXNG_enable']) | default(false) %}
    {% set ns.searxngUrl = external['SEARXNG_url'] %}
{% elif SEARXNG_enable | default(false)  %}
    {% set ns.searxngUrl = 'http://searxng:8080' %}
{% else %}
    {% set ns.searxngUrl = '' %}
{% endif %}   {# external['SEARXNG_enable'] #}

{#  ================================== Extension Fields ========================================== #}

{%if logging_driver == "fluentd" | default(false) %}
x-logging: &logging
  logging:
    driver: fluentd
    options:
      fluentd-address: "{{loggin_fluentd_address}}"
      fluentd-async-connect: "true"
      tag: docker
{% endif %}   {# logging_driver = 'fluentd' #}

{%if logging_driver == "loki" | default(false) %}
x-logging: &logging
  logging:
    driver: loki
    options:
      loki-url: "http://localhost:3100/loki/api/v1/push"
      loki-batch-size: "400"
      loki-retries: "0"
{% endif %}   {# logging_driver = 'loki' #}

{%if logging_driver == "syslog" | default(false) %}
x-logging: &logging
  logging:
    driver: syslog
    options:
      syslog-address: "{{logging_syslog_address}}"
      tag: docker
{% endif %}   {# logging_driver = 'syslog' #}

{%if logging_driver == "splunk" | default(false) %}
x-logging: &logging
  logging:
    driver: splunk
    options:
      splunk-url: "{{logging_splunk_url}}"
      splunk-token: "{{logging_splunk_token | default(omit) }}"
      tag: "docker"
{% endif %}   {# logging_driver = 'syslog' #}

{#  ================================== Services ========================================== #}

services:

{% if ZOOKEEPER_enable | default(false) %}
  #  ================================== Zookeeper ========================================== #
  {% for num in range(ZOOKEEPER_nodes | default('1') ) %}
  {% set port = 2181 + (dataCenterId * ZOOKEEPER_nodes) + loop.index-1 %}
  {% set promPort = 1240 + (dataCenterId * ZOOKEEPER_nodes) + loop.index-1 %}
  zookeeper{{dataCenter}}-{{loop.index}}:
    image: confluentinc/cp-zookeeper:{{__CONFLUENT_PLATFORM_version}}
    container_name: zookeeper{{dataCenter}}-{{loop.index}}
    hostname: zookeeper{{dataCenter}}-{{loop.index}}
    labels:
      com.platys.name: "zookeeper"
      com.platys.description: "Zookeeper Node {{loop.index}}"
    ports:
      - "{{port}}:{{port}}"
      - "{{promPort}}:1234"
    environment:
      ZOOKEEPER_SERVER_ID: {{loop.index}}
      ZOOKEEPER_CLIENT_PORT: {{port}}
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    {% if ZOOKEEPER_nodes == 3 %}
      ZOOKEEPER_SERVERS: zookeeper{{dataCenter}}-1:22888:23888;zookeeper{{dataCenter}}-2:32888:33888;zookeeper{{dataCenter}}-3:42888:43888
    {% endif %}
    {% if jmx_monitoring_with_prometheus_enable %}
      EXTRA_ARGS: -javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.20.0.jar=1234:/usr/share/jmx-exporter/zookeeper.yml
    {% endif -%}
    {% if ns.secureKafkaEnabled and (KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or KAFKA_sasl_mechanism == 'SCRAM-SHA-512' or KAFKA_sasl_mechanism == 'PLAIN') %}
      KAFKA_OPTS: -Djava.security.auth.login.config=/etc/kafka/zookeeper_server_jaas.conf
                -Dzookeeper.authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
                -Dzookeeper.allowSaslFailedClients=false
                -Dzookeeper.requireClientAuthScheme=sasl
    {% endif -%} {# KAFKA_sasl_mechanism = 'SCRAM-SHA-256' or 'SCRAM-SHA-512' #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and (KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or KAFKA_sasl_mechanism == 'SCRAM-SHA-512' or KAFKA_sasl_mechanism == 'PLAIN') %}
      - ./security/zookeeper/zookeeper.sasl.jaas.config:/etc/kafka/zookeeper_server_jaas.conf
    {% endif -%} {# KAFKA_sasl_mechanism = 'SCRAM-SHA-256' or 'SCRAM-SHA-512' #}
    {% if ZOOKEEPER_volume_map_data %}
      - "./container-volume/zookeeper/zookeeper-{{loop.index}}/data:/var/lib/zookeeper/data"
      - "./container-volume/zookeeper/zookeeper-{{loop.index}}/log:/var/lib/zookeeper/log"
    {% endif %}
    {% if jmx_monitoring_with_prometheus_enable %}
      - ./scripts/kafka/jmx-exporter/:/usr/share/jmx-exporter
    {% endif -%}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  {% if ZOOKEEPER_NAVIGATOR_enable | default(false) %}
  zoonavigator:
    image: elkozmon/zoonavigator-web:{{__ZOONAVIGATOR_version}}
    container_name: zoonavigator
    hostname: zoonavigator
    labels:
      com.platys.name: "zoonavigator"
      com.platys.description: "Zookeeper Navigator"
      com.platys.webui.title: 'Zoonavigator UI'
      com.platys.webui.url: http://dataplatform:28100
    ports:
     - "28100:8010"
    depends_on:
     - zoonavigator-api
    environment:
      WEB_HTTP_PORT: 8010
      API_HOST: "zoonavigator-api"
      API_PORT: 9010
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  zoonavigator-api:
    image: elkozmon/zoonavigator-api:{{__ZOONAVIGATOR_API_version}}
    container_name: zoonavigator-api
    labels:
      com.platys.name: "zoonavigator"
      com.platys.description: "Zookeeper Navigator backend"
      com.platys.restapi.title: 'Zoonavigator REST API'
      com.platys.restapi.url: 'http://dataplatform:28101'
    ports:
     - "28101:9010"
    environment:
      API_HTTP_PORT: 9010
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {# ZOOKEEPER_NAVIGATOR_enable #}
{% endif %}   {# zookeeper_enable or KAFKA_enable or ATLAS_enable #}

{% if (KAFKA_enable) | default(false) %}
  {# make sure that internal replication factor is not larger than the number of Kafka nodes #}
  {% if KAFKA_internal_replication_factor > KAFKA_broker_nodes -%}
    {% set KAFKA_internal_replication_factor = KAFKA_broker_nodes -%}
  {% endif -%}
  #  ================================== Kafka ========================================== #
  {% for serviceName in (ns.serviceNames.split(',')) %}
    {% set port = ns.ports.split(',')[loop.index-1] %}
    {% if KAFKA_use_standard_port_for_external_interface | default(false) %}
      {% set externalPort = port %}
      {% set dockerHostPort = '2' ~ port %}
    {% else %}
      {% set externalPort = '2' ~ port %}
      {% set dockerHostPort = port %}
    {% endif %}
    {% set portJMX = ns.portsJMX.split(',')[loop.index-1] %}
    {% set portProm = ns.portsProm.split(',')[loop.index-1] %}
    {% set dcName = 'rack1' %}
    {% set zookeeperPort = 2181 + (dataCenterId * ZOOKEEPER_nodes) %}
  {{serviceName}}:
  {% if (KAFKA_edition | lower) == 'enterprise' or (KAFKA_edition | lower) == 'community' | default(false) %}
    image: confluentinc/{{ 'cp-server' if (KAFKA_edition | lower) == 'enterprise' else 'cp-kafka' }}:{{__CONFLUENT_PLATFORM_version}}
  {% else %}
    image: apache/kafka:{{__KAFKA_version}}
  {% endif %}
    container_name: {{serviceName}}
    hostname: {{serviceName}}
    labels:
      com.platys.name: "kafka"
      com.platys.description: "Kafka Broker {{loop.index}}"
    {% if not KAFKA_use_kraft_mode | default(false) %}
    depends_on:
      - zookeeper{{dataCenter}}-1
    {% endif %}
    ports:
      - "{{port}}:{{port}}"
      - "1{{port}}:1{{port}}"
      - "2{{port}}:2{{port}}"
      - "3{{port}}:3{{port}}"
      - "{{portJMX}}:{{portJMX}}"
      - "{{portProm}}:1234"
    environment:
      KAFKA_BROKER_ID: {{loop.index}}
      KAFKA_BROKER_RACK: '{{dcName}}'
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
    {% if KAFKA_use_kraft_mode | default(false) %}
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: '{{ns.controllerQuorumVoters}}'
      KAFKA_PROCESS_ROLES: {{ 'broker,controller' if loop.index <= 3 else 'broker' }}
      KAFKA_NODE_ID: {{loop.index}}
      CLUSTER_ID: {{KAFKA_cluster_id}}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:{{ns.securityProtocolCONTROLLER}},BROKER:{{ns.securityProtocolBROKER}},LOCAL:{{ns.securityProtocolLOCAL}},DOCKERHOST:{{ns.securityProtocolDOCKERHOST}},EXTERNAL:{{ns.securityProtocolEXTERNAL}}'
      KAFKA_LISTENERS: 'CONTROLLER://{{serviceName}}:4{{port}},BROKER://{{serviceName}}:1{{port}},LOCAL://{{serviceName}}:3{{port}},DOCKERHOST://{{serviceName}}:{{dockerHostPort}},EXTERNAL://{{serviceName}}:{{externalPort}}'
    {% else %}
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper{{dataCenter}}-1:{{zookeeperPort}}'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'BROKER:{{ns.securityProtocolBROKER}},LOCAL:{{ns.securityProtocolLOCAL}},DOCKERHOST:{{ns.securityProtocolDOCKERHOST}},EXTERNAL:{{ns.securityProtocolEXTERNAL}}'
      KAFKA_LISTENERS: 'BROKER://{{serviceName}}:1{{port}},LOCAL://{{serviceName}}:3{{port}},DOCKERHOST://{{serviceName}}:{{dockerHostPort}},EXTERNAL://{{serviceName}}:{{externalPort}}'
    {% endif %}
      KAFKA_ADVERTISED_LISTENERS: 'BROKER://{{serviceName}}:1{{port}},LOCAL://localhost:3{{port}},DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:{{dockerHostPort}},EXTERNAL://${PUBLIC_IP:-127.0.0.1}:{{externalPort}}'
      KAFKA_REPLICA_SELECTOR_CLASS: {{KAFKA_replica_selector_class | default(omit)}}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SASL_PLAINTEXT' %}
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: {{ns.saslMechanismBROKER}}
      {% if KAFKA_use_kraft_mode | default(false) %}
      KAFKA_SASL_ENABLED_MECHANISMS: {{[ns.saslMechanismCONTROLLER,ns.saslMechanismBROKER,ns.saslMechanismLOCAL,ns.saslMechanismDOCKERHOST,ns.saslMechanismEXTERNAL] | unique | join(',')}}
      KAFKA_SASL_MECHANISM_CONTROLLER_PROTOCOL: {{ns.saslMechanismCONTROLLER}}
      KAFKA_LISTENER_NAME_CONTROLLER_SASL_ENABLED_MECHANISMS: {{ns.saslMechanismCONTROLLER}}
      KAFKA_LISTENER_NAME_CONTROLLER_{{ns.saslMechanismCONTROLLER}}_SASL_JAAS_CONFIG: {{ns.loginModuleCONTROLLER}} required username="{{KAFKA_admin_username}}" password="{{KAFKA_admin_password}}" user_{{KAFKA_admin_username}}="{{KAFKA_admin_password}}";
      {% else %}
      KAFKA_SASL_ENABLED_MECHANISMS: {{[ns.saslMechanismBROKER,ns.saslMechanismLOCAL,ns.saslMechanismDOCKERHOST,ns.saslMechanismEXTERNAL] | unique | join(',')}}
      {% endif %}
      KAFKA_LISTENER_NAME_BROKER_SASL_ENABLED_MECHANISMS: {{ns.saslMechanismBROKER}}
      KAFKA_LISTENER_NAME_BROKER_{{ns.saslMechanismBROKER}}_SASL_JAAS_CONFIG: {{ns.loginModuleBROKER}} required username="{{KAFKA_admin_username}}" password="{{KAFKA_admin_password}}"
        {% for user in ns.users.split(",") %}
        user_{{user.split(':')[0]}}="{{user.split(':')[1]}}"
        {% endfor %}
        ;
      KAFKA_LISTENER_NAME_LOCAL_SASL_ENABLED_MECHANISMS: {{ns.saslMechanismLOCAL}}
      KAFKA_LISTENER_NAME_LOCAL_{{ns.saslMechanismLOCAL}}_SASL_JAAS_CONFIG: {{ns.loginModuleLOCAL}} required username="{{KAFKA_admin_username}}" password="{{KAFKA_admin_password}}"
        {% for user in ns.users.split(",") %}
        user_{{user.split(':')[0]}}="{{user.split(':')[1]}}"
        {% endfor %}
        ;
      KAFKA_LISTENER_NAME_DOCKERHOST_SASL_ENABLED_MECHANISMS: {{ns.saslMechanismDOCKERHOST}}
      KAFKA_LISTENER_NAME_DOCKERHOST_{{ns.saslMechanismDOCKERHOST}}_SASL_JAAS_CONFIG: {{ns.loginModuleDOCKERHOST}} required username="{{KAFKA_admin_username}}" password="{{KAFKA_admin_password}}"
        {% for user in ns.users.split(",") %}
        user_{{user.split(':')[0]}}="{{user.split(':')[1]}}"
        {% endfor %}
        ;
      KAFKA_LISTENER_NAME_EXTERNAL_SASL_ENABLED_MECHANISMS: {{ns.saslMechanismEXTERNAL}}
      KAFKA_LISTENER_NAME_EXTERNAL_{{ns.saslMechanismEXTERNAL}}_SASL_JAAS_CONFIG: {{ns.loginModuleEXTERNAL}} required username="{{KAFKA_admin_username}}" password="{{KAFKA_admin_password}}"
        {% for user in ns.users.split(",") %}
        user_{{user.split(':')[0]}}="{{user.split(':')[1]}}"
        {% endfor %}
        ;
      KAFKA_SASL_SERVER_CALLBACK_HANDLER_CLASS: {{KAFKA_sasl_server_callback_handler_class}}
      CONFLUENT_METRICS_REPORTER_SASL_MECHANISM: {{KAFKA_sasl_mechanism}}
      CONFLUENT_METRICS_REPORTER_SECURITY_PROTOCOL: {{KAFKA_security_protocol}}
      CONFLUENT_METRICS_REPORTER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required \
                username=\"{{KAFKA_tools_username}}\" \
                password=\"{{KAFKA_tools_password}}\";"
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf"
    {% elif ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      KAFKA_SSL_KEYSTORE_LOCATION: '/certs/server.{{serviceName}}.ks.p12'
      KAFKA_SSL_KEYSTORE_PASSWORD: 'abc123!'
      KAFKA_SSL_KEY_PASSWORD: 'abc123!'
      KAFKA_SSL_TRUSTSTORE_LOCATION: '/certs/server.ts.p12'
      KAFKA_SSL_TRUSTSTORE_PASSWORD: 'abc123!'
      {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KAFKA_SSL_CLIENT_AUTH: 'required'
      {% endif -%} {# KAFKA_ssl_client_authentication_enable #}
    {% endif -%}   {# ns.secureKafkaEnabled #}
    {% if KAFKA_authorizer_enable and ns.secureKafkaEnabled | default(false) %}
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "{{KAFKA_allow_everyone_if_no_acl_found | lower }}"
      {% if not KAFKA_use_kraft_mode | default(false) %}
      KAFKA_AUTHORIZER_CLASS_NAME: "kafka.security.authorizer.AclAuthorizer"
      {% else %}
      KAFKA_AUTHORIZER_CLASS_NAME: "org.apache.kafka.metadata.authorizer.StandardAuthorizer"
      {% endif -%} {# KAFKA_use_kraft_mode #}
      {% if KAFKA_super_users is defined and KAFKA_super_users and KAFKA_super_users | length %}
      KAFKA_SUPER_USERS: "{% for user in KAFKA_super_users.split(',') -%}User:{{ user }}{% if not loop.last %};{% endif %} {%- endfor %}"
      {% endif -%} {# KAFKA_super_users #}
    {% endif -%} {# KAFKA_authorizer_enable and ns.secureKafkaEnabled #}
    {% if KAFKA_CCC_enable and (KAFKA_edition | lower) == 'enterprise' | default(false) %}
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
    {% endif %}
    {% if (KAFKA_edition | lower) == 'enterprise' | default(false) %}
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      {% if KAFKA_confluent_log_placement_constraints is defined and KAFKA_confluent_log_placement_constraints %}
      KAFKA_CONFLUENT_LOG_PLACEMENT_CONSTRAINTS: '{{KAFKA_confluent_log_placement_constraints}}'
      {% endif %}
      {%if KAFKA_confluent_tier_enable | default(false) %}
      KAFKA_CONFLUENT_CLUSTER_LINK_ENABLE: '{{KAFKA_confluent_cluster_link_enable | default(omit) }}'
      {% endif -%} {# KAFKA_confluent_tier_enable #}
      {%if KAFKA_confluent_tier_enable | default(false) %}
      KAFKA_CONFLUENT_TIER_ENABLE: '{{KAFKA_confluent_tier_enable | default(omit) }}'
      KAFKA_CONFLUENT_TIER_FEATURE: '{{KAFKA_confluent_tier_feature | default(omit) }}'
      KAFKA_CONFLUENT_TIER_BACKEND: {{KAFKA_confluent_tier_backend | default(omit) }}
      KAFKA_CONFLUENT_TIER_S3_BUCKET: '{{KAFKA_confluent_tier_s3_bucket| default('kafka-logs') }}'
      KAFKA_CONFLUENT_TIER_S3_PREFIX: '{{KAFKA_confluent_tier_s3_prefix| default(omit) }}'
      KAFKA_CONFLUENT_TIER_S3_REGION: '{{KAFKA_confluent_tier_s3_region | default('us-east-1') }}'
      KAFKA_CONFLUENT_TIER_S3_AWS_ENDPOINT_OVERRIDE: '{{KAFKA_confluent_tier_s3_aws_endpoint_override | default(omit) }}'
      KAFKA_CONFLUENT_TIER_S3_FORCE_PATH_STYLE_ACCESS: '{{KAFKA_confluent_tier_s3_force_path_style_access | default(omit) }}'
      KAFKA_CONFLUENT_TIER_S3_SSE_ALGORITHM: "none"
      KAFKA_CONFLUENT_TIER_LOCAL_HOTSET_BYTES: {{KAFKA_confluent_tier_local_hotset_bytes | default(omit) }}
      KAFKA_CONFLUENT_TIER_LOCAL_HOTSET_MS: {{KAFKA_confluent_tier_local_hotset_ms | default(omit) }}
      KAFKA_CONFLUENT_TIER_ARCHIVER_NUM_THREADS: {{KAFKA_confluent_tier_archiver_num_threads | default(omit) }}
      KAFKA_CONFLUENT_TIER_FETCHER_NUM_THREADS: {{KAFKA_confluent_tier_fetcher_num_threads | default(omit) }}
      KAFKA_CONFLUENT_TIER_TOPIC_DELETE_CHECK_INTERVAL_MS: {{KAFKA_confluent_tier_topic_delete_check_interval_ms | default(omit) }}
      KAFKA_CONFLUENT_TIER_METADATA_REPLICATION_FACTOR: {{KAFKA_confluent_tier_metadata_replication_factor | default(1) }}
      #KAFKA_CONFLUENT_TIER_S3_AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      #KAFKA_CONFLUENT_TIER_S3_AWS_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
      {% endif -%} {# KAFKA_confluent_tier_enable #}
    {% endif -%} {# KAFKA_edition == ENTERPRISE #}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      KAFKA_MESSAGE_TIMESTAMP_TYPE: {{KAFKA_message_timestamp_type | default('CreateTime')}}
      KAFKA_LOG_DIRS: {{KAFKA_log_dirs if KAFKA_log_dirs is defined and KAFKA_log_dirs and KAFKA_log_dirs | length else omit }}
      KAFKA_COMPRESSION_TYPE: '{{KAFKA_compression_type | default(omit) }}'
      KAFKA_LOG_SEGMENT_BYTES: {{KAFKA_log_segment_bytes | default(omit) }}
      KAFKA_LOG_RETENTION_MS: {{KAFKA_log_retention_ms | default(omit) }}
      KAFKA_LOG_RETENTION_HOURS: {{KAFKA_log_retention_hours | default(omit) }}
      KAFKA_LOG_RETENTION_BYTES: {{KAFKA_log_retention_bytes | default(omit) }}
      KAFKA_MIN_INSYNC_REPLICAS: {{KAFKA_min_insync_replicas | default(1) }}
      KAFKA_DELETE_TOPIC_ENABLE: '{{ KAFKA_delete_topic_enable | default(omit) }}'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: '{{ KAFKA_auto_create_topics_enable | default(omit) }}'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: {{portJMX}}
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port={{portJMX}}'
      KAFKA_JMX_HOSTNAME: '${PUBLIC_IP:-127.0.0.1}'
    {% if (KAFKA_edition | lower) == 'enterprise' or (KAFKA_edition | lower) == 'community' | default(false) %}
      KAFKA_LOG4J_ROOT_LOGLEVEL: '{{KAFKA_log4j_root_level}}'
    {% endif -%} {# KAFKA_edition == enterprise or community #}
    {% if KAFKA_log4j_loggers is defined and KAFKA_log4j_loggers %}
      KAFKA_LOG4J_LOGGERS: '{{KAFKA_log4j_loggers}}'
    {% endif -%} {# KAFKA_log4j_loggers #}
      KAFKA_TOOLS_LOG4J_LOGLEVEL: '{{KAFKA_tools_log4j_level}}'
    {% if jmx_monitoring_with_prometheus_enable %}
      EXTRA_ARGS: -javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.20.0.jar=1234:/usr/share/jmx-exporter/kafka_broker.yml
    {% endif -%}
    {% if OPENLDAP_enable  | default(false) %}
      KAFKA_AUTHN_LDAP_HOST: openldap
      KAFKA_AUTHN_LDAP_PORT: 389
      KAFKA_AUTHN_LDAP_BASE_DN: dc=platysplatform,dc=io
      KAFKA_AUTHN_LDAP_USERNAME_TO_DN_FORMAT: "cn=%s,ou=users,dc=platysplatform,dc=io"
    {% endif -%}   {#  OPENLDAP_enable #}
    {% if KAFKA_unstable_api_enabled | default(false) %}
      KAFKA_UNSTABLE_API_VERSIONS_ENABLE: true
      KAFKA_GROUP_COORDINATOR_REBALANCE_PROTOCOLS: classic,consumer,share
    {% endif -%}   {#  KAFKA_unstable_api_enabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if KAFKA_additional_jars is defined and KAFKA_additional_jars | default(false) %}
      {% for jarName in (KAFKA_additional_jars.split(',')) %}
      - ./plugins/kafka/jars/{{jarName}}.jar:/usr/share/java/kafka/{{jarName}}.jar
      {% endfor %}
    {% endif %}
    {% if KAFKA_volume_map_data %}
      - "./container-volume/kafka/kafka-{{loop.index}}:/var/lib/kafka/data"
    {% endif %}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SASL_PLAINTEXT' %}
      {% if KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or KAFKA_sasl_mechanism == 'SCRAM-SHA-512' %}
      - ./security/kafka/sasl-scram/kafka.jaas.conf:/etc/kafka/kafka_server_jaas.conf
      - ./security/kafka/sasl-scram/client.properties:/tmp/client.properties
      - ./scripts/kafka/kraft/update_storage.sh:/tmp/kraft/update_storage.sh
      {% elif KAFKA_sasl_mechanism == 'OAUTHBEARER' %}
      - ./security/kafka/oauthbearer/kafka.jaas.conf:/etc/kafka/kafka_server_jaas.conf
      - ./security/kafka/oauthbearer/client.properties:/tmp/client.properties
      {% elif KAFKA_sasl_mechanism == 'PLAIN' %}
      - ./security/kafka/plain/kafka.jaas.conf:/etc/kafka/kafka_server_jaas.conf
      - ./security/kafka/plain/client.properties:/tmp/client.properties
      {% endif -%}
    {% elif ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/server-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {% if jmx_monitoring_with_prometheus_enable %}
      - ./scripts/kafka/jmx-exporter/:/usr/share/jmx-exporter
    {% endif -%}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    {% if KAFKA_use_kraft_mode and ns.secureKafkaEnabled and (KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or KAFKA_sasl_mechanism == 'SCRAM-SHA-512') | default(false) %}
    command: "bash -c '/tmp/kraft/update_storage.sh {{KAFKA_cluster_id}} {{ns.users}} {{KAFKA_sasl_mechanism}} {{KAFKA_security_debug | lower }} && /etc/confluent/docker/run ;'"
    {% endif -%}  {#  ns.secureKafkaEnabled and KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or 'SCRAM-SHA-512') #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
  {% endfor %} {# for serviceName in (ns.serviceNames) #}
{% endif %} {# KAFKA_enable #}

{% if (KAFKA_enable and not KAFKA_use_kraft_mode and ns.secureKafkaEnabled and (KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or KAFKA_sasl_mechanism == 'SCRAM-SHA-512')) | default(false) %}
  #  ================================== Kafka Sec(urity) Init ========================================== #
  kafka-sec-init:
    image: confluentinc/cp-kafka:{{__CONFLUENT_PLATFORM_version}}
    container_name: kafka-sec-init
    hostname: kafka-sec-init
    depends_on:
      - zookeeper-1
    command: "bash -c 'echo Waiting for Zookeeper to be ready... && \
                          cub zk-ready zookeeper-1:2181 120 && \
  {% for user in ns.users.split(",") %}
                          kafka-configs --zookeeper zookeeper-1:2181 --alter --add-config 'SCRAM-SHA-256=[password={{user.split(":")[1]}}],SCRAM-SHA-512=[password={{user.split(":")[1]}}]' --entity-type users --entity-name {{user.split(":")[0]}} {{ '&&' if not loop.last }} \
  {% endfor %}
                      '"
    environment:
      KAFKA_BROKER_ID: ignored
      KAFKA_ZOOKEEPER_CONNECT: ignored
    init: true
{% endif %} {# KAFKA_enable and not KAFKA_use_kraft_mode and ns.secureKafkaEnabled #}

{% if (KAFKA_enable or external['KAFKA_enable']) and KAFKA_INIT_enable | default(false) %}
  #  ================================== Kafka Init ========================================== #
  kafka-init:
    image: trivadis/kafka-init:{{__KAFKA_INIT_version}}
    container_name: kafka-init
    hostname: kafka-init
    depends_on:
      kafka-1:
        condition: service_started
    environment:
      BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      KAFKA_TIMEOUT: 120
      KAFKA_EXPECTED_BROKERS: {{KAFKA_broker_nodes}}
      KAFKA_DELETE_TOPIC_ENABLE: false
      KAFKA_TOPIC_CONFIG: ''
      KAFKA_CREATE_TOPICS: '{{KAFKA_INIT_topics}}'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    init: true
{% endif %} {# KAFKA_enable or external['KAFKA_enable'] and KAFKA_INIT #}

{% if (KAFKA_enable or external['KAFKA_enable']) and KAFKA_CLI_enable | default(false) %}
  #  ================================== Kafka CLI ========================================== #
  kafka-cli:
    image: confluentinc/cp-kafka:{{__CONFLUENT_PLATFORM_version}}
    container_name: kafka-cli
    hostname: kafka-cli
    labels:
      com.platys.name: "kafka"
      com.platys.description: "Kafka Command Line utilities"
    environment:
      BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
    {% if external['KAFKA_enable'] and external['KAFKA_sasl_mechanism'] is defined and external['KAFKA_sasl_mechanism'] | length %}
      SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";"
    {% endif -%}   {#  KAFKA_sasl_mechanism #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/kafka/config.properties:/tmp/config.properties
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - /bin/sh
      - -c
      - |
        eval "echo \"$(cat /tmp/config.properties)\"" >> config.properties
        while [ 1 -eq 1 ];do sleep 60;done
    restart: {{container_restart_policy}}
{% endif %} {# KAFKA_enable or external['KAFKA_enable'] and KAFKA_CLI_enable #}

{% if ( (KAFKA_enable or external['KAFKA_enable']) and KAFKA_CONNECT_enable ) | default(false) %}
  #  ================================== Kafka Connect ========================================== #
  {% for num in range(KAFKA_CONNECT_nodes | default(1) ) %}
    {% set external_port = 8083 + loop.index - 1 %}
    {% set port = 8083 + loop.index - 1 %}
  kafka-connect-{{loop.index}} :
    image: confluentinc/cp-kafka-connect:{{__CONFLUENT_PLATFORM_version}}
    container_name: kafka-connect-{{loop.index}}
    hostname: kafka-connect-{{loop.index}}
    labels:
      com.platys.name: "kafka-connect"
      com.platys.description: "Kafka Connect Node {{loop.index}}"
      com.platys.restapi.title: 'Kafka Connect REST API'
      com.platys.restapi.url: "http://dataplatform:{{external_port}}"
    ports:
      - "{{external_port}}:{{port}}"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      CONNECT_LISTENERS: 'http://0.0.0.0:{{port}}'
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-{{loop.index}}
      CONNECT_REST_ADVERTISED_PORT: {{port}}
      CONNECT_GROUP_ID: kafka-connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_ACCESS_CONTROL_ALLOW_ORIGIN: "*"
      CONNECT_ACCESS_CONTROL_ALLOW_METHODS: "GET,POST,PUT,DELETE"
      CONNECT_ACCESS_CONTROL_ALLOW_HEADERS: "origin,content-type,accept,authorization"
   {%if SCHEMA_REGISTRY_enable | default(false) %}
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
   {% else %}
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
   {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/addl-plugins,/etc/kafka-connect/cflthub-plugins"
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: 'All'
    {% if ns.secureKafkaEnabled %}
      CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      CONNECT_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      CONNECT_CONSUMER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      CONNECT_PRODUCER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      CONNECT_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONNECT_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"

      # Consumer
      CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      CONNECT_CONSUMER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      # Producer
      CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      CONNECT_PRODUCER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      CONNECT_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONNECT_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      CONNECT_CONSUMER_SASL_MECHANISM_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONNECT_CONSUMER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      CONNECT_PRODUCER_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONNECT_PRODUCER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      CONNECT_SSL_KEYSTORE_TYPE: "PKCS12"
      CONNECT_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONNECT_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONNECT_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      CONNECT_CONSUMER_SSL_KEYSTORE_TYPE: "PKCS12"
      CONNECT_CONSUMER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONNECT_CONSUMER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONNECT_CONSUMER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      CONNECT_PRODUCER_SSL_KEYSTORE_TYPE: "PKCS12"
      CONNECT_PRODUCER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONNECT_PRODUCER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONNECT_PRODUCER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% else -%}   {#  ns.secureKafkaEnabled #}
      CONNECT_SECURITY_PROTOCOL: "PLAINTEXT"
    {% endif -%}   {#  ns.secureKafkaEnabled #}
      #CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
      # External secrets config
      # See https://docs.confluent.io/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: '{{KAFKA_CONNECT_config_providers}}'
      CONNECT_CONFIG_PROVIDERS_{{KAFKA_CONNECT_config_providers | upper }}_CLASS: '{{KAFKA_CONNECT_config_providers_classes}}'
    {% if OTEL_COLLECTOR_enable | default(false) %}
      OTEL_RESOURCE_ATTRIBUTES: 'service.name=connect'
      KAFKA_OPTS: '-javaagent:/otel/opentelemetry-javaagent.jar -Dotel.exporter.otlp.endpoint=http://otel-collector:4317'
    {% endif -%}
    {% if jmx_monitoring_with_prometheus_enable %}
      EXTRA_ARGS: -javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.20.0.jar=1234:/usr/share/jmx-exporter/kafka_connect.yml
    {% endif -%}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/kafka-connect/jars:/etc/kafka-connect/jars
      - ./plugins/opentelemetry/agents:/otel
    {%if KAFKA_CONNECT_map_settings_file | default(false) %}
      - ./custom-conf/kafka-connect/settings.properties:/data/settings.properties
    {% endif -%}   {#  KAFKA_CONNECT_map_credentials_file #}
    {% if jmx_monitoring_with_prometheus_enable %}
      - ./scripts/kafka/jmx-exporter/:/usr/share/jmx-exporter
    {% endif -%}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
    {% if KAFKA_CONNECT_connectors is defined and KAFKA_CONNECT_connectors and KAFKA_CONNECT_connectors|length %}
        echo "Installing Connectors"
        mkdir -p /etc/kafka-connect/cflthub-plugins
        for i in $$(echo "{{KAFKA_CONNECT_connectors}}" | sed "s/,/ /g")
        do
          confluent-hub install --no-prompt --component-dir /etc/kafka-connect/cflthub-plugins --verbose "$$i"
        done
    {% endif -%}   {#  KAFKA_CONNECT_connectors #}
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --user connectAdmin:connectAdmin --fail --silent --insecure https://kafka-connect-{{loop.index}}:{{port}}/ --output /dev/null || exit 1
  {% endfor %}
{% endif %}   {# KAFKA_enable and KAFKA_CONNECT_enable  #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KSQLDB_enable )| default(false) %}
  #  ================================== ksqlDB ========================================== #
  {% for num in range(KSQLDB_nodes | default(1) ) %}
    {% set external_port = 8088 + loop.index - 1 %}
    {% set port = 8088 %}
    {% set jmx_external_port = 1095 + loop.index - 1 %}
    {% set jmx_port = 1095 %}
    {% set connect_external_port = 8083 + loop.index - 1 %}
    {% set connect_port = 8083 + loop.index - 1 %}
  ksqldb-server-{{loop.index}}:
    image: confluentinc/{{ 'ksqldb-server' if (KSQLDB_edition | lower) == 'oss' else 'cp-ksqldb-server' }}:{{__KSQLDB_version if (KSQLDB_edition | lower) == 'oss' else __CONFLUENT_PLATFORM_version}}
    hostname: ksqldb-server-{{loop.index}}
    container_name: ksqldb-server-{{loop.index}}
    labels:
      com.platys.name: "ksqldb"
      com.platys.description: "ksqlDB Streaming Database - Node {{loop.index}}"
      com.platys.restapi.title: 'ksqlDB Server REST API'
      com.platys.restapi.url: "http://dataplatform:{{external_port}}"
    ports:
      - "{{external_port}}:{{port}}"
      - "{{jmx_external_port}}:{{jmx_port}}"
    {% if ( KSQLDB_use_embedded_connect ) | default(false) %}
      - {{connect_external_port}}:{{connect_port}}
    {% endif %}
    environment:
    {% if ( KSQLDB_edition | lower) == 'cp'  | default(false) %}
      KSQL_LOG4J_ROOT_LOGLEVEL: "INFO"
      KSQL_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      KSQL_LOG4J_PROCESSING_LOG_TOPIC: '{{KSQLDB_log_topic}}'
      KSQL_LOG4J_PROCESSING_LOG_BROKERLIST: '{{ns.bootstrapServers}}'
    {% else %}
      KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/tmp/log4j.properties"
      KSQL_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      KSQL_LOG4J_PROCESSING_LOG_TOPIC: '{{KSQLDB_log_topic}}'
      KSQL_LOG4J_PROCESSING_LOG_BROKERLIST: '{{ns.bootstrapServers}}'
    {% endif %}
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: '{{KSQLDB_log_topic}}'
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: {{KSQLDB_internal_replication_factor | default(1)}}
      KSQL_KSQL_LOGGING_PROCESSING_ROWS_INCLUDE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'

      # For Demo purposes: improve resource utilization and avoid timeouts
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1

      KSQL_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      KSQL_KSQL_PERSISTENCE_DEFAULT_FORMAT_KEY: '{{KSQLDB_persistence_default_format_key | default(KAFKA) }}'
    {% if KSQLDB_persistence_default_format_value is defined and KSQLDB_persistence_default_format_value|length %}
      KSQL_KSQL_PERSISTENCE_DEFAULT_FORMAT_VALUE: '{{KSQLDB_persistence_default_format_value}}'
    {% endif %}

      KSQL_APPLICATION_ID: "ksqldb-cluster"
      KSQL_KSQL_SERVICE_ID: "ksqldb-cluster"
      KSQL_HOST_NAME: ksqldb-server-{{loop.index}}
      KSQL_LISTENERS: http://0.0.0.0:{{port}}
      KSQL_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_RESPONSE_HTTP_HEADERS_CONFIG: '{{KSQLDB_response_http_headers_config}}'
    {% if OTEL_COLLECTOR_enable | default(false) %}
      OTEL_RESOURCE_ATTRIBUTES: 'service.name=ksqldb'
      KSQL_OPTS: '-javaagent:/otel/opentelemetry-javaagent.jar -Dotel.exporter.otlp.endpoint=http://otel-collector:4317'
    {% endif -%}   {#  OTEL_COLLECTOR_enable #}
    {% if ns.secureKafkaEnabled %}
      KSQL_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      KSQL_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      KSQL_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      KSQL_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KSQL_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KSQL_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KSQLDB_kafka_username if KSQLDB_kafka_username is defined and KSQLDB_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KSQLDB_kafka_password if KSQLDB_kafka_password is defined and KSQLDB_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      # producer
      KSQL_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KSQL_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KSQLDB_kafka_username if KSQLDB_kafka_username is defined and KSQLDB_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KSQLDB_kafka_password if KSQLDB_kafka_password is defined and KSQLDB_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      # consumer
      KSQL_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KSQL_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KSQLDB_kafka_username if KSQLDB_kafka_username is defined and KSQLDB_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KSQLDB_kafka_password if KSQLDB_kafka_password is defined and KSQLDB_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      KSQL_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KSQL_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KSQL_SSL_KEYSTORE_TYPE: "PKCS12"
      KSQL_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KSQL_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KSQL_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
      KSQL_PRODUCER__SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KSQL_PRODUCER__SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
    {% else %}
      KSQL_SECURITY_PROTOCOL: "PLAINTEXT"
    {% endif -%}   {#  ns.secureKafkaEnabled #}

    {% if ( KAFKA_CONNECT_enable ) | default(false) %}
      KSQL_KSQL_CONNECT_URL: {{ns1.connectServersInternalUrl.split(',')[0]}}
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
      KSQL_KSQL_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
    {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
      KSQL_KSQL_SCHEMA_REGISTRY_BASIC_AUTH_CREDENTIALS_SOURCE: "USER_INFO"
      KSQL_KSQL_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO:  "{{ns.schemaRegistryUsername}}:{{ns.schemaRegistryPassword}}"
    {% endif %}
      KSQL_KSQL_INTERNAL_TOPIC_REPLICAS: {{KSQLDB_internal_replication_factor | default(1)}}
      KSQL_KSQL_SINK_REPLICAS: {{KSQLDB_internal_replication_factor | default(1)}}
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: {{KSQLDB_internal_replication_factor | default(1)}}
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: "true"
      KSQL_KSQL_HIDDEN_TOPICS: '^_.*,default_ksql_processing_log'
    {% if (KAFKA_edition | lower) == 'enterprise' and KAFKA_CCC_enable %}
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
    {% endif %}

      KSQL_KSQL_SUPPRESS_ENABLED: "{{KSQLDB_suppress_enabled | default(omit)}}"
      KSQL_KSQL_SUPPRESS_BUFFER_SIZE_BYTES: "{{KSQLDB_suppress_buffer_size_bytes | default(-1)}}"

      KSQL_KSQL_QUERY_PULL_TABLE_SCAN_ENABLED: "{{KSQLDB_query_pull_table_scan_enabled | default(omit)}}"
      KSQL_CONFIG_DIR: "/etc/ksqldb"
      KSQL_KSQL_EXTENSION_DIR: '/etc/ksqldb/ext/'

    {% if KSQLDB_queries_file is defined and KSQLDB_queries_file|length %}
      KSQL_KSQL_QUERIES_FILE: "{{KSQLDB_queries_file}}"
    {% endif -%}   {#  KSQLDB_queries_file #}
      KSQL_JMX_OPTS: "-Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port={{jmx_port}} -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port={{jmx_port}}"
    {% if jmx_monitoring_with_prometheus_enable %}
      KSQL_OPTS: '-javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.20.0.jar=1234:/usr/share/jmx-exporter/confluent_ksql.yml'
    {% endif -%}
    {% if KSQLDB_use_embedded_connect %}
      KSQL_KSQL_CONNECT_WORKER_CONFIG: "/etc/ksqldb/connect.properties"
      # Kafka Connect config below
      KSQL_CONNECT_BOOTSTRAP_SERVERS: "{{ns.bootstrapServers}}"
      KSQL_CONNECT_REST_ADVERTISED_HOST_NAME: 'ksqldb-server-{{loop.index}}'
      KSQL_CONNECT_REST_ADVERTISED_PORT: {{connect_port}}
      KSQL_CONNECT_GROUP_ID: ksqldb-kafka-connect-group-01
      KSQL_CONNECT_CONFIG_STORAGE_TOPIC: _ksqldb-kafka-connect-group-01-configs
      KSQL_CONNECT_OFFSET_STORAGE_TOPIC: _ksqldb-kafka-connect-group-01-offsets
      KSQL_CONNECT_STATUS_STORAGE_TOPIC: _ksqldb-kafka-connect-group-01-status
      KSQL_CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      KSQL_CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      KSQL_CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry-1:8081'
      KSQL_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: '1'
      KSQL_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: '1'
      KSQL_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: '1'
      KSQL_CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      KSQL_CONNECT_PLUGIN_PATH: '/usr/share/java,/etc/kafka-connect/addl-plugins,/etc/kafka-connect/cflthub-plugins'
      KSQL_CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: 'All'
      {% if ns.secureKafkaEnabled %}
      KSQL_CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      KSQL_CONNECT_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      KSQL_CONNECT_CONSUMER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      KSQL_CONNECT_PRODUCER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
        {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KSQL_CONNECT_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KSQL_CONNECT_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"

      # Consumer
      KSQL_CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      KSQL_CONNECT_CONSUMER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KSQL_CONNECT_CONSUMER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      # Producer
      KSQL_CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      KSQL_CONNECT_PRODUCER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KSQL_CONNECT_PRODUCER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
        {% elif ns.securityProtocolBROKER == 'SSL' %}
      KSQL_CONNECT_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KSQL_CONNECT_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      KSQL_CONNECT_CONSUMER_SASL_MECHANISM_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KSQL_CONNECT_CONSUMER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      KSQL_CONNECT_PRODUCER_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KSQL_CONNECT_PRODUCER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
          {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KSQL_CONNECT_SSL_KEYSTORE_TYPE: "PKCS12"
      KSQL_CONNECT_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KSQL_CONNECT_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KSQL_CONNECT_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      KSQL_CONNECT_CONSUMER_SSL_KEYSTORE_TYPE: "PKCS12"
      KSQL_CONNECT_CONSUMER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KSQL_CONNECT_CONSUMER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KSQL_CONNECT_CONSUMER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      KSQL_CONNECT_PRODUCER_SSL_KEYSTORE_TYPE: "PKCS12"
      KSQL_CONNECT_PRODUCER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KSQL_CONNECT_PRODUCER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KSQL_CONNECT_PRODUCER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
          {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
        {% endif -%}   {#  ns.securityProtocol #}
      {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% endif -%}   {#  KSQLDB_use_embedded_connect #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksqldb:/etc/ksqldb/ext
    {% if ( KSQLDB_edition | lower) == 'oss'  | default(false) %}
      - ./conf/ksqldb/etc/log4j.properties:/tmp/log4j.properties.templ
    {% endif -%}   {#  KSQLDB_edition = 'oss' #}
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/kafka-connect/jars:/etc/kafka-connect/jars
      - ./plugins/opentelemetry/agents:/otel
    {% if jmx_monitoring_with_prometheus_enable %}
      - ./scripts/kafka/jmx-exporter/:/usr/share/jmx-exporter
    {% endif -%}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
    {%if KSQLDB_use_embedded_connect and loop.index == 1 | default(false) %}
        echo "Installing Connectors"
          for i in $$(echo "{{KSQLDB_connect_connectors}}" | sed "s/,/ /g")
        do
          confluent-hub install --no-prompt --component-dir /etc/kafka-connect/cflthub-plugins --verbose "$$i"
        done
    {% endif %}
        python -c 'import os,sys; sys.stdout.write(os.path.expandvars(sys.stdin.read()))' < /tmp/log4j.properties.templ > /tmp/log4j.properties
        #
        echo "Launching ksqlDB server"
        #/etc/confluent/docker/run &
        /usr/bin/docker/run &
        #
        sleep infinity
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      start_period: 10s
      interval: 10s
      retries: 20
      test: curl --user ksqlDBUser:ksqlDBUser -fail --silent http://ksqldb-server-{{loop.index}}:{{port}}/info | grep RUNNING 1>/dev/null || exit 1
  {% endfor %}

  # Access the cli by running:
  # > docker exec -it ksqldb-cli ksql http://ksqldb-server-1:8088
  ksqldb-cli:
    image: confluentinc/ksqldb-cli:{{__KSQLDB_version}}
    container_name: ksqldb-cli
    hostname: ksqldb-cli
    labels:
      com.platys.name: "ksqldb-cli"
      com.platys.description: "ksqlDB Command Line Utility"
    depends_on:
      - ksqldb-server-1
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: /bin/sh
    tty: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_broker_enable and KSQLDB_enable #}

{% if (MATERIALIZE_enable) | default(false) %}
  #  ================================== Materialize ========================================== #
  materialize-1:
    image: materialize/materialized:{{__MATERIALIZE_version}}
    hostname: materialize-1
    container_name: materialize-1
    labels:
      com.platys.name: "materialize"
      com.platys.description: "Materialize Streaming DB"
    ports:
      - "6875:6875"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if (MATERIALIZE_CLI_enable) | default(false) %}
  mzcli:
    image: materialize/mzcli:latest
    hostname: mzcli
    container_name: mzcli
    labels:
      com.platys.name: "materialize"
      com.platys.description: "Materialize CLI"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    restart: {{container_restart_policy}}
  {% endif %}   {# MATERIALIZE_CLI_enable #}
{% endif %}   {# MATERIALIZE_enable #}

{% if (HSTREAMDB_enable) | default(false) %}
  #  ================================== HStreamDB ========================================== #
  hserver-1:
    image: hstreamdb/hstream:{{__HSTREAMDB_version}}
    hostname: hserver-1
    container_name: hserver-1
    labels:
      com.platys.name: "hstreamdb"
      com.platys.description: "HStreamDB Streaming Database"
    ports:
      - "6570:6570"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - bash
      - "-c"
      - |
        set -e
        /usr/local/script/wait-for-storage.sh hstore-1 6440 zookeeper-1 2181 600 \
        /usr/local/bin/hstream-server \
        --bind-address 0.0.0.0 --port 6570 \
        --internal-port 6571 \
        --server-id 100 \
        --seed-nodes "$$(hostname -I | awk '{print $$1}'):6571" \
        --advertised-address $$(hostname -I | awk '{print $$1}') \
        --metastore-uri zk://zookeeper-1:2181 \
        --store-config /data/store/logdevice.conf \
        --store-admin-host hstore-1 --store-admin-port 6440 \
        --store-log-level warning \
        --io-tasks-path /tmp/io/tasks \
        --io-tasks-network hstream-quickstart
    restart: {{container_restart_policy}}

  hstore-1:
    image: hstreamdb/hstream:{{__HSTREAMDB_version}}
    hostname: hstore-1
    container_name: hstore-1
    labels:
      com.platys.name: "hstreamdb"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - bash
      - "-c"
      - |
        set -ex
        # N.B. "enable-dscp-reflection=false" is required for linux kernel which
        # doesn't support dscp reflection, e.g. centos7.
        /usr/local/bin/ld-dev-cluster --root /data/store \
        --use-tcp --tcp-host $$(hostname -I | awk '{print $$1}') \
        --user-admin-port 6440 \
        --param enable-dscp-reflection=false \
        --no-interactive
    restart: {{container_restart_policy}}

{% endif %}   {# HSTOREDB_enable #}

{% if BENTHOS_enable | default(false) %}
  #  ================================== Benthos ========================================== #
  benthos-1:
    image: jeffail/benthos:{{__BENTHOS_version}}
    hostname: benthos-1
    container_name: benthos-1
    labels:
      com.platys.name: "benthos"
      com.platys.description: "Benthos Stream Processing"
      com.platys.webui.title: "Benthos UI"
      com.platys.webui.url: "http://dataplatform:4195"
    ports:
      - "4195:4195"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/benthos:/benthos
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: [ '-w', '-c', '/benthos.yaml', '-w' ]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if BENTHOS_SERVER_enable | default(false) %}
  benthos-server:
    image: jeffail/benthos:{{__BENTHOS_version}}
    hostname: benthos-server
    container_name: benthos-server
    labels:
      com.platys.name: "benthos"
      com.platys.webui.title: "Benthos Server UI"
      com.platys.webui.url: "http://dataplatform:4196"
    ports:
      - "4196:4196"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/benthos:/benthos
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ['blobl', 'server', '--no-open', '--host', '0.0.0.0', '--port', '4196']
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  BENTHOS_SERVER_enable #}
{% endif %}   {#  BENTHOS_enable #}

{% if RISINGWAVE_enable | default(false) %}
  #  ================================== RisingWave ========================================== #
  {% if RISINGWAVE_flavor == 'playground' | default(false) %}
  risingwave:
    image: risingwavelabs/risingwave:{{__RISINGWAVE_version}}
    hostname: risingwave
    container_name: risingwave
    labels:
      com.platys.name: "risingwave"
      com.platys.description: "RisingWave Streaming DB"
      com.platys.webui.title: "RisingWave UI"
      com.platys.webui.url: "http://dataplatform:5691"
    expose:
      - "4566"
      - "5691"
    ports:
      - "4566:4566"
      - "5691:5691"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: playground
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/4566; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5690; exit $$?;'
      interval: 1s
      timeout: 5s
  {% endif %}  {#  RISINGWAVE_flavor == 'playground' #}

  {% if RISINGWAVE_flavor == 'standalone' | default(false) %}
  risingwave:
    image: risingwavelabs/risingwave:{{__RISINGWAVE_version}}
    hostname: risingwave
    container_name: risingwave
    labels:
      com.platys.name: "risingwave"
      com.platys.description: "Risingwave Streaming DB"
      com.platys.webui.title: "Risingwave UI"
      com.platys.webui.url: "http://dataplatform:5691"
      com.platys.password.envvars: "PLATYS_RISINGWAVE_DATABASE_PASSWORD"
    depends_on:
      - minio-1
    expose:
      - "6660"
      - "4566"
      - "5688"
      - "5690"
      - "1250"
      - "5691"
    ports:
      - "4566:4566"
      - "5690:5690"
      - "5691:5691"
      - "1250:1250"
    environment:
      - RUST_BACKTRACE="1"
      # If ENABLE_TELEMETRY is not set, telemetry will start by default
      - ENABLE_TELEMETRY={{RISINGWAVE_telemetry_enable | default(false) }}
      - RW_TELEMETRY_TYPE="docker-compose"
      - RW_SECRET_STORE_PRIVATE_KEY_HEX=0123456789abcdef0123456789abcdef
      - RW_LICENSE_KEY=
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - RW_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - RW_IS_FORCE_PATH_STYLE={{s3PathStyleAccess | default(omit) }}
      - AWS_REGION={{s3DefaultRegion | default(omit) }}
      - AWS_ACCESS_KEY_ID={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AWS_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
    {% endif %}      
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/risingwave/risingwave.toml:/risingwave.toml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: "standalone \
                    --meta-opts=\" \
                      --listen-addr 0.0.0.0:5690 \
                      --advertise-addr 0.0.0.0:5690 \
                      --dashboard-host 0.0.0.0:5691 \
                      --prometheus-host 0.0.0.0:1250 \
    {%- if PROMETHEUS_enable | default(false) %}
                      --prometheus-endpoint http://prometheus-1:9090 \
    {% endif %}      
                      --backend sql \
    {%- if RISINGWAVE_meta_store_type | lower == 'sqlite' | default(false) %}
                      --sql-endpoint sqlite:///tmp/metadata.db?mode=rwc \
    {%- elif RISINGWAVE_meta_store_type | lower == 'postgresql' | default(false) %}      
                      --sql-endpoint postgres://{{RISINGWAVE_meta_store_username}}:${PLATYS_RISINGWAVE_DATABASE_PASSWORD:-{{RISINGWAVE_meta_store_password | default('abc123!') }}}@postgresql:5432/{{RISINGWAVE_meta_store_database}} \
    {%- elif RISINGWAVE_meta_store_type | lower == 'mysql' | default(false) %}      
                      --sql-endpoint mysql://{{RISINGWAVE_meta_store_username}}:${PLATYS_RISINGWAVE_DATABASE_PASSWORD:-{{RISINGWAVE_meta_store_password | default('abc123!') }}}@mysql:3306/{{RISINGWAVE_meta_store_database}} \
    {% endif %}      
    {%- if RISINGWAVE_state_store_type | lower == 's3' | default(false) %}
                      --state-store hummock+s3://{{RISINGWAVE_s3_bucket_name}} \
    {% endif %}      
                      --data-directory {{RISINGWAVE_data_directory}} \
                      --config-path /risingwave.toml\" \
                    --compute-opts=\" \
                      --config-path /risingwave.toml \
                      --listen-addr 0.0.0.0:5688 \
                      --prometheus-listener-addr 0.0.0.0:1250 \
                      --advertise-addr 0.0.0.0:5688 \
                      --async-stack-trace verbose \
                      --connector-rpc-endpoint 0.0.0.0:50051 \
                      --parallelism 8 \
                      --total-memory-bytes 16811696128 \
                      --role both \
                      --meta-address http://0.0.0.0:5690\" \
                    --frontend-opts=\" \
                      --config-path /risingwave.toml \
                      --listen-addr 0.0.0.0:4566 \
                      --advertise-addr 0.0.0.0:4566 \
                      --prometheus-listener-addr 0.0.0.0:1250 \
                      --health-check-listener-addr 0.0.0.0:6786 \
                      --meta-addr http://0.0.0.0:5690\" \
                    --compactor-opts=\" \
                      --listen-addr 0.0.0.0:6660 \
                      --prometheus-listener-addr 0.0.0.0:1250 \
                      --advertise-addr 0.0.0.0:6660 \
                      --meta-address http://0.0.0.0:5690\""
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    deploy:
      resources:
        limits:
          memory: 28G
        reservations:
          memory: 28G
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/6660; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5688; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/4566; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5690; exit $$?;'
      interval: 1s
      timeout: 5s
  {% endif %}  {#  RISINGWAVE_flavor == 'standalone' #}

  {% if RISINGWAVE_flavor == 'cluster' | default(false) %}
  {% for num in range(RISINGWAVE_compactor_nodes | default(1) ) %}  
  risingwave-compactor-{{loop.index}}:
    image: risingwavelabs/risingwave:{{__RISINGWAVE_version}}
    hostname: risingwave-compactor-{{loop.index}}
    container_name: risingwave-compactor-{{loop.index}}
    labels:
      com.platys.name: "risingwave"
      com.platys.description: "Risingwave Streaming DB"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G      
    depends_on:
      - risingwave-meta-1
    expose:
      - "6660"
      - "1260"
    environment:
      - RUST_BACKTRACE="1"
      # If ENABLE_TELEMETRY is not set, telemetry will start by default
      - ENABLE_TELEMETRY={{RISINGWAVE_telemetry_enable | default(false) }}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - RW_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - RW_IS_FORCE_PATH_STYLE={{s3PathStyleAccess | default(omit) }}
      - AWS_REGION={{s3DefaultRegion | default(omit) }}
      - AWS_ACCESS_KEY_ID={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AWS_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
    {% endif %}          
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/risingwave/risingwave.toml:/risingwave.toml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - compactor-node
      - "--listen-addr=0.0.0.0:6660"
      - "--advertise-addr=risingwave-compactor-1:6660"
      - "--prometheus-listener-addr=0.0.0.0:1260"
      - "--meta-address=http://risingwave-meta-1:5690"
      - "--config-path=/risingwave.toml"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/6660; exit $$?;'
      interval: 1s
      timeout: 5s
  {% endfor %}

  {% for num in range(RISINGWAVE_compute_nodes | default(1) ) %}
  risingwave-compute-{{loop.index}}:
    image: risingwavelabs/risingwave:{{__RISINGWAVE_version}}
    hostname: risingwave-compute-{{loop.index}}
    container_name: risingwave-compute-{{loop.index}}
    labels:
      com.platys.name: "risingwave"
      com.platys.description: "Risingwave Streaming DB"
    deploy:
      resources:
        limits:
          memory: {{RISINGWAVE_compute_memory | default('4G')}}
        reservations:
          memory: {{RISINGWAVE_compute_memory | default('4G')}}      
    depends_on:
      - risingwave-meta-1
    expose:
      - "5688"
      - "1222"
    environment:
      - RUST_BACKTRACE="1"
      # If ENABLE_TELEMETRY is not set, telemetry will start by default
      - ENABLE_TELEMETRY={{RISINGWAVE_telemetry_enable | default(false) }}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - RW_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - RW_IS_FORCE_PATH_STYLE={{s3PathStyleAccess | default(omit) }}
      - AWS_REGION={{s3DefaultRegion | default(omit) }}
      - AWS_ACCESS_KEY_ID={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AWS_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
    {% endif %}          
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/risingwave/risingwave.toml:/risingwave.toml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - compute-node
      - "--listen-addr=0.0.0.0:5688"
      - "--advertise-addr=risingwave-compute-1:5688"      
      - "--prometheus-listener-addr=0.0.0.0:1222"
      - "--meta-address=http://risingwave-meta-1:5690"
      - "--config-path=/risingwave.toml"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5690; exit $$?;'
      interval: 1s
      timeout: 5s
  {% endfor %}

  risingwave-frontend-1:
    image: risingwavelabs/risingwave:{{__RISINGWAVE_version}}
    hostname: risingwave-frontend-1
    container_name: risingwave-frontend-1
    labels:
      com.platys.name: "risingwave"
      com.platys.description: "Risingwave Streaming DB"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G      
    depends_on:
      - risingwave-meta-1
    expose:
      - "4566"
    ports:
      - "4566:4566"
    environment:
      - RUST_BACKTRACE="1"
      # If ENABLE_TELEMETRY is not set, telemetry will start by default
      - ENABLE_TELEMETRY={{RISINGWAVE_telemetry_enable | default(false) }}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/risingwave/risingwave.toml:/risingwave.toml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - frontend-node
      - "--listen-addr=0.0.0.0:4566"
      - "--meta-addr=http://risingwave-meta-1:5690"
      - "--advertise-addr=risingwave-frontend-1:4566"
      - "--config-path=/risingwave.toml"
      - "--prometheus-listener-addr=0.0.0.0:2222"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/4566; exit $$?;'
      interval: 1s
      timeout: 5s

  risingwave-meta-1:
    image: risingwavelabs/risingwave:{{__RISINGWAVE_version}}
    hostname: risingwave-meta-1
    container_name: risingwave-meta-1
    labels:
      com.platys.name: "risingwave"
      com.platys.description: "Risingwave Streaming DB"
      com.platys.webui.title: "Risingwave UI"
      com.platys.webui.url: "http://dataplatform:5691"
    expose:
      - "5690"
      - "1250"
      - "5691"
    ports:
      - "5690:5690"
      - "5691:5691"
    environment:
      - RUST_BACKTRACE="1"
      # If ENABLE_TELEMETRY is not set, telemetry will start by default
      - ENABLE_TELEMETRY={{RISINGWAVE_telemetry_enable | default(false) }}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - RW_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - RW_IS_FORCE_PATH_STYLE={{s3PathStyleAccess | default(omit) }}
      - AWS_REGION={{s3DefaultRegion | default(omit) }}
      - AWS_ACCESS_KEY_ID={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AWS_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
    {% endif %}      
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/risingwave/risingwave.toml:/risingwave.toml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - meta-node
      - "--listen-addr=0.0.0.0:5690"
      - "--advertise-addr=risingwave-meta-1:5690"
      - "--dashboard-host=0.0.0.0:5691"
      - "--prometheus-host=0.0.0.0:1250"
    {%- if PROMETHEUS_enable | default(false) %}
      - "--prometheus-endpoint"
      - "http://prometheus-1:9090"      
    {% endif %}      
      - "--backend=sql"      
    {% if RISINGWAVE_meta_store_type | lower == 'sqlite' | default(false) %}
      - "--sql-endpoint=sqlite:///tmp/metadata.db?mode=rwc"
    {%- elif RISINGWAVE_meta_store_type | lower == 'postgresql' | default(false) %}      
      - "--sql-endpoint=postgres://{{RISINGWAVE_meta_store_username}}:${PLATYS_RISINGWAVE_DATABASE_PASSWORD:-{{RISINGWAVE_meta_store_password | default('abc123!') }}}@postgresql:5432/{{RISINGWAVE_meta_store_database}}"
    {%- elif RISINGWAVE_meta_store_type | lower == 'mysql' | default(false) %}      
      - "--sql-endpoint=mysql://{{RISINGWAVE_meta_store_username}}:${PLATYS_RISINGWAVE_DATABASE_PASSWORD:-{{RISINGWAVE_meta_store_password | default('abc123!') }}}@mysql:3306/{{RISINGWAVE_meta_store_database}}"
    {% endif %}      
    {% if RISINGWAVE_state_store_type | lower == 's3' | default(false) %}
      - "--state-store=hummock+s3://{{RISINGWAVE_s3_bucket_name}}"
    {% endif %}            
      - "--data-directory={{RISINGWAVE_data_directory}}"
      - "--config-path=/risingwave.toml"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5690; exit $$?;'
      interval: 1s
      timeout: 5s
  {% endif %}  {#  RISINGWAVE_flavor == 'cluster' #}
{% endif %}   {#  RISINGWAVE_enable #}

{% if TIMEPLUS_enable | default(false) %}
  #  ================================== Timeplus OSS (Proton) or Enterprise ========================================== #
  timeplus:
  {% if (TIMEPLUS_edition | lower) == 'enterprise'  %}
    image: timeplus/timeplus-enterprise:{{__TIMEPLUS_ENTERPRISE_version}}
  {% elif (TIMEPLUS_edition | lower) == 'oss' %}
    image: d.timeplus.com/timeplus-io/proton:{{__TIMEPLUS_PROTON_version}}
  {% endif -%}   {#  TIMEPLUS_edition #}    
    hostname: timeplus
    container_name: timeplus
    cap_add:
      - SYS_PTRACE    
    labels:
      com.platys.name: "timeplus"
      com.platys.description: "Timeplus Streaming DB"
  {% if (TIMEPLUS_edition | lower) == 'enterprise'  %}
      com.platys.webui.title: "Timeplus UI"
      com.platys.webui.url: "http://dataplatform:28421"  
  {% endif -%}   {#  TIMEPLUS_edition #}    
    ports:
  {% if (TIMEPLUS_edition | lower) == 'enterprise'  %}
      - "28421:8000" # App (only for Enterprise)
  {% endif -%}   {#  TIMEPLUS_edition #}    
      - "8463:8463" # TCP (Streaming)
      - "8123:8123" # HTTP (Batch)
      - "3218:3218" # HTTP (Streaming)
      - "7587:7587" # TCP (Batch)
      - "5440:5432" # Postgres (Batch)    
      - "9363:9363" # Prometheus Metrics  
    environment:
  {% if (TIMEPLUS_edition | lower) == 'enterprise'  %}
      - ENABLE_AUTHENTICATION=true
      - TIMEPLUSD_ADMIN_PASSWORD=abc123!    
  {% endif -%}   {#  TIMEPLUS_edition #}    
      - STREAM_STORAGE_BROKERS={{ns.bootstrapServers}}
      - STREAM_STORAGE_QUEUE_BUFFERING_MAX_MS=50 # Control producer latency
      - STREAM_STORAGE_FETCH_WAIT_MAX_MS=500 # Control consumer latency
      - STREAM_STORAGE_TYPE=kafka # log type
      - STREAM_STORAGE_CLUSTER_ID=default-sys-kafka-cluster-id
      - STREAM_STORAGE_SECURITY_PROTOCOL=PLAINTEXT
      - ENABLE_LOG_STREAM=true # log stream    
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
  {% if (TIMEPLUS_edition | lower) == 'oss'  %}
    command: >
      /bin/bash -c "sleep 2; /entrypoint.sh"    
  {% endif -%}   {#  TIMEPLUS_edition #}    
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "curl", "http://localhost:3218/timeplusd/ping"]
      interval: 2s
      timeout: 10s
      retries: 3
      start_period: 10s    
{% endif %}  {#  TIMEPLUS_enable #}

{% if ARROYO_enable | default(false) %}
  #  ================================== Arroyo ========================================== #
  arroyo:
    image: ghcr.io/arroyosystems/arroyo-single:{{__ARROYO_version}}
    hostname: arroyo
    container_name: arroyo
    labels:
      com.platys.name: "arroyo"
      com.platys.description: "Arroyo Streaming DB"
      com.platys.webui.title: "Arroyo UI"
      com.platys.webui.url: "http://dataplatform:5115"
      com.platys.restapi.title: "Arroyo REST API"
      com.platys.restapi.url: "http://dataplatform:5115/api/v1/connectors"
    ports:
      - "5115:5115"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5115/api/v1/ping"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 5s
{% endif %}  {#  ARROYO_enable #}

{% if SQLFLOW_enable | default(false) %}
  #  ================================== SqlFlow ========================================== #
  sqlflow:
    image: turbolytics/sql-flow:{{__SQLFLOW_version}}
    hostname: sqlflow
    container_name: sqlflow
    labels:
      com.platys.name: "sqlflow"
      com.platys.description: "SQLFlow Streaming DB"
    environment:
      - SQLFLOW_LOG_LEVEL=INFO
    {%if KAFKA_enable or external['KAFKA_enable'] | default(false) %}
      - SQLFLOW_KAFKA_BOOTSTRAP_SERVERS='{{ns.bootstrapServers}}'
    {% endif -%}   {#  KAFKA_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/sqlflow:/tmp/scripts
    {%if SQLFLOW_volume_map_data | default(false) %}  
      - ./container-volume/sqlflow:/tmp/sqlflow
    {% endif -%}   {#  SQLFLOW_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: 
    {%- if SQLFLOW_scripts is defined and SQLFLOW_scripts and SQLFLOW_scripts | length | default(false) -%}
    {{' '}}run 
      {%- for script in SQLFLOW_scripts.split(",") -%}
        {{" "}}/tmp/scripts/{{script}}
      {%- endfor %}      
    {% endif -%}   {#  SQLFLOW_scripts #}
    restart: {{container_restart_policy}}
{% endif %}  {#  SQLFLOW_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_RESTPROXY_enable) | default(false) %}
  #  ================================== Confluent REST Proxy ========================================== #
  kafka-rest-1:
    image: confluentinc/cp-kafka-rest:{{__CONFLUENT_PLATFORM_version}}
    container_name: kafka-rest-1
    hostname: kafka-rest-1
    labels:
      com.platys.name: "kafka-rest"
      com.platys.restapi.title: 'Kafka REST Proxy'
      com.platys.restapi.url: "http://dataplatform:18086"
    ports:
      - "18086:8086"
    environment:
      KAFKA_REST_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      KAFKA_REST_LISTENERS: 'http://0.0.0.0:8086'
      KAFKA_REST_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
      KAFKA_REST_HOST_NAME: 'kafka-rest-1'
      KAFKA_REST_ACCESS_CONTROL_ALLOW_ORIGIN: "*"
      KAFKA_REST_ACCESS_CONTROL_ALLOW_METHODS: "GET,POST,PUT,DELETE"
      KAFKA_REST_ACCESS_CONTROL_ALLOW_HEADERS: "origin,content-type,accept,authorization"
    {% if ns.secureKafkaEnabled %}
      KAFKA_REST_CLIENT_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KAFKA_REST_CLIENT_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KAFKA_REST_CLIENT_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_RESTPROXY_kafka_username if KAFKA_RESTPROXY_kafka_username is defined and KAFKA_RESTPROXY_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_RESTPROXY_kafka_password if KAFKA_RESTPROXY_kafka_password is defined and KAFKA_RESTPROXY_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      KAFKA_REST_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KAFKA_REST_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KAFKA_REST_SSL_KEYSTORE_TYPE: "PKCS12"
      KAFKA_REST_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KAFKA_REST_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KAFKA_REST_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if jmx_monitoring_with_prometheus_enable %}
      KAFKAREST_OPTS: -javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.20.0.jar=1234:/usr/share/jmx-exporter/confluent_rest.yml
    {% endif -%}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if jmx_monitoring_with_prometheus_enable %}
      - ./scripts/kafka/jmx-exporter/:/usr/share/jmx-exporter
    {% endif -%}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_enable and KAFKA_RESTPROXY_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_MQTTPROXY_enable) | default(false) %}
  #  ================================== Confluent MQTT Proxy ========================================== #
  kafka-mqtt-1:
    image: confluentinc/cp-kafka-mqtt:{{__CONFLUENT_PLATFORM_version}}
    container_name: kafka-mqtt-1
    hostname: kafka-mqtt-1
    labels:
      com.platys.name: "kafka-mqtt"
      com.platys.description: "Confluent MQTT Proxy"
    ports:
      - "1882:1882"
    environment:
      KAFKA_MQTT_TOPIC_REGEX_LIST: {{KAFKA_MQTTPROXY_topic_regex_list}}
      KAFKA_MQTT_LISTENERS: 0.0.0.0:1882
      KAFKA_MQTT_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      KAFKA_MQTT_CONFLUENT_TOPIC_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
    {% if ns.secureKafkaEnabled %}
      KAFKA_MQTT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      KAFKA_MQTT_PRODUCER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KAFKA_MQTT_PRODUCER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KAFKA_MQTT_PRODUCER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_MQTTPROXY_kafka_username if KAFKA_MQTTPROXY_kafka_username is defined and KAFKA_MQTTPROXY_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_MQTTPROXY_kafka_password if KAFKA_MQTTPROXY_kafka_password is defined and KAFKA_MQTTPROXY_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      KAFKA_MQTT_PRODUCER_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KAFKA_MQTT_PRODUCER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KAFKA_MQTT_PRODUCER_SSL_KEYSTORE_TYPE: "PKCS12"
      KAFKA_MQTT_PRODUCER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KAFKA_MQTT_PRODUCER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KAFKA_MQTT_PRODUCER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}

    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_enable and KAFKA_MQTTPROXY_enable #}

{% if ZILLA_enable | default(false) %}
  #  ================================== Zilla ========================================== #
  zilla:
    image: ghcr.io/aklivity/zilla:{{__ZILLA_version}}
    container_name: zilla
    hostname: zilla
    labels:
      com.platys.name: "zilla"
      com.platys.description: "Zilla Multi-Protocol Proxy"
      com.platys.restapi.title: 'Zilla'
      com.platys.restapi.url: "http://dataplatform:28277"
    ports:
      - "28277:8080"
      - "28278:9090"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: [ "start", "-v", "-e" ]
    restart: {{container_restart_policy}}
{% endif %}   {# ZILLA_enable #}

{% if (SCHEMA_REGISTRY_enable and SCHEMA_REGISTRY_flavour | lower) == 'confluent' | default(false) %}
  #  ================================== Confluent Schema Registry ========================================== #
  {% for num in range(SCHEMA_REGISTRY_nodes | default(1) ) %}
    {% set external_port = 8081 + loop.index - 1 %}
    {% set port = 8081 + loop.index - 1 %}
  schema-registry-{{loop.index}}:
    image: confluentinc/cp-schema-registry:{{__CONFLUENT_PLATFORM_version}}
    hostname: schema-registry-{{loop.index}}
    container_name: schema-registry-{{loop.index}}
    labels:
      com.platys.name: "schema-registry"
      com.platys.description: "Confluent Schema Registry"
      com.platys.restapi.title: 'Schema Registry REST API'
      com.platys.restapi.url: "http://dataplatform:{{external_port}}"
    ports:
      - "{{external_port}}:{{port}}"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-{{loop.index}}
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:{{port}}"
    {% if CONFLUENT_SCHEMA_REGISTRY_use_zookeeper_election %}
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper-1:2181'
    {% endif %}
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      SCHEMA_REGISTRY_GROUP_ID: "{{CONFLUENT_SCHEMA_REGISTRY_group_id}}"
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: "{{CONFLUENT_SCHEMA_REGISTRY_leader_eligibility}}"
      SCHEMA_REGISTRY_MODE_MUTABILITY: "{{CONFLUENT_SCHEMA_REGISTRY_mode_mutability}}"
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: "{{CONFLUENT_SCHEMA_REGISTRY_schema_compatibility_level}}"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: {{CONFLUENT_SCHEMA_REGISTRY_replication_factor}}
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: 'GET,POST,PUT,OPTIONS'
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: "{{CONFLUENT_SCHEMA_REGISTRY_log4j_root_loglevel}}"
      SCHEMA_REGISTRY_DEBUG: "{{CONFLUENT_SCHEMA_REGISTRY_debug}}"
    {% if ns.secureKafkaEnabled %}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{SCHEMA_REGISTRY_kafka_username if SCHEMA_REGISTRY_kafka_username is defined and SCHEMA_REGISTRY_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{SCHEMA_REGISTRY_kafka_password if SCHEMA_REGISTRY_kafka_password is defined and SCHEMA_REGISTRY_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEYSTORE_TYPE: "PKCS12"
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% else %}
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: "PLAINTEXT"
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if OTEL_COLLECTOR_enable | default(false) %}
      OTEL_RESOURCE_ATTRIBUTES: 'service.name=schema-registry'
      SCHEMA_REGISTRY_OPTS: '-javaagent:/otel/opentelemetry-javaagent.jar -Dotel.exporter.otlp.endpoint=http://otel-collector:4317'
    {% endif -%}
    {% if jmx_monitoring_with_prometheus_enable %}
      EXTRA_ARGS: -javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.20.0.jar=1234:/usr/share/jmx-exporter/confluent_schemaregistry.yml
    {% endif -%}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/opentelemetry/agents:/otel
    {% if jmx_monitoring_with_prometheus_enable %}
      - ./scripts/kafka/jmx-exporter/:/usr/share/jmx-exporter
    {% endif -%}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      start_period: 10s
      interval: 10s
      retries: 20
      test: curl --user superUser:superUser --fail --silent --insecure https://schema-registry-{{loop.index}}:{{port}}/subjects --output /dev/null || exit 1
  {% endfor %}
{% endif %}   {# KAFKA_enable and SCHEMA_REGISTRY_flavour == confluent #}

{% if (SCHEMA_REGISTRY_enable and SCHEMA_REGISTRY_flavour | lower) == 'apicurio' | default(false) %}
  #  ================================== Apicurio Schema Registry ========================================== #
  {% for num in range(SCHEMA_REGISTRY_nodes | default(1) ) %}
    {% set http_external_port = 8081 + loop.index - 1 %}
    {% set http_port = 8080 + loop.index - 1 %}
  apicurio-registry-{{loop.index}}:
    image: apicurio/apicurio-registry-{{APICURIO_SCHEMA_REGISTRY_storage}}:{{__APICURIO_SCHEMA_REGISTRY_version}}
    hostname: apicurio-registry-{{loop.index}}
    container_name: apicurio-registry-{{loop.index}}
    labels:
      com.platys.name: "apicurio-registry"
      com.platys.description: "Apicurio Schema Registry"
      com.platys.webui.title: 'Apicurio Registry UI'
      com.platys.webui.url: "http://dataplatform:{{http_external_port}}"
      com.platys.restapi.title: 'Apicurio Registry REST API (Confluent compliant)'
      com.platys.restapi.url: "http://dataplatform:{{http_external_port}}/apis/ccompat/v6"
    ports:
      - "{{http_external_port}}:{{http_port}}"
    environment:
      QUARKUS_PROFILE: prod
    {% if (APICURIO_SCHEMA_REGISTRY_storage | lower) == 'kafkasql'  %}
      KAFKA_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      REGISTRY_KAFKASQL_TOPIC: 'kafkasql-journal'
    {% endif -%}   {#  APICURIO_SCHEMA_REGISTRY_storage == sql #}
    {% if (APICURIO_SCHEMA_REGISTRY_storage | lower) == 'sql'  %}
      REGISTRY_DATASOURCE_URL: jdbc:postgresql://postgresql/{{APICURIO_SCHEMA_REGISTRY_sql_storage_database}}
      REGISTRY_DATASOURCE_USERNAME: {{APICURIO_SCHEMA_REGISTRY_sql_storage_user}}
      REGISTRY_DATASOURCE_PASSWORD: {{APICURIO_SCHEMA_REGISTRY_sql_storage_password}}
    {% endif -%}   {#  APICURIO_SCHEMA_REGISTRY_storage == sql #}
    {% if APICURIO_auth_enabled | default(false) %}
      AUTH_ENABLED: '{{APICURIO_auth_enabled}}'
      KEYCLOAK_URL: 'http://${PUBLIC_IP}:28204/auth'
      KEYCLOAK_REALM: 'registry'
      KEYCLOAK_API_CLIENT_ID: 'registry-api'
      KEYCLOAK_UI_CLIENT_ID: 'apicurio-registry'
      REGISTRY_AUTH_ANONYMOUS_READ_ACCESS_ENABLED: '{{APICURIO_auth_anonymous_read_access_enabled}}'
      ROLE_BASED_AUTHZ_ENABLED: 'True'
      ROLE_BASED_AUTHZ_SOURCE: 'token'
#     OWNER_ONLY_AUTHZ_ENABLED
#     REGISTRY_AUTH_ADMIN_OVERRIDE_ENABLED: 'True'
#     REGISTRY_AUTH_ADMIN_OVERRIDE_ROLE: ''
#     CLIENT_CREDENTIALS_BASIC_AUTH_ENABLED: 'True'
      {% if APICURIO_basic_auth_enabled | default(false) %}
      CLIENT_CREDENTIALS_BASIC_AUTH_ENABLED: 'True'
      {% endif -%}   {#  APICURIO_basic_auth_enabled #}
    {% endif -%}   {#  APICURIO_auth_enabled #}
    {% if APICURIO_eventsourcing_enabled | default(false) %}
      {% if (APICURIO_eventsourcing_transport | lower) == 'kafka'  %}
      KAFKA_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      REGISTRY_EVENTS_KAFKA_TOPIC: '{{APICURIO_eventsourcing_kafka_topic}}'
      {% else -%}   {#  APICURIO_eventsourcing_type #}
      REGISTY_EVENTS_SINK_CONSUMER: '{{APICURIO_eventsourcing_http_endpoint}}'
      {% endif -%}   {#  APICURIO_eventsourcing_type #}
    {% endif -%}   {#  APICURIO_eventsourcing_enabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {# KAFKA_enable and SCHEMA_REGISTRY_flavour == apicurio #}

{% if (SCHEMA_REGISTRY_enable and SCHEMA_REGISTRY_UI_enable) | default(false) %}
  #  ================================== Schema Registry UI ========================================== #
  schema-registry-ui:
    image: landoop/schema-registry-ui:{{__SCHEMA_REGISTRY_UI_version}}
    container_name: schema-registry-ui
    hostname: schema-registry-ui
    labels:
      com.platys.name: "schema-registry-ui"
      com.platys.description: "Schema Registry UI"
      com.platys.webui.title: 'Confluent Schema Registry UI'
      com.platys.webui.url: "http://dataplatform:28102"
    ports:
      - "28102:8000"
    environment:
      SCHEMAREGISTRY_URL: '{{ns.schemaRegistryUrlExternal}}'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if SCHEMA_REGISTRY_UI_map_resolv_conf | default(false) %}
      - ./conf/schema-registry-ui/resolv.conf:/etc/resolv.conf:ro
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# SCHEMA_REGISTRY_enable and SCHEMA_REGISTRY_UI_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and (KAFKA_edition | lower) == 'enterprise' and KAFKA_CCC_enable) | default(false) %}
  #  ================================== Confluent Control Center ========================================== #
  {% set externalPort = 9021 + dataCenterId %}
  control-center{{dataCenter}}:
    image: confluentinc/cp-enterprise-control-center:{{__CONFLUENT_PLATFORM_version}}
    hostname: control-center{{dataCenter}}
    container_name: control-center{{dataCenter}}
    labels:
      com.platys.name: "confluent-control-center"
      com.platys.description: "Confluent Control Center"
      com.platys.webui.title: 'Control Center UI'
      com.platys.webui.url: "http://dataplatform:{{externalPort}}"
    ports:
      - "{{externalPort}}:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
  {% if ns.secureKafkaEnabled %}
      CONTROL_CENTER_STREAMS_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
    {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      CONTROL_CENTER_STREAMS_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONTROL_CENTER_STREAMS_SASL_MECHANISM: "{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";"
    {% elif ns.securityProtocolBROKER == 'SSL' %}
      CONTROL_CENTER_STREAMS_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONTROL_CENTER_STREAMS_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      CONTROL_CENTER_STREAMS_SSL_KEYSTORE_TYPE: "PKCS12"
      CONTROL_CENTER_STREAMS_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONTROL_CENTER_STREAMS_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONTROL_CENTER_STREAMS_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
    {% endif -%}   {#  ns.securityProtocolBROKER #}
  {% endif -%}   {#  ns.secureKafkaEnabled #}
  {% if ( KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect) | default(false) %}
      CONTROL_CENTER_CONNECT_DEMO_CLUSTER: '${DOCKER_HOST_IP}:8083'
  {% endif %}
  {% if ( KSQLDB_enable ) | default(false) %}
      CONTROL_CENTER_KSQL_DEMO_URL: "http://ksqldb-server-1:8088"
      CONTROL_CENTER_KSQL_DEMO_ADVERTISED_URL: "http://dataplatform:8088"
  {% endif %}
      CONTROL_CENTER_SCHEMA_REGISTRY_ENABLE: 'true'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
      CONTROL_CENTER_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: {{KAFKA_internal_replication_factor | default(1)}}
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: {{KAFKA_internal_replication_factor | default(1)}}
      CONFLUENT_METRICS_TOPIC_REPLICATION: {{KAFKA_internal_replication_factor | default(1)}}
      PORT: 9021
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_enable and KAFKA_CCC_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and (KAFKA_edition | lower) == 'enterprise' and KAFKA_REPLICATOR_enable) | default(false) %}
  replicator-1:
    image: confluentinc/cp-enterprise-replicator:{{__CONFLUENT_PLATFORM_version}}
    container_name: replicator-1
    hostname: replicator-1
    labels:
      com.platys.name: "confluent-replicator"
      com.platys.description: "Confluent Replicator"
    ports:
      - 18083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'replicator-1'
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-replicator
      CONNECT_CONFIG_STORAGE_TOPIC: _replicator-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _replicator-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _replicator-status
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
      CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_INTERNAL_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_LOG4J_ROOT_LOGLEVEL: 'INFO'
      CONNECT_LOG4J_LOGGERS: 'org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR'
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: {{KAFKA_internal_replication_factor | default(1)}}
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components/'
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: 'All'
    {% if ns.secureKafkaEnabled %}
      CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      CONNECT_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      CONNECT_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONNECT_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      # Consumer
      CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      CONNECT_CONSUMER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      CONNECT_CONSUMER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      # Producer
      CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      CONNECT_PRODUCER_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      CONNECT_PRODUCER_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_CONNECT_kafka_username if KAFKA_CONNECT_kafka_username is defined and KAFKA_CONNECT_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_CONNECT_kafka_password if KAFKA_CONNECT_kafka_password is defined and KAFKA_CONNECT_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      CONNECT_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONNECT_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      CONNECT_CONSUMER_SASL_MECHANISM_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONNECT_CONSUMER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
      CONNECT_PRODUCER_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      CONNECT_PRODUCER_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      CONNECT_SSL_KEYSTORE_TYPE: "PKCS12"
      CONNECT_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONNECT_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONNECT_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      CONNECT_CONSUMER_SSL_KEYSTORE_TYPE: "PKCS12"
      CONNECT_CONSUMER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONNECT_CONSUMER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONNECT_CONSUMER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
      CONNECT_PRODUCER_SSL_KEYSTORE_TYPE: "PKCS12"
      CONNECT_PRODUCER_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      CONNECT_PRODUCER_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      CONNECT_PRODUCER_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_enable and KAFKA_REPLICATOR_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and (KAFKA_edition | lower) == 'enterprise' and KAFKA_MM2_enable) | default(false) %}
  mirror-maker{{dataCenter}}-1:
    image: trivadis/mirrormaker2:{{__KAFKA_MM2_version}}
    container_name: mirror-maker{{dataCenter}}-1:
    hostname: mirror-maker{{dataCenter}}-1:
    labels:
      com.platys.name: "mirror-maker2"
      com.platys.description: "Kafka MirrorMaker2"
    environment:
      MM2: ''
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/mirror-maker2/mm2.properties:/opt/mm2/kafka-mm2.properties:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    restart: {{container_restart_policy}}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
{% endif %}   {# KAFKA_enable and KAFKA_MM2_enable #}

{% if (LENSES_BOX_enable) | default(false) %}
  #  ================================== Lenses Box ========================================== #
  lenses-box:
    image: lensesio/box:{{__LENSES_BOX_version}}
    container_name: lenses-box
    hostname: lenses-box
    labels:
      com.platys.name: "lenses-box"
      com.platys.description: "Landoop Lenses Box"
      com.platys.webui.title: 'Lenses Box UI'
      com.platys.webui.url: "http://dataplatform:28369"
    ports:
      - 28369:3030
      - 9192:9192
    environment:
      ADV_HOST: "${PUBLIC_IP}"
      BROKER_PORT: 9192
      EULA: "{{LENSES_BOX_license}}"
      SAMPLEDATA: 1
      RUNNING_SAMPLEDATA: 1
      BROWSECONFIGS: 1
      CONNECT_PLUGIN_PATH: "/usr/share/java,/connectors,/etc/kafka-connect/addl-plugins"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# LENSES_BOX_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KCAT_enable) | default(false) %}
  #  ================================== kcat (used to be kafkacat)  ========================================== #
  kcat:
    image: edenhill/kcat:{{__KCAT_version}}
    container_name: kcat
    hostname: kcat
    labels:
      com.platys.name: "kcat"
      com.platys.description: "Generic non-JVM Kafka consumer and producer"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KCAT_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KASKADE_enable) | default(false) %}
  #  ================================== kcat (used to be kafkacat)  ========================================== #
  kaskade:
    image: sauljabin/kaskade:{{__KASKADE_version}}
    container_name: kaskade
    hostname: kaskade
    labels:
      com.platys.name: "kaskade"
      com.platys.description: "Kafka CLI"
    environment:
      BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
      SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
    {% if ns.secureKafkaEnabled %}
      SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      SASL_USERNAME: "{{ns.kafkaToolsDefaultUsername}}"
      SASL_USERNAME: "{{ns.kafkaToolsDefaultPassword}}"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      SSL_KEYSTORE_TYPE: "PKCS12"
      SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/kaskade/kaskade.yml:/kaskade/kaskade.yml
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KASKADE_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKACTL_enable) | default(false) %}
  #  ================================== kafkacatl  ========================================== #
  kafkactl:
    image: docker.io/jbvmio/kafkactl:{{__KAFKACTL_version}}
    container_name: kafkactl
    hostname: kafkactl
    labels:
      com.platys.name: "kafkactl"
      com.platys.description: "Kafka CLI"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/kafkactl/kafkactl.yaml:/root/.kafkactl.yaml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKACTL_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and JIKKOU_enable) | default(false) %}
  #  ================================== Jikkou ========================================== #
  jikkou:
    image: streamthoughts/jikkou:{{__JIKKOU_version}}
    container_name: jikkou
    hostname: jikkou
    labels:
      com.platys.name: "jikkou"
      com.platys.description: "Resource as Code framework for Apache Kafka"
    environment:
      JIKKOU_DEFAULT_KAFKA_BOOTSTRAP_SERVERS: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      JIKKOU_DEFAULT_KAFKA_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      JIKKOU_DEFAULT_KAFKA_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      JIKKOU_DEFAULT_KAFKA_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_RESTPROXY_kafka_username if KAFKA_RESTPROXY_kafka_username is defined and KAFKA_RESTPROXY_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_RESTPROXY_kafka_password if KAFKA_RESTPROXY_kafka_password is defined and KAFKA_RESTPROXY_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      JIKKOU_DEFAULT_KAFKA_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      JIKKOU_DEFAULT_KAFKA_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      JIKKOU_DEFAULT_KAFKA_SSL_KEYSTORE_TYPE: "PKCS12"
      JIKKOU_DEFAULT_KAFKA_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      JIKKOU_DEFAULT_KAFKA_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      JIKKOU_DEFAULT_KAFKA_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if (SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      JIKKOU_DEFAULT_SCHEMA_REGISTRY_URL: {{ns.schemaRegistryUrl}}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {%if JIKKOU_kafka_brokers_wait_for_min_available_enabled | default(true) %}
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE: {{KAFKA_broker_nodes}}
    {% endif -%}   {#  JIKKOU_kafka_brokers_wait_for_min_available_enabled #}
    {%if JIKKOU_kafka_brokers_wait_for_enabled | default(true) %}
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED: 'true'
    {% else -%}
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED: 'false'
    {% endif -%}   {#  JIKKOU_kafka_brokers_wait_for_enabled #}
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS: {{JIKKOU_kafka_brokers_wait_for_retry_backoff_ms if JIKKOU_kafka_brokers_wait_for_retry_backoff_ms is defined and JIKKOU_kafka_brokers_wait_for_retry_backoff_ms else omit }}
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS: {{JIKKOU_kafka_brokers_wait_for_timeout_ms if JIKKOU_kafka_brokers_wait_for_timeout_ms is defined and JIKKOU_kafka_brokers_wait_for_timeout_ms else omit }}
      VALIDATION_DEFAULT_TOPIC_NAME_REGEX: "{{JIKKOU_validation_default_topic_name_regex if JIKKOU_validation_default_topic_name_regex is defined and JIKKOU_validation_default_topic_name_regex and JIKKOU_validation_default_topic_name_regex | length else omit }}"
      VALIDATION_DEFAULT_TOPIC_MIN_NUM_PARTITIONS: {{JIKKOU_validation_default_topic_min_num_partitions if JIKKOU_validation_default_topic_min_num_partitions is defined and JIKKOU_validation_default_topic_min_num_partitions else omit }}
      VALIDATION_DEFAULT_TOPIC_MIN_REPLICATION_FACTOR: {{JIKKOU_validation_default_topic_min_replication_factor if JIKKOU_validation_default_topic_min_replication_factor is defined and JIKKOU_validation_default_topic_min_replication_factor else omit }}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/jikkou/:/jikkou
      - ./conf/jikkou/application.conf:/appuser/.jikkou/application.conf
      - ./conf/jikkou/config:/etc/jikkou/config
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - "apply"
      - "--files"
      - "/jikkou/"
      - "--file-name"
      - "**/*.{yaml,yml}"
    {% if JIKKOU_set_labels is defined and JIKKOU_set_labels %}
      {% for label in JIKKOU_set_labels.split(",") %}
      - "--set-label"
      - "{{label}}"
      {% endfor -%}
    {% endif -%}   {#  JIKKOU_set_labels #}
    {% if JIKKOU_set_values is defined and JIKKOU_set_values %}
      {% for value in JIKKOU_set_values.split(",") %}
      - "--set-value"
      - "{{value}}"
      {% endfor -%}
    {% endif -%}   {#  JIKKOU_set_values #}
    {% if JIKKOU_exclude_resources_regexp | default(true) %}
      - "--selector"
      - {{JIKKOU_exclude_resources_regexp}}
      - {{JIKKOU_include_resources_regexp}}
    {% endif -%}   {#  JIKKOU_exclude_resources_regexp #}
    {% if JIKKOU_output_format is defined and JIKKOU_output_format and JIKKOU_output_format and JIKKOU_output_format | length | default(false) %}
      - "--output"
      - "{{JIKKOU_output_format}}"
    {% endif -%}   {#  JIKKOU_output_format #}
    {% if JIKKOU_use_dryrun | default(false) %}
      - "--dry-run"
    {% endif -%}   {#  JIKKOU_use_verbose_option #}
    {% if JIKKOU_logger_level is defined and JIKKOU_logger_level and JIKKOU_logger_level and JIKKOU_logger_level | length | default(false) %}
      - "--logger-level"
      - "{{JIKKOU_logger_level | upper}}"
    {% endif -%}   {#  JIKKOU_logger_level #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: on-failure
{% endif %}   {# JIKKOU_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and JIKKOU_SERVER_enable) | default(false) %}
  #  ================================== Jikkou Server  ========================================== #
  jikkou-server:
    image: streamthoughts/jikkou-api-server:{{__JIKKOU_SERVER_version}}
    container_name: jikkou-server
    hostname: jikkou-server
    labels:
      com.platys.name: "jikkou"
      com.platys.description: "Resource as Code framework for Apache Kafka"
      com.platys.restapi.title: ""
      com.platys.restapi.url: "http://dataplatform:28388"
    ports:
      - "28388:8080"      
    environment:
      JIKKOU_CONFIGURATION: |
        micronaut:
          server:
            port: 8080
          security:
            enabled: false
            #  # Token based authentication configuration
            #  authentication: bearer
            #  token:
            #    enabled: false
            #    jwt:
            #      signatures:
            #        secret:
            #          generator:
            #            secret: '"${JWT_GENERATOR_SIGNATURE_SECRET:pleaseChangeThisSecretForANewOne}"'            
        jikkou:
        #  security:
        #    basic-auth:
        #      - username: "admin"
        #        password: "{noop}admin"

          # The paths from which to load extensions
          extension.paths: []        
          
          # The extension providers
          extension.providers:
            # By default, disable all extensions
            default.enabled: false
            # Explicitly enabled/disable extensions
            #<provider_name>.enabled: <boolean>
            core.enabled: true
            kafka.enabled: true
            kafka-connect.enabled: true
          kafka.client:
            bootstrap.servers: '{{ns.bootstrapServers}}'
          schemaRegistry:
            url: '{{ns.schemaRegistryUrl}}'  
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
{% endif %}   {# JIKKOU_SERVER_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_TOPICS_UI_enable) | default(false) %}
  #  ================================== Kafka Topics UI ========================================== #
  kafka-topics-ui:
    image: landoop/kafka-topics-ui:{{__KAFKA_TOPICS_UI_version}}
    container_name: kafka-topics-ui
    hostname: kafka-topics-ui
    labels:
      com.platys.name: "kafka-topics-ui"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kafka Topics UI'
      com.platys.webui.url: "http://dataplatform:28141"
    ports:
      - "28141:8000"
    environment:
      KAFKA_REST_PROXY_URL: 'http://kafka-rest-1:8086'
      LAZY_LOAD_TOPIC_META: 'false'
      PROXY: 'true'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if KAFKA_TOPICS_UI_map_resolv_conf | default(false) %}
      - ./conf/kafka-topics-ui/resolv.conf:/etc/resolv.conf:ro
    {% endif -%}   {#  KAFKA_TOPICS_UI_map_resolv_conf #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_enable and KAFKA_SCHEMA_REGISTRY_UI_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and (KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect) and KAFKA_CONNECT_UI_enable) | default(false) %}
  #  ================================== Kafka Connect UI ========================================== #
  kafka-connect-ui:
    image: landoop/kafka-connect-ui:{{__KAFKA_CONNECT_UI_version}}
    container_name: kafka-connect-ui
    hostname: kafka-connect-ui
    labels:
      com.platys.name: "kafka-connect-ui"
      com.platys.description: "Kafka Connect GUI"
      com.platys.webui.title: 'Kafka Connect UI'
      com.platys.webui.url: "http://dataplatform:28103"
    ports:
      - "28103:8000"
    environment:
      CONNECT_URL: {{ns1.connectServersInternalUrl}}
      PROXY: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if KAFKA_CONNECT_UI_map_resolv_conf | default(false) %}
      - ./conf/kafka-connect-ui/resolv.conf:/etc/resolv.conf:ro
    {% endif -%}   {#  KAFKA_TOPICS_UI_map_resolv_conf #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KAFKA_enable and KAFKA_CONNECT_UI_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and CMAK_enable) | default(false) %}
  #  ================================== Cluster Manager for Apache Kafka (CMAK) ========================================== #
  {% set externalPort = 28104 + dataCenterId %}
  cmak{{dataCenter}}:
    image: trivadis/cmak:{{__CMAK_version}}
    container_name: cmak{{dataCenter}}
    hostname: cmak{{dataCenter}}
    labels:
      com.platys.name: "cmak"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Cluster Manager for Apache Kafka UI'
      com.platys.webui.url: "http://dataplatform:{{externalPort}}"
    ports:
      - "{{externalPort}}:9000"
    environment:
      ZK_HOSTS: 'zookeeper-1:2181'
      APPLICATION_SECRET: 'abc123!'
      KAFKA_MANAGER_AUTH_ENABLED: {{CMAK_auth_enabled}}
      KAFKA_MANAGER_USERNAME: {{CMAK_username}}
      KAFKA_MANAGER_PASSWORD: {{CMAK_password}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and CMAK_enable #}


{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFDROP_enable) | default(false) %}
  #  ================================== Kafdrop ========================================== #
  kafdrop:
    image: obsidiandynamics/kafdrop:{{__KAFDROP_version}}
    container_name: kafdrop
    hostname: kafdrop
    labels:
      com.platys.name: "kafdrop"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kafdrop UI'
      com.platys.webui.url: "http://dataplatform:28110"
    ports:
      - "28110:9020"
    environment:
      KAFKA_BROKERCONNECT: '{{ns.bootstrapServers}}'
      SCHEMAREGISTRY_CONNECT: '{{ns.schemaRegistryUrl}}'
      {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
      SCHEMAREGISTRY_AUTH: '{{ns.schemaRegistryUsername}}:{{ns.schemaRegistryPassword}}'
      {% endif %}
      MESSAGE_FORMAT: AVRO
      JVM_OPTS: "-Xms32M -Xmx64M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
      SERVER_SERVLET_CONTEXTPATH: "/"
      SERVER_PORT: 9020
    {% if ns.secureKafkaEnabled %}
      KAFKA_PROPERTIES_FILE: "/tmp/client.properties"
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled %}
      - ./security/kafka/sasl-scram/client.properties:/tmp/client.properties
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFDROP_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KADMIN_enable) | default(false) %}
  #  ================================== KAdmin ========================================== #
  kadmin:
    image: bettercloud/kadmin:{{__KADMIN_version}}
    container_name: kadmin
    hostname: kadmin
    labels:
      com.platys.name: "kadmin"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'KAdmin UI'
      com.platys.webui.url: "http://dataplatform:28106"
    ports:
      - "28106:8080"
    environment:
    {% if external['KAFKA_enable'] | default(false) %}
      ZOOKEEPER_HOST: zookeeper-1:2181
    {% endif -%}   {#  external['KAFKA_enable']) #}
      KAFKA_HOST: {{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      SECURITY_PROTOCOL: SSL
      TRUST_STORE_LOCATION: /certs/client.ts.p12
      TRUST_STORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KEY_STORE_LOCATION: /certs/client.ks.p12
      KEY_STORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KADMIN_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and AKHQ_enable) | default(false) %}
  #  ================================== Apache Kafka HQ (AKHQ) ========================================== #
  {% set externalPort = 28107 + dataCenterId %}
  {% set monitoringExternalPort = 28320 + dataCenterId %}
  akhq{{dataCenter}}:
    image: tchiotludo/akhq:{{__AKHQ_version}}
    container_name: akhq{{dataCenter}}
    hostname: akhq{{dataCenter}}
    labels:
      com.platys.name: "akhq"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'AKHQ UI'
      com.platys.webui.url: "http://dataplatform:{{externalPort}}"
      com.platys.restapi.title: 'AKHQ API'
      com.platys.restapi.url: "http://dataplatform:{{externalPort}}/api"
      com.platys.password.envvars: "PLATYS_AKHQ_ADMIN_PASSWORD,PLATYS_AKHQ_READER_PASSWORD"
    ports:
      - {{externalPort}}:8080
      - {{monitoringExternalPort}}:28081
    environment:
      AKHQ_CONFIGURATION: |
    {% if AKHQ_auth_enable | default(false) %}
        micronaut:
          security:
            enabled: true
            default-group: no-roles
    {% endif -%}   {#  AKHQ_read_only_mode #}
        akhq:
    {% if AKHQ_read_only_mode | default(false) %}
          security:
            default-group: reader
    {% endif -%}   {#  AKHQ_read_only_mode #}
    {% if AKHQ_auth_enable and AKHQ_auth_type == 'basic' | default(false) %}
          security:
            enabled: true
            default-group: no-roles
            basic-auth:
              - username: {{AKHQ_username}}
                password: "${PLATYS_AKHQ_ADMIN_PASSWORD:-{{AKHQ_password}}}"
                groups:
                - admin
              - username: reader
                password: "${PLATYS_AKHQ_READER_PASSWORD:-{{AKHQ_password}}}" 
                groups:
                - reader
    {% endif -%}   {#  AKHQ_auth_type == 'basic' #}
          pagination.page-size: {{AKHQ_topic_page_size | default(omit)}}
          topic-data:
            size: {{AKHQ_topic_data_size | default(omit)}}
            poll-timeout: {{AKHQ_topic_data_poll_timeout | default(omit)}}
            kafka-max-message-length: {{AKHQ_topic_data_kafka_max_message_length | default(omit)}}
          ui-options:
            topic:
              default-view: {{AKHQ_default_view | default('HIDE_INTERNAL')}}
              skip-consumer-groups: {{'false' if AKHQ_show_consumer_groups else 'true'}}
              skip-last-record: {{'false' if AKHQ_show_last_record else 'true'}}
              show-all-consumer-groups: {{'true' if AKHQ_show_all_consumer_groups else 'false'}}
            topic-data:
              sort: {{AKHQ_sort | default('OLDEST')}}
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
                security.protocol: "{{ns.securityProtocolBROKER}}"
                sasl.mechanism: "{{ns.saslMechanismBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
                sasl.jaas.config: "{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
                ssl.truststore.location: /certs/client.ts.p12
                ssl.truststore.password: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
                ssl.keystore.type: "PKCS12"
                ssl.keystore.location: /certs/client.ks.p12
                ssl.keystore.password: {{KAFKA_client_keystore_password}}
                ssl.key.password: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if (SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
              schema-registry:
                url: "{{ns.schemaRegistryUrl}}"
                type: "confluent"
        {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
                basic-auth-username: "{{ns.schemaRegistryUsername}}"
                basic-auth-password: "{{ns.schemaRegistryPassword}}"
        {% endif %}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {% if ( KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect) | default(false) %}
              connect:
      {% for item in ns1.connectServersInternalUrl.split(",") %}
                - name: "connect-{{loop.index}}"
                  url: "{{item}}"
      {% endfor -%}
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
    {% if KSQLDB_enable | default(false) %}
              ksqldb:
                - name: "ksqldb"
                  url: "http://ksqldb-server-1:8088"
    {% endif -%}   {#  KSQLDB_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and AKHQ_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_UI_enable) | default(false) %}
  #  ================================== Kafka UI ========================================== #
  kafka-ui:
    image: provectuslabs/kafka-ui:{{__KAFKA_UI_version}}
    container_name: kafka-ui
    hostname: kafka-ui
    labels:
      com.platys.name: "kafka-ui"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kafka UI'
      com.platys.webui.url: "http://dataplatform:28179"
    ports:
      - 28179:8080
    environment:
    {% if external['KAFKA_enable'] | default(false) %}
      KAFKA_CLUSTERS_0_NAME: external
    {% else %}
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper-1:2181
      KAFKA_CLUSTERS_0_JMXPORT: '9992'
    {% endif -%}   {#  external['KAFKA_enable'] #}
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: "{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: "{{ns.saslMechanismBROKER}}"
      KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: "{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      KAFKA_CLUSTERS_0_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: 'https'
      KAFKA_CLUSTERS_0_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KAFKA_CLUSTERS_0_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KAFKA_CLUSTERS_0_SSL_KEYSTORE_TYPE: "PKCS12"
      KAFKA_CLUSTERS_0_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KAFKA_CLUSTERS_0_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KAFKA_CLUSTERS_0_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if (SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: '{{ns.schemaRegistryUrlExternal}}'
      {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
      KAFKA_CLUSTERS_0_SCHEMAREGISTRYAUTH_USERNAME: "{{ns.schemaRegistryUsername}}"
      KAFKA_CLUSTERS_0_SCHEMAREGISTRYAUTH_PASSWORD: "{{ns.schemaRegistryPassword}}"
      {% endif %}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {% if KSQLDB_enable | default(false) %}
      KAFKA_CLUSTERS_0_KSQLDBSERVER: 'http://ksqldb-server-1:8088'
    {% endif -%}   {#  KSQLDB_enable #}
    {% if ( KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect ) | default(false) %}
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: '{{ns1.connectClusterName}}'
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: {{ns1.connectServersInternalUrl.split(',')[0]}}
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
      KAFKA_CLUSTERS_0_PROPERTIES_CLIENT_DNS_LOOKUP: use_all_dns_ips
      KAFKA_CLUSTERS_0_DISABLELOGDIRSCOLLECTION: 'true'
      KAFKA_CLUSTERS_0_READONLY: 'false'
      DYNAMIC_CONFIG_ENABLED: 'true'
      LOGGING_LEVEL_ROOT: 'info'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFKA_UI_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and EFAK_enable) | default(false) %}
  #  ================================== Kafka EFAK (previously Kafka Eagle) ========================================== #
  efak:
    image: nickzurich/efak:{{__EFAK_version}}
    container_name: efak
    hostname: efak
    labels:
      com.platys.name: "efak"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'EFAK (prev. Kafka Eagle) UI'
      com.platys.webui.url: "http://dataplatform:8048"
    ports:
      - 8048:8048
    environment:
      EFAK_CLUSTER_ZK_LIST: zookeeper-1:2181
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and EFAK_UI_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KOWL_enable) | default(false) %}
  #  ================================== Kowl ========================================== #
  kowl:
    image: quay.io/cloudhut/kowl:{{__KOWL_version}}
    container_name: kowl
    hostname: kowl
    labels:
      com.platys.name: "kowl"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kowl UI'
      com.platys.webui.url: "http://dataplatform:28239"
    ports:
      - 28239:8080
    environment:
      # config according to: https://github.com/cloudhut/kowl/blob/master/docs/config/kowl.yaml
      KAFKA_BROKERS: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      KAFKA_TLS_ENABLED: 'true'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KAFKA_SASL_ENABLED: 'true'
      KAFKA_SASL_MECHANISM: '{{ns.saslMechanismBROKER}}'
      KAFKA_SASL_USERNAME: {{ns.kafkaToolsDefaultUsername}}
      KAFKA_SASL_PASSWORD: {{ns.kafkaToolsDefaultPassword}}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
    {% if ( SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      KAFKA_SCHEMAREGISTRY_ENABLED: 'True'
      KAFKA_SCHEMAREGISTRY_URLS: '{{ns.schemaRegistryUrlExternal}}'
      {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
      KAFKA_SCHEMAREGISTRY_USERNAME: "{{ns.schemaRegistryUsername}}"
      KAFKA_SCHEMAREGISTRY_PASSWORD: "{{ns.schemaRegistryPassword}}"
      {% endif -%}   {#  ns.schemaRegistryUsername is defined #}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {% if ( KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect) | default(false) %}
      CONNECT_ENABLED: 'True'
      CONNECT_CLUSTERS_NAME: '{{ns1.connectClusterName}}'
      CONNECT_CLUSTERS_URL: '{{ns1.connectServersInternalUrl.split(',')[0]}}'
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KOWL_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and  REDPANDA_CONSOLE_enable) | default(false) %}
  #  ================================== Redpanda Console ========================================== #
  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:{{__REDPANDA_CONSOLE_version}}
    container_name: redpanda-console
    hostname: redpanda-console
    labels:
      com.platys.name: "redpanda-console"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Redpanda Console UI'
      com.platys.webui.url: "http://dataplatform:28289"
    ports:
      - 28289:8080
    environment:
      # config according to: https://github.com/cloudhut/kowl/blob/master/docs/config/kowl.yaml
      KAFKA_BROKERS: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      KAFKA_TLS_ENABLED: 'true'
      KAFKA_SASL_ENABLED: 'true'
      KAFKA_SASL_MECHANISM: '{{ns.saslMechanismBROKER}}'
      KAFKA_SASL_USERNAME: {{ns.kafkaToolsDefaultUsername}}
      KAFKA_SASL_PASSWORD: {{ns.kafkaToolsDefaultPassword}}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if ( SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      KAFKA_SCHEMAREGISTRY_ENABLED: 'True'
      KAFKA_SCHEMAREGISTRY_URLS: '{{ns.schemaRegistryUrlExternal}}'
      {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
      KAFKA_SCHEMAREGISTRY_USERNAME: "{{ns.schemaRegistryUsername}}"
      KAFKA_SCHEMAREGISTRY_PASSWORD: "{{ns.schemaRegistryPassword}}"
      {% endif -%}   {#  ns.schemaRegistryUsername is defined #}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {% if ( KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect) | default(false) %}
      CONNECT_ENABLED: 'True'
      CONNECT_CLUSTERS_NAME: '{{ns1.connectClusterName}}'
      CONNECT_CLUSTERS_URL: '{{ns1.connectServersInternalUrl.split(',')[0]}}'
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
    {% if REDPANDA_CONSOLE_edition == 'enterprise' %}
      REDPANDA_LICENSE_FILEPATH: /etc/console-mounts/redpanda.license
    {% endif -%}   {#  REDPANDA_CONSOLE_edition #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if REDPANDA_CONSOLE_edition == 'enterprise' %}
      - ./licenses/redpanda/redpanda.license/:/etc/console-mounts/redpanda.license
    {% endif -%}   {#  REDPANDA_CONSOLE_edition #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and REDPANDA_CONSOLE_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KOUNCIL_enable) | default(false) %}
  #  ================================== Kouncil ========================================== #
  kouncil:
    image: consdata/kouncil:{{__KOUNCIL_version}}
    container_name: kouncil
    hostname: kouncil
    labels:
      com.platys.name: "kouncil"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kouncil UI'
      com.platys.webui.url: "http://dataplatform:28253"
    ports:
      - 28253:8080
    environment:
      bootstrapServers: '{{ns.bootstrapServers}}'
    {% if (SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      schemaRegistryUrl: '{{ns.schemaRegistryUrl}}'
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KOUNCIL_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_MAGIC_enable) | default(false) %}
  #  ================================== Kafka Magic ========================================== #
  kafka-magic:
    image: digitsy/kafka-magic:{{__KAFKA_MAGIC_version}}
    container_name: kafka-magic
    hostname: kafka-magic
    labels:
      com.platys.name: "kafka-magic"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kafka Magic UI'
      com.platys.webui.url: "http://dataplatform:28254"
    ports:
      - 28254:80
    environment:
      KMAGIC_ALLOW_TOPIC_DELETE: "true"
      KMAGIC_ALLOW_SCHEMA_DELETE: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFKA_MAGIC_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_WEBVIEW_enable) | default(false) %}
  #  ================================== Kafka Webview ========================================== #
  kafka-webview:
    image: sourcelaborg/kafka-webview:{{__KAFKA_WEBVIEW_version}}
    container_name: kafka-webview
    hostname: kafka-webview
    labels:
      com.platys.name: "kafka-webview"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kafka Webview UI'
      com.platys.webui.url: "http://dataplatform:28274"
    ports:
      - 28274:8080
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFKA_WEBVIEW_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KPOW_enable) | default(false) %}
  #  ================================== kpow ========================================== #
  kpow:
    image: factorhouse/kpow-{{KPOW_edition | lower}}:{{__KPOW_version}}
    container_name: kpow
    hostname: kpow
    labels:
      com.platys.name: "kpow"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'kpow UI'
      com.platys.webui.url: "http://dataplatform:28283"
    ports:
      - 28283:3000
    {% if KPOW_use_external_license_info %}
    env_file:
      - ./licenses/kpow/kpow-license.env
    {% endif -%}   {#  KPOW_use_external_license_info #}
    environment:
      - PORT=3000
      - ENVIRONMENT_NAME=platys-stack
      - BOOTSTRAP={{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      - SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SSL_KEYSTORE_TYPE="PKCS12"
      - SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
    {% if (KPOW_edition | lower) == 'ce' or (KPOW_edition | lower) == 'se' or (KPOW_edition | lower) == 'ee' %}
      {% if not KPOW_use_external_license_info %}
      - LICENSE_ID={{KPOW_license_id if KPOW_license_id is defined and KPOW_license_id | length else omit}}
      - LICENSE_CODE={{KPOW_license_code if KPOW_license_code is defined and KPOW_license_code | length else omit}}
      - LICENSEE={{KPOW_licensee if KPOW_licensee is defined and KPOW_licensee | length else omit}}
      - LICENSE_EXPIRY={{KPOW_license_expiry if KPOW_license_expiry is defined and KPOW_license_expiry | length else omit}}
      - LICENSE_SIGNATURE={{KPOW_license_signature if KPOW_license_signature is defined and KPOW_license_signature | length else omit}}
      {% endif -%}   {#  KPOW_use_external_license_info #}
      - ALLOW_TOPIC_CREATE=true
      - ALLOW_TOPIC_DELETE=true
      - ALLOW_TOPIC_TRUNCATE=true
      - ALLOW_TOPIC_EDIT=true
      - ALLOW_TOPIC_INSPECT=true
      - ALLOW_TOPIC_PRODUCE=true
      - ALLOW_GROUP_EDIT=true
      - ALLOW_BROKER_EDIT=true
      - ALLOW_ACL_EDIT=true
      - ALLOW_SCHEMA_CREATE=true
      - ALLOW_SCHEMA_EDIT=true
      - ALLOW_CONNECT_CREATE=true
      - ALLOW_CONNECT_EDIT=true
      - ALLOW_CONNECT_INSPECT=true
      - ALLOW_BULK_ACTION=true
    {% endif -%}   {#  KPOW_edition = ce or se or ee #}
    {% if ( KAFKA_CONNECT_enable or KSQLDB_use_embedded_connect ) | default(false) %}
      - CONNECT_NAME=kafka-connect
      - CONNECT_REST_URL={{ns1.connectServersInternalUrl.split(',')[0]}}
#      - CONNECT_AWS_REGION=
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
    {% if (SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      - SCHEMA_REGISTRY_URL={{ns.schemaRegistryUrlExternal}}
      {% if ns.schemaRegistryUsername is defined and ns.schemaRegistryUsername|length %}
      - SCHEMA_REGISTRY_USER={{ns.schemaRegistryUsername}}
      - SCHEMA_REGISTRY_PASSWORD={{ns.schemaRegistryPassword}}
      {% endif %}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {% if KSQLDB_enable | default(false) %}
      - KSQLDB_NAME=ksqldb
      - KSQLDB_HOST=ksqldb-server-1
      - KSQLDB_PORT=8088
    {% endif -%}   {#  KSQLDB_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KPOW_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and CONDUKTOR_PLATFORM_enable) | default(false) %}
  #  ================================== Conduktor Platform ========================================== #
  conduktor-platform:
    image: conduktor/conduktor-platform:{{__CONDUKTOR_PLATFORM_version}}
    container_name: conduktor-platform
    hostname: conduktor-platform
    labels:
      com.platys.name: "conduktor-platform"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Conduktor Platform UI'
      com.platys.webui.url: "http://dataplatform:28285"
    ports:
      - 28285:8080
    environment:
      - RUN_MODE=nano
      - CDK_LISTENING_PORT=8080
      - CONSOLE_ENABLED=true
      - TESTING_ENABLED=true
      - MONITORING_ENABLED=true
      - DATA_MASKING_ENABLED=false
      - SCANNER_ENABLED=false
      - GOVERNANCE_ENABLED=false
    {% if (CONDUKTOR_PLATFORM_use_external_postgres or external['CONDUKTOR_PLATFORM_use_external_postgres']) | default(false) %}
      - EMBEDDED_POSTGRES="false"
      - CDK_DATABASE_HOST={{CONDUKTOR_PLATFORM_postgres_host}}
      - CDK_DATABASE_PORT={{CONDUKTOR_PLATFORM_postgres_port}}
      - CDK_DATABASE_NAME={{CONDUKTOR_PLATFORM_postgres_db}}
      - CDK_DATABASE_USERNAME={{CONDUKTOR_PLATFORM_postgres_username}}
      - CDK_DATABASE_PASSWORD={{CONDUKTOR_PLATFORM_postgres_password}}!
    {% endif -%}   {#  CONDUKTOR_PLATFORM_use_external_postgres #}
      - CDK_IN_CONF_FILE=/etc/platform-config.yaml
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServers}}
      - LICENSE_KEY={{CONDUKTOR_PLATFORM_license_key}}
      - ORGANISATION_NAME={{CONDUKTOR_PLATFORM_organisation_name}}
      - ADMIN_EMAIL={{CONDUKTOR_PLATFORM_admin_email}}
      - ADMIN_PSW={{CONDUKTOR_PLATFORM_admin_psw}}
    {% if ns.secureKafkaEnabled %}
      - KAFKA_SECURITY_PROTOCOL="{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - KAFKA_SASL_MECHANISM="{{ns.saslMechanismBROKER}}"
      - KAFKA_SASL_JAAS_CONFIG="{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SSL_KEYSTORE_TYPE="PKCS12"
      - SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if (SCHEMA_REGISTRY_enable or external['SCHEMA_REGISTRY_enable']) | default(false) %}
      - SCHEMA_REGISTRY_URL={{ns.schemaRegistryUrl}}
    {% endif -%}   {#  SCHEMA_REGISTRY_enable #}
    {% if ( KAFKA_CONNECT_enable ) | default(false) %}
      - KAFKA_CONNECT_URL={{ns1.connectServersInternalUrl.split(',')[0]}}
    {% endif -%}   {#  KAFKA_CONNECT_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/conduktor-platform/platform-config.yaml:/etc/platform-config.yaml
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and CONDUKTOR_PLATFORM_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KADECK_enable) | default(false) %}
  #  ================================== kadeck ========================================== #
  kadeck:
    image: xeotek/kadeck:{{__KADECK_version}}
    container_name: kadeck
    hostname: kadeck
    labels:
      com.platys.name: "kadeck"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kadeck UI'
      com.platys.webui.url: "http://dataplatform:28313"
    ports:
      - 28313:80
    environment:
      - xeotek_kadeck_port=80
    {% if (KADECK_edition | lower) == 'free' %}
      - xeotek_kadeck_free={{KADECK_free_email_address}}
    {% endif -%}   {#  KADECK_edition = free #}
    {% if (KADECK_edition | lower) == 'enterprise' %}
      - xeotek_kadeck_teamid={{KADECK_ee_team_id}}
      - xeotek_kadeck_secret={{KADECK_ee_secret}}
    {% endif -%}   {#  KADECK_edition = free #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KADECK_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKISTRY_enable) | default(false) %}
  #  ================================== Kafkistry ========================================== #
  kafkistry:
    image: infobip/kafkistry:{{__KAFKISTRY_version}}
    container_name: kafkistry
    hostname: kafkistry
    labels:
      com.platys.name: "kafkistry"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Kafkistry UI'
      com.platys.webui.url: "http://dataplatform:28303"
    ports:
      - 28303:8080
    environment:
      - USERS_PASSWORDS={{KAFKISTRY_users_passwords if KAFKISTRY_users_passwords_yaml_file is not defined or not KAFKISTRY_users_passwords_yaml_file or KAFKISTRY_users_passwords_yaml_file | length == 0 else omit }}
      - USERS_PASSWORDS_YAML_FILE={{'/config/' ~ KAFKISTRY_users_passwords_yaml_file if KAFKISTRY_users_passwords_yaml_file is defined and KAFKISTRY_users_passwords_yaml_file and KAFKISTRY_users_passwords_yaml_file | length else omit }}
      - OWNER_GROUPS={{KAFKISTRY_owner_groups if KAFKISTRY_owner_groups_yaml_file is not defined or not KAFKISTRY_owner_groups_yaml_file or KAFKISTRY_owner_groups_yaml_file | length == 0 else omit }}
      - OWNER_GROUPS_YAML_FILE={{'/config/' ~ KAFKISTRY_owner_groups_yaml_file if KAFKISTRY_owner_groups_yaml_file is defined and KAFKISTRY_owner_groups_yaml_file and KAFKISTRY_owner_groups_yaml_file | length else omit }}
      - CONSUME_ENABLED=true
      - OLDEST_RECORD_AGE_ENABLED=true
      - SQL_ENABLED=true
      - HTTP_REQUESTS_LOGGING_ENABLED=false
    {% if KAFKISTRY_repository_type == 'dir' %}
      - SPRING_PROFILES_ACTIVE=dir
      - APP_REPOSITOY_DIR_PATH=/kafkistry/repository
    {% else %}
      - LOCAL_GIT_DIR=/kafkistry/repository/git-repository
      - GIT_COMMIT_TO_MASTER_BY_DEFAULT=false
      - GIT_MAIN_BRANCH=master
    {% endif -%}   {#  KAFKISTRY_repository #}
    {% if ns.secureKafkaEnabled %}
      - APP_KAFKA_PROPERTIES_SECURITY_PROTOCOL="{{ns.securityProtocolBROKER}}"
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - APP_KAFKA_PROPERTIES_SASL_MECHANISM="{{ns.saslMechanismBROKER}}"
      - APP_KAFKA_PROPERTIES_SASL_JAAS_CONFIG="{{ns.loginModuleBROKER}} required
                        username=\"{{KAFKA_RESTPROXY_kafka_username if KAFKA_RESTPROXY_kafka_username is defined and KAFKA_RESTPROXY_kafka_username | length else ns.kafkaToolsDefaultUsername}}\"
                        password=\"{{KAFKA_RESTPROXY_kafka_password if KAFKA_RESTPROXY_kafka_password is defined and KAFKA_RESTPROXY_kafka_password | length else ns.kafkaToolsDefaultPassword}}\";"
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - APP_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - APP_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - APP_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - APP_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - APP_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - APP_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - APP_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {% if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if KAFKISTRY_users_passwords_yaml_file is defined and KAFKISTRY_users_passwords_yaml_file and KAFKISTRY_users_passwords_yaml_file | length %}
      - ./custom-conf/kafkistry/{{KAFKISTRY_users_passwords_yaml_file}}:/config
    {% endif -%}   {#  KAFKISTRY_users_passwords_yaml_file #}
    {% if KAFKISTRY_owner_groups_yaml_file is defined and KAFKISTRY_owner_groups_yaml_file and KAFKISTRY_owner_groups_yaml_file | length %}
      - ./custom-conf/kafkistry/{{KAFKISTRY_owner_groups_yaml_file}}:/config
    {% endif -%}   {#  KAFKISTRY_users_passwords_yaml_file #}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFKISTRY_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KLAW_enable) | default(false) %}
  #  ================================== Kafkistry ========================================== #
  klaw-core:
    image: aivenoy/klaw-core:{{__KLAW_version}}
    container_name: klaw-core
    hostname: klaw-core
    labels:
      com.platys.name: "klaw"
      com.platys.description: "Kafka GUI"
      com.platys.webui.title: 'Klaw UI'
      com.platys.webui.url: "http://dataplatform:28309"
      com.platys.password.envvars: "PLATYS_KLAW_SUPERADMIN_PASSWORD,PLATYS_KLAW_QUICKSTART_USER_PASSWORD"      
    ports:
      - 28309:9097
    extra_hosts:
      - "host.docker.internal:host-gateway"        
    environment:
      - KLAW_CLUSTERAPI_ACCESS_BASE64_SECRET=VGhpc0lzRXhhY3RseUEzMkNoYXJTdHJpbmdTZWNyZXQK
      - SPRING_DATASOURCE_URL=jdbc:h2:file:/klaw/klawprodb;DB_CLOSE_ON_EXIT=FALSE;DB_CLOSE_DELAY=-1;MODE=MySQL;CASE_INSENSITIVE_IDENTIFIERS=TRUE;
      - KLAW_CORAL_ENABLED=true
      - KLAW_UIAPI_SERVERS=http://klaw-core:9097
      - KLAW_QUICKSTART_ENABLED={{KLAW_quickstart_enable | default(false)}}
      - KLAW_DOCKER_INTERNAL_HOST=host.docker.internal
      - KLAW_DOCKER_CLUSTERAPI_HOST=http://klaw-cluster-api:9343
      - KLAW_DOCKER_KAFKA_CLUSTER={{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
        {% if KAFKA_sasl_mechanism == 'SCRAM-SHA-256' %}
      - KLAW_DOCKER_KAFKA_CLUSTER_SECURITYPROTOCOL=SASL_SSL/SCRAM-SHA-256
        {% elif KAFKA_sasl_mechanism == 'SCRAM-SHA-512' %}
      - KLAW_DOCKER_KAFKA_CLUSTER_SECURITYPROTOCOL=SASL_SSL/SCRAM-SHA-512
        {% elif KAFKA_sasl_mechanism == 'OAUTHBEARER' %}
      - KLAW_DOCKER_KAFKA_CLUSTER_SECURITYPROTOCOL=SASL_SSL/not-supported
        {% elif KAFKA_sasl_mechanism == 'PLAIN' %}
      - KLAW_DOCKER_KAFKA_CLUSTER_SECURITYPROTOCOL=SASL_PLAIN
        {% endif -%}
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - KLAW_DOCKER_KAFKA_CLUSTER_SECURITYPROTOCOL=SSL
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
      - KLAW_DOCKER_SR_CLUSTER={{ ns.schemaRegistryUrl | replace('http://','') }}
      - KLAW_QUICKSTART_DEFAULT_USER1={{KLAW_quickstart_username_1 if KLAW_quickstart_username_1 is defined and KLAW_quickstart_username_1 and KLAW_quickstart_username_1 | length else omit}}
      - KLAW_QUICKSTART_DEFAULT_USER2={{KLAW_quickstart_username_2 if KLAW_quickstart_username_2 is defined and KLAW_quickstart_username_2 and KLAW_quickstart_username_2 | length else omit}}
      - KLAW_QUICKSTART_DEFAULT_PWD=${PLATYS_KLAW_QUICKSTART_USER_PASSWORD:-{{KLAW_quickstart_password}}}
      - LOGGING_LEVEL_ROOT={{KLAW_logging_level | default(info)}}
      - KLAW_SUPERADMIN_DEFAULT_USERNAME={{KLAW_superadmin_username}}
      - KLAW_SUPERADMIN_DEFAULT_PASSWORD=${PLATYS_KLAW_SUPERADMIN_PASSWORD:-{{KLAW_superadmin_password}}}
      - SPRING_MAIL_HOST={{KLAW_mail_host}}
      - SPRING_MAIL_PORT={{KLAW_mail_port}}
      - KLAW_MONITORING_METRICS_ENABLE=false
      - KLAW_EXPORT_SCHEDULER_ENABLE=false
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  klaw-cluster-api:
    image: aivenoy/klaw-cluster-api:{{__KLAW_version}}
    container_name: klaw-cluster-api
    hostname: klaw-cluster-api
    labels:
      com.platys.name: "klaw"
      com.platys.description: "Kafka GUI"
    ports:
      - 9343:9343
    environment:
      - KLAW_CLUSTERAPI_ACCESS_BASE64_SECRET=VGhpc0lzRXhhY3RseUEzMkNoYXJTdHJpbmdTZWNyZXQK
    {% if ns.secureKafkaEnabled %}
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
        {% if KAFKA_sasl_mechanism == 'SCRAM-SHA-256' or KAFKA_sasl_mechanism == 'SCRAM-SHA-512' %}
      - {{KLAW_cluster_id}}_KAFKASASL_JAASCONFIG_SCRAM={{ns.loginModuleBROKER}} required username='{{ns.kafkaToolsDefaultUsername}}' password='{{ns.kafkaToolsDefaultPassword}}';
        {% elif KAFKA_sasl_mechanism == 'OAUTHBEARER' %}
      - {{KLAW_cluster_id}}_KAFKASASL_JAASCONFIG_OAUTHBEARER={{ns.loginModuleBROKER}} required username='{{ns.kafkaToolsDefaultUsername}}' password='{{ns.kafkaToolsDefaultPassword}}';
        {% elif KAFKA_sasl_mechanism == 'PLAIN' %}
      - {{KLAW_cluster_id}}_KAFKASASL_JAASCONFIG_PLAIN={{ns.loginModuleBROKER}} required username='{{ns.kafkaToolsDefaultUsername}}' password='{{ns.kafkaToolsDefaultPassword}}';
        {% endif -%}
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - {{KLAW_cluster_id}}_KAFKASASL_TRUSTSTORE_TYPE="PKCS12"
      - {{KLAW_cluster_id}}_KAFKASASL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - {{KLAW_cluster_id}}_KAFKASASL_TRUSTSTORE_PWD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - {{KLAW_cluster_id}}_KAFKASASL_KEYSTORE_TYPE="PKCS12"
      - {{KLAW_cluster_id}}_KAFKASASL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - {{KLAW_cluster_id}}_KAFKASASL_KEYSTORE_PWD={{KAFKA_client_keystore_password}}
      - {{KLAW_cluster_id}}_KAFKASASL_KEY_PWD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
      - LOGGING_LEVEL_ROOT={{KLAW_logging_level | default(info)}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KLAW_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_CONNECTOR_BOARD_enable) | default(false) %}
  #  ================================== Kafka Connector Board ========================================== #
  kafka-connector-board:
    image: tranglolab/kafka-connector-board:{{__KAFKA_CONNECTOR_BOARD_version}}
    container_name: kafka-connector-board
    hostname: kafka-connector-board
    labels:
      com.platys.name: "kafka-connector-board"
      com.platys.description: "Kafka Connect GUI"
      com.platys.webui.title: 'Kafka Connector Board UI'
      com.platys.webui.url: "http://dataplatform:28282"
    ports:
      - 28282:80
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFKA_CONNECTOR_BOARD_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and STREAMS_EXPLORER_enable) | default(false) %}
  #  ================================== Streams Explorer ========================================== #
  streams-explorer:
    image: bakdata/streams-explorer:{{__STREAMS_EXPLORER_version}}
    container_name: streams-explorer
    hostname: streams-explorer
    labels:
      com.platys.name: "streams-explorer"
      com.platys.description: "Explore Kafka Pipelines"
      com.platys.webui.title: 'Streams Explorer UI'
      com.platys.webui.url: "http://dataplatform:28195"
    ports:
      - 28195:80
    environment:
      SE_SCHEMAREGISTRY__url: '{{ns.schemaRegistryUrl}}'
      SE_KAFKACONNECT__url: {{ns1.connectServersInternalUrl.split(',')[0]}}
      SE_PROMETHEUS__url: http://host.docker.internal:9090
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and STREAMS_EXPLORER_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_LAG_EXPORTER_enable) | default(false) %}
  #  ================================== Kafka Lag Exporter ========================================== #
  kafka-lag-exporter:
    image: seglo/kafka-lag-exporter:{{__KAFKA_LAG_EXPORTER_version}}
    container_name: kafka-lag-exporter
    hostname: kafka-lag-exporter
    labels:
      com.platys.name: "kafka-lag-exporter"
      com.platys.description: "Kafka Lag Exporter"
      com.platys.description: "Kafka GUI"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/kafka/kafka-lag-exporter:/opt/docker/conf
#      - $MONITORING_STACK/assets/prometheus/security:/etc/kafka/secrets
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and KAFKA_LAG_EXPORTER_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and REMORA_enable) | default(false) %}
  #  ================================== Remora ========================================== #
  remora:
    image: registry.opensource.zalan.do/machina/remora:{{__REMORA_version}}
    container_name: remora
    hostname: remora
    labels:
      com.platys.name: "remora"
      com.platys.description: "Kafka GUI"
      com.platys.restapi.title: 'Remora API'
      com.platys.restapi.url: "http://dataplatform:28256/consumers"
    ports:
      - 28256:9000
    environment:
      KAFKA_ENDPOINT: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      KAFKA_COMMAND_CONFIG: "/auth.properties"
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled %}
      - ./security/remora/auth.properties:/auth.properties
    {% endif -%}   {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KAFKA_enable and REMORA_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and BURROW_enable) | default(false) %}
  #  ================================== Burrow ========================================== #
  burrow:
    image: ghcr.io/linkedin/burrow/burrow:{{__BURROW_version}}
    container_name: burrow
    hostname: burrow
    labels:
      com.platys.name: 'burrow'
      com.platys.description: "Kafka Consumer Lag Checking"
      com.platys.webui.title: 'Burrow'
      com.platys.webui.url: "http://dataplatform:28260"
    ports:
      - 28260:8000
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/burrow:/etc/burrow/
      - ./tmp:/var/tmp/burrow
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if (BURROW_enable and BURROW_UI_enable) | default(false) %}
  burrow-ui:
    image: generalmills/burrowui:{{__BURROW_UI_version}}
    container_name: burrow-ui
    hostname: burrow-ui
    labels:
      com.platys.name: 'burrow-ui'
      com.platys.description: "Kafka Consumer Lag Checking"
      com.platys.webui.title: 'Burrow UI'
      com.platys.webui.url: "http://dataplatform:28258"
    ports:
      - 28258:3000
    environment:
      BURROW_HOME: "http://dataplatform:28110/v3/kafka"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  BURROW_enable and BURROW_UI_enable #}

  {% if (BURROW_enable and BURROW_DASHBOARD_enable) | default(false) %}
  burrow-dashboard:
    image: joway/burrow-dashboard:{{__BURROW_DASHBOARD_version}}
    container_name: burrow-dashboard
    hostname: burrow-dashboard
    labels:
      com.platys.name: 'burrow-dashboard'
      com.platys.description: "Kafka Consumer Lag Checking"
      com.platys.webui.title: 'Burrow Dashboard'
      com.platys.webui.url: "http://dataplatform:28259"
    ports:
      - 28259:80
    environment:
      BURROW_BACKEND: "http://dataplatform:28110"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  BURROW_enable and BURROW_DASHBOARD_enable #}
{% endif %}   {# KAFKA_enable and BURROW_enable #}

{% if DEBEZIUM_SERVER_enable | default(false) %}
  #  ================================== Debezium UI ========================================== #
  debezium-server:
    image: debezium/server:{{__DEBEZIUM_SERVER_version}}
    container_name: debezium-servers
    hostname: debezium-server
    labels:
      com.platys.name: "debezium-server"
      com.platys.description: "Application that streams change events to messaging infrastructures"
      com.platys.webui.title: 'Debezium Sever Health Endpoint'
      com.platys.webui.url: "http://dataplatform:28246"
    ports:
      - "28246:8080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./debezium-server/conf:/debezium/conf
    {% if DEBEZIUM_SERVER_volume_map_data %}
      - ./container-volume/debezium-server/data:/debezium/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# DEBEZIUM_SERVER_enable #}

{% if ((KAFKA_enable or external['KAFKA_enable']) and KAFKA_CONNECT_enable and DEBEZIUM_UI_enable) | default(false) %}
  #  ================================== Debezium UI ========================================== #
  debezium-ui:
    image: debezium/debezium-ui:{{__DEBEZIUM_UI_version}}
    container_name: debezium-ui
    hostname: debezium-ui
    labels:
      com.platys.name: "debezium-ui"
      com.platys.description: "Debezium GUI"
      com.platys.webui.title: 'Debezium UI'
      com.platys.webui.url: "http://dataplatform:28227"
    ports:
      - "28227:8080"
    environment:
      KAFKA_CONNECT_URI: "http://dataplatform:8083/,http://${PUBLIC_IP}:8084/"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      interval: 10s
      retries: 3
      test: [CMD, curl, --fail, "http://localhost:8080"]
      timeout: 10s    
{% endif %}   {# KAFKA_enable and DEBEZIUM_UI_enable #}

{% if HADOOP_enable | default(false) %}

  #  ================================== Apache Hadoop ========================================== #
  namenode:
    image: bde2020/hadoop-namenode:{{__HADOOP_image_version}}
    container_name: namenode
    hostname: namenode
    labels:
      com.platys.name: "hadoop"
      com.platys.description: "Hadoop Namenode"
      com.platys.webui.name: "Hadoop NameNode UI"
      com.platys.webui.url: "http://dataplatform:9870"
    ports:
      - "9870:9870"
    env_file:
      - ./conf/hadoop.env
    environment:
      CLUSTER_NAME: 'test'
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/hadoop/namenode:/hadoop/dfs/name
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% for num in range(HADOOP_datanodes| default(1) ) %}
    {% set external_port = 9864 + loop.index - 1 %}

  datanode-{{loop.index}}:
    image: bde2020/hadoop-datanode:{{__HADOOP_image_version}}
    container_name: datanode-{{loop.index}}
    labels:
      com.platys.name: "hadoop"
      com.platys.description: "Hadoop Datanode"
      com.platys.webui.title: 'Hadoop DataNode-{{loop.index}} UI'
      com.platys.webui.url: "http://dataplatform:{{external_port}}"
    ports:
      - "{{external_port}}:9864"
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      SERVICE_PRECONDITION: "namenode:9870"
      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/hadoop/datanode-{{loop.index}}:/hadoop/dfs/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:{{__HADOOP_image_version}}
    container_name: resourcemanager
    hostname: resourcemanager
    labels:
      com.platys.name: "hadoop"
      com.platys.description: "Hadoop Resource Manager"
      com.platys.webui.title: "YARN RessourceManager UI"
      com.platys.webui.url: "http://dataplatform:18088"
    ports:
      - "18088:8088"
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      YARN_CONF_yarn_resourcemanager_webapp_address: 'resourcemanager:8088'
      YARN_CONF_yarn_nodemanager_webapp_address: 'nodemanager:8042'
      YARN_CONF_yarn_timeline___service_webapp_address: '${PUBLIC_IP}:18188'
      YARN_CONF_yarn_log_server_url: '${PUBLIC_IP}:18188'
      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  nodemanager:
    image: bde2020/hadoop-nodemanager:{{__HADOOP_image_version}}
    container_name: nodemanager
    hostname: nodemanager
    labels:
      com.platys.name: "hadoop"
      com.platys.description: "Hadoop Node Manager"
      com.platys.webui.title: "YARN NodeManager UI"
      com.platys.webui.url: "http://dataplatform:18042"
    ports:
      - "18042:8042"
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      YARN_CONF_yarn_resourcemanager_webapp_address: 'resourcemanager:8088'
      YARN_CONF_yarn_nodemanager_webapp_address: 'nodemanager:8042'
      YARN_CONF_yarn_timeline___service_webapp_address: '${PUBLIC_IP}:18188'
      YARN_CONF_yarn_log_server_url: '${PUBLIC_IP}:18188'
      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  historyserver:
    image: bde2020/hadoop-historyserver:{{__HADOOP_image_version}}
    container_name: historyserver
    hostname: historyserver
    labels:
      com.platys.name: "hadoop"
      com.platys.description: "Hadoop History Server"
      com.platys.webui.title: "YARN History Server"
      com.platys.webui.url: "http://dataplatform:18188"
    ports:
      - "18188:8188"
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      SERVICE_PRECONDITION: "namenode:9870 datanode-1:9864 resourcemanager:18088"
      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if SQOOP_enable | default(false) %}
  sqoop:
    image: trivadis/apache-sqoop:{{__SQOOP_version}}
    container_name: sqoop
    hostname: sqoop
    labels:
      com.platys.name: "sqoop"
      com.platys.description: "Sqoop Data Loader"
    env_file:
      - ./conf/hadoop.env
    environment:
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      HIVE_SITE_CONF_fs_s3a_endpoint: {{s3Endpoint | default(omit) }}
      HIVE_SITE_CONF_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      HIVE_SITE_CONF_fs_s3a_secret_key: {{s3SecretAccessKey}}
      HIVE_SITE_CONF_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    {% endif %}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: tail -f /dev/null
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  SQOOP_enable #}

{% endif %}   {#  HADOOP_enable #}

{% if SPARK_enable | default(false) %}
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: bitnami/spark:{{__SPARK_version}}
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.name: 'spark'
      com.platys.description: "Spark Master Node"
      com.platys.webui.title: 'Spark UI'
      com.platys.webui.url: "http://dataplatform:28304"
    ports:
      - 28304:28304
      - 6066:6066
      - 7077:7077
      - "4040-4044:4040-4044"
    environment:
      # bitnami env vars
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_MASTER_WEBUI_PORT: 28304
      SPARK_MASTER_OPTS: '{{SPARK_master_opts if SPARK_master_opts is defined and SPARK_master_opts else omit }}'
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
#     INIT_DAEMON_STEP: setup_spark
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: {{s3SecretAccessKey}}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
    {% endif %}
    {%if HADOOP_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: hdfs://namenode:9000
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'hdfs://namenode:9000/user/hive/warehouse'}}
    {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://{{s3AdminBucketName}}
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/hive/warehouse'}}
    {% else %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: file:///tmp
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'file:///hive/warehouse'}}
    {% endif -%}   {#  HADOOP_enable #}
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
    {%if (SPARK_catalog | lower) == 'in-memory' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
    {% else %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
    {% endif -%}   {#  SPARK_catalog #}
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions: {{SPARK_driver_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions: {{SPARK_executor_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_cores_max: {{SPARK_cores_max if SPARK_cores_max is defined and SPARK_cores_max else omit }}
      SPARK_DEFAULTS_CONF_spark_executor_memory: {{SPARK_executor_memory if SPARK_executor_memory is defined and SPARK_executor_memory else omit }}
#      SPARK_DEFAULTS_CONF_spark_sql_extensions: {{SPARK_sql_extensions if SPARK_sql_extensions is defined and SPARK_sql_extensions else omit }}
#      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: {{SPARK_sql_catalog_spark_catalog if SPARK_sql_catalog_spark_catalog is defined and SPARK_sql_catalog_spark_catalog else omit }}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      SPARK_DEFAULTS_CONF_spark_extraListeners: 'datahub.spark.DatahubSparkListener'
      SPARK_DEFAULTS_CONF_spark_datahub_rest_server: {{ns.gmsUrl}}
      SPARK_DEFAULTS_CONF_spark_datahub_metadata_dataset_env: {{env}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
    {%if SPARK_table_format_type == 'iceberg' and not NESSIE_enable and not LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: iceberg
    {% elif SPARK_table_format_type == 'iceberg' and NESSIE_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_uri: http://nessie:19120/api/v1
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_authentication_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: nessie
    {% elif SPARK_table_format_type == 'iceberg' and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_catalog__impl: io.lakefs.iceberg.LakeFSCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_uri: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_warehouse: {{'lakefs://' ~ LAKEFS_quickstart_repo_name | default('demo')}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: lakefs
    {%elif SPARK_table_format_type == 'delta' and not UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
    {%elif SPARK_table_format_type == 'delta' and UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_uri: 'http://unity-catalog:8080'     
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_token: ''     
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: my_catalog      
    {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.hudi.catalog.HoodieCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.spark.sql.hudi.HoodieSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_serializer: org.apache.spark.serializer.KryoSerializer
    {% endif -%}   {#  SPARK_table_format_type #}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories: {{SPARK_jars_repositories}}
      SPARK_DEFAULTS_CONF_spark_jars_packages: "{{SPARK_jars_packages}}"
      SPARK_DEFAULTS_CONF_spark_jars_excludes: {{SPARK_jars_excludes}}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script (triggered in the init script)
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}},{{ns.datahubSparkPluginDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}{{',' if ns.sparkTableFormatJar | length}}{{ns.datahubSparkPluginJar | default('')}}
    {% else %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings: {{SPARK_jars_ivySettings}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./scripts/spark/pyspark:/opt/bitnami/spark/bin/pyspark
      - ./container-volume/spark/logs/:/var/log/spark/logs
#      - ./scripts/docker/maven-download.sh:/usr/src/app/maven-download.sh
    {%if AIRFLOW_enable or ZEPPELIN_enable or JUPYTER_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/bitnami/spark
    {% endif -%}   {#  AIRFLOW_enable or ZEPPELIN_enable or JUPYTER_enable  #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "spark-submit", "--version"]
      interval: 30s
      timeout: 10s
      retries: 5

  {% for num in range(SPARK_workers | default(1) ) %}
      {% set external_port = 28111 + loop.index - 1 %}
      {% set webui_port = 28111 + loop.index - 1 %}

  spark-worker-{{loop.index}}:
    image: bitnami/spark:{{__SPARK_version}}
    container_name: spark-worker-{{loop.index}}
    hostname: spark-worker-{{loop.index}}
    labels:
      com.platys.name: 'spark'
      com.platys.description: "Spark Worker Node"
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "{{external_port}}:{{webui_port}}"
    environment:
      # bitnami env vars
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_WORKER_WEBUI_PORT: "{{webui_port}}"
      SPARK_WORKER_CORES: '{{SPARK_worker_cores if SPARK_worker_cores is defined and SPARK_worker_cores else omit }}'
      SPARK_WORKER_MEMORY: '{{SPARK_worker_memory if SPARK_worker_memory is defined and SPARK_worker_memory else omit }}'
      SPARK_WORKER_OPTS: '{{SPARK_worker_opts if SPARK_worker_opts is defined and SPARK_worker_opts else omit }}'
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: {{s3SecretAccessKey}}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
    {% endif %}
    {%if HADOOP_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: hdfs://namenode:9000
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'hdfs://namenode:9000/user/hive/warehouse'}}
    {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://{{s3AdminBucketName}}
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/hive/warehouse'}}
    {% else %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: file:///tmp
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'file:///hive/warehouse'}}
    {% endif -%}   {#  HADOOP_enable #}
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
    {%if (SPARK_catalog | lower) == 'in-memory' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
    {% else %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
    {% endif -%}   {#  SPARK_catalog #}
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions: {{SPARK_driver_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions: {{SPARK_executor_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_cores_max: {{SPARK_cores_max if SPARK_cores_max is defined and SPARK_cores_max else omit }}
      SPARK_DEFAULTS_CONF_spark_executor_memory: {{SPARK_executor_memory if SPARK_executor_memory is defined and SPARK_executor_memory else omit }}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      SPARK_DEFAULTS_CONF_spark_extraListeners: 'datahub.spark.DatahubSparkListener'
      SPARK_DEFAULTS_CONF_spark_datahub_rest_server: {{ns.gmsUrl}}
      SPARK_DEFAULTS_CONF_spark_datahub_metadata_dataset_env: {{env}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
    {%if SPARK_table_format_type == 'iceberg' and not NESSIE_enable and not LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: iceberg
    {% elif SPARK_table_format_type == 'iceberg' and NESSIE_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_uri: http://nessie:19120/api/v1
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_authentication_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: nessie
    {% elif SPARK_table_format_type == 'iceberg' and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_catalog__impl: io.lakefs.iceberg.LakeFSCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_uri: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_warehouse: {{'lakefs://' ~ LAKEFS_quickstart_repo_name | default('demo')}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: lakefs
    {%elif SPARK_table_format_type == 'delta' and not UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
    {%elif SPARK_table_format_type == 'delta' and UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_uri: 'http://unity-catalog:8080'     
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_token: ''     
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: my_catalog      
    {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.hudi.catalog.HoodieCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.spark.sql.hudi.HoodieSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_serializer: org.apache.spark.serializer.KryoSerializer
    {% endif -%}   {#  SPARK_table_format_type #}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories: {{SPARK_jars_repositories}}
      SPARK_DEFAULTS_CONF_spark_jars_packages: "{{SPARK_jars_packages}}{{',' if SPARK_jars_packages | length}}{{SPARK_datahub_package | default('')}}"
      SPARK_DEFAULTS_CONF_spark_jars_excludes: {{SPARK_jars_excludes}}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}},{{ns.datahubSparkPluginDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}{{',' if ns.sparkTableFormatJar | length}}{{ns.datahubSparkPluginJar | default('')}}
    {% else %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings: {{SPARK_jars_ivySettings}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% endfor %}

  {% if SPARK_HISTORY_enable | default(false) %}
  spark-history:
    image: bitnami/spark:{{__SPARK_version}}
    container_name: spark-history
    hostname: spark-history
    labels:
      com.platys.name: 'spark-historyserver'
      com.platys.description: "Spark History Server"
      com.platys.webui.title: "Spark History Server"
      com.platys.webui.url: "http://dataplatform:28117"
      com.platys.restapi.title: "Spark History Server"
      com.platys.restapi.url: "http://dataplatform:28117/api/v1"
    expose:
      - 18080
    ports:
      - 28117:18080
    environment:
      SPARK_DEFAULTS_CONF_spark_jars_repositories: {{SPARK_jars_repositories}}
      SPARK_DEFAULTS_CONF_spark_jars_packages: "{{SPARK_jars_packages}}{{',' if SPARK_jars_packages | length}}{{SPARK_datahub_package | default('')}}"
      SPARK_DEFAULTS_CONF_spark_jars_excludes: {{SPARK_jars_excludes}}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}},{{ns.datahubSparkPluginDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}{{',' if ns.sparkTableFormatJar | length}}{{ns.datahubSparkPluginJar | default('')}}
    {% else %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings: {{SPARK_jars_ivySettings}}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: {{s3Endpoint | default(omit) }}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
    {% endif -%}    {#  MINIO_enable #}
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
    {%if (SPARK_catalog | lower) == 'in-memory' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
    {% else %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
    {% endif -%}   {#  SPARK_catalog #}
    {%if HADOOP_enable | default(false) %}
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'hdfs://namenode:9000/user/hive/warehouse'}}
    {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      CORE_CONF_fs_defaultFS: s3a://{{s3AdminBucketName}}
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/hive/warehouse'}}
    {% else %}
      CORE_CONF_fs_defaultFS: file:///tmp
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'file:///hive/warehouse'}}
    {% endif -%}   {#  HADOOP_enable #}
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: '/opt/bitnami/spark/hive-site.xml'
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions: {{SPARK_driver_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions: {{SPARK_executor_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_cores_max: {{SPARK_cores_max if SPARK_cores_max is defined and SPARK_cores_max else omit }}
      SPARK_DEFAULTS_CONF_spark_executor_memory: {{SPARK_executor_memory if SPARK_executor_memory is defined and SPARK_executor_memory else omit }}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      SPARK_DEFAULTS_CONF_spark_extraListeners: 'datahub.spark.DatahubSparkListener'
      SPARK_DEFAULTS_CONF_spark_datahub_rest_server: {{ns.gmsUrl}}
      SPARK_DEFAULTS_CONF_spark_datahub_metadata_dataset_env: {{env}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
    {%if SPARK_table_format_type == 'iceberg' and not NESSIE_enable and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: iceberg
    {% elif SPARK_table_format_type == 'iceberg' and NESSIE_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_uri: http://nessie:19120/api/v1
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_authentication_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: nessie
    {% elif SPARK_table_format_type == 'iceberg' and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_catalog__impl: io.lakefs.iceberg.LakeFSCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_uri: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_warehouse: {{'lakefs://' ~ LAKEFS_quickstart_repo_name | default('demo')}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: lakefs
    {%elif SPARK_table_format_type == 'delta' and not UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
    {%elif SPARK_table_format_type == 'delta' and UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_uri: 'http://unity-catalog:8080'     
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_token: ''     
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: my_catalog      
    {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.hudi.catalog.HoodieCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.spark.sql.hudi.HoodieSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_serializer: org.apache.spark.serializer.KryoSerializer
    {% endif -%}   {#  SPARK_table_format_type #}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    restart: {{container_restart_policy}}
  {% endif %}   {#  SPARK_HISTORY_enable #}
{% endif %}   {#  SPARK_enable #}

{% if SPARK_THRIFTSERVER_enable | default(false) %}
  spark-thriftserver:
    image: bitnami/spark:{{__SPARK_version}}
    container_name: spark-thriftserver
    hostname: spark-thriftserver
    labels:
      com.platys.name: 'spark-thriftserver'
      com.platys.description: "Spark Thriftserver"
      com.platys.webui.title: "Spark Thriftserver UI"
      com.platys.webui.url: "http://dataplatform:28298"
    ports:
      - "28118:10000"
      - "28298:4040"
    environment:
    {%if SPARK_THRIFTSERVER_use_local_spark | default(false) %}
      SPARK_MASTER_URL: 'local[*]'
    {% else %}
      SPARK_MASTER_URL: '{{ns.sparkMasterUrl}}'
    {% endif -%}   {#  SPARK_THRIFTSERVER_use_local_spark  #}          
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: {{s3SecretAccessKey}}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
    {% endif %}
    {%if HADOOP_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: hdfs://namenode:9000
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'hdfs://namenode:9000/user/hive/warehouse'}}
    {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://{{s3AdminBucketName}}
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/hive/warehouse'}}
    {% else %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: file:///tmp
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'file:///hive/warehouse'}}
    {% endif -%}   {#  HADOOP_enable #}
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
    {%if (SPARK_catalog | lower) == 'in-memory' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
    {% else %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
    {% endif -%}   {#  SPARK_catalog #}

    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      SPARK_DEFAULTS_CONF_spark_extraListeners: 'datahub.spark.DatahubSparkListener'
      SPARK_DEFAULTS_CONF_spark_datahub_metadata_dataset_env: {{env}}
      SPARK_DEFAULTS_CONF_spark_datahub_rest_server: {{ns.gmsUrl}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
    {%if SPARK_table_format_type == 'iceberg' and not NESSIE_enable and not LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: iceberg
    {% elif SPARK_table_format_type == 'iceberg' and NESSIE_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_uri: http://nessie:19120/api/v1
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_authentication_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: nessie
    {% elif SPARK_table_format_type == 'iceberg' and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_catalog__impl: io.lakefs.iceberg.LakeFSCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_uri: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_warehouse: {{'lakefs://' ~ LAKEFS_quickstart_repo_name | default('demo')}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: lakefs
    {%elif SPARK_table_format_type == 'delta' and not UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
    {%elif SPARK_table_format_type == 'delta' and UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_uri: 'http://unity-catalog:8080'     
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_token: ''     
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: my_catalog      
    {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.hudi.catalog.HoodieCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.spark.sql.hudi.HoodieSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_serializer: org.apache.spark.serializer.KryoSerializer
    {% endif -%}   {#  SPARK_table_format_type #}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions: {{SPARK_driver_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions: {{SPARK_executor_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_cores_max: {{SPARK_THRIFTSERVER_cores_max if SPARK_THRIFTSERVER_cores_max is defined and SPARK_THRIFTSERVER_cores_max else omit }}
      SPARK_DEFAULTS_CONF_spark_executor_memory: {{SPARK_THRIFTSERVER_executor_memory if SPARK_THRIFTSERVER_executor_memory is defined and SPARK_THRIFTSERVER_executor_memory else omit }}
      SPARK_DEFAULTS_CONF_spark_jars_repositories: {{SPARK_jars_repositories}}
      SPARK_DEFAULTS_CONF_spark_jars_packages: "{{SPARK_jars_packages}}{{',' if SPARK_jars_packages | length}}{{SPARK_datahub_package | default('')}}"
      SPARK_DEFAULTS_CONF_spark_jars_excludes: {{SPARK_jars_excludes}}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}},{{ns.datahubSparkPluginDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}{{',' if ns.sparkTableFormatJar | length}}{{ns.datahubSparkPluginJar | default('')}}
    {% else %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings: {{SPARK_jars_ivySettings}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      # we override the run.sh script to start thriftserver instead of master/worker
      - ./scripts/spark/run-thriftserver.sh:/opt/bitnami/scripts/spark/run.sh
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SPARK_THRIFTSERVER_enable #}


{% if KYUUBI_enable | default(false) %}
  kyuubi:
    image: apache/kyuubi:{{__KYUUBI_version}}
    container_name: kyuubi
    hostname: kyuubi
    labels:
      com.platys.name: 'kyuubi'
      com.platys.description: "gateway to provide serverless SQL"
      com.platys.webui.title: "Kyuubi UI"
      com.platys.webui.url: "http://dataplatform:10099"
      com.platys.restapi.title: "Kyuubi REST API"
      com.platys.restapi.url: "http://dataplatform:10099/api/v1/sessions"      
    ports:
      - 28385-28387:4040-4042    
      - "10009:10009"
      - "10099:10099"
    environment:
      KYUUBI_DEFAULTS_CONF_kyuubi_authentication: 'NONE'
      KYUUBI_DEFAULTS_CONF_kyuubi_frontend_protocols: {{KYUUBI_frontend_protocols if KYUUBI_frontend_protocols is defined and KYUUBI_frontend_protocols and KYUUBI_frontend_protocols | length else omit}}
    {% if (KAFKA_enable or external['KAFKA_enable']) and 'streams' in NEO4J_download_plugins | default(false) %}
      KYUUBI_DEFAULTS_CONF_kyuubi_backend_server_event_loggers: KAFKA
      KYUUBI_DEFAULTS_CONF_kyuubi_backend_server_event_kafka_bootstrap_servers: {{ns.bootstrapServers}}
      KYUUBI_DEFAULTS_CONF_kyuubi_backend_server_event_kafka_topic: kyuubi-log
    {% endif -%}   {#  KAFKA_enable #}
      KYUUBI_DEFAULTS_CONF_kyuubi_engine_type: {{KYUUBI_engine_type | upper if KYUUBI_engine_type is defined and KYUUBI_engine_type and KYUUBI_engine_type | length else omit}}
    {%if KYUUBI_engine_type | upper == 'TRINO' | default(false) %}
      KYUUBI_DEFAULTS_CONF_kyuubi_session_engine_trino_connection_url: http://trino-1:8080
      KYUUBI_DEFAULTS_CONF_kyuubi_session_engine_trino_connection_catalog: minio
      KYUUBI_DEFAULTS_CONF_kyuubi_session_engine_trino_connection_user: trino
    {% endif -%}   {#  KYUUBI_engine_type  #}    
    {% if KYUUBI_engine_type | upper == 'SPARK_SQL' | default(false) %}
      {%if  KYUUBI_use_local_spark | default(false) %}
      SPARK_MASTER_URL: 'local[*]'
      {% else %}
      SPARK_MASTER_URL: '{{ns.sparkMasterUrl}}'
      {% endif -%}   {#  KYUUBI_use_local_spark  #}    
      {%if MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: {{s3SecretAccessKey}}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
      {% endif %} 
      {%if HADOOP_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: hdfs://namenode:9000
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{KYUUBI_local_spark_sql_warehouse_dir if KYUUBI_local_spark_sql_warehouse_dir is defined and KYUUBI_local_spark_sql_warehouse_dir | length else 'hdfs://namenode:9000/user/hive/warehouse'}}
      {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://{{s3AdminBucketName}}
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{KYUUBI_local_spark_sql_warehouse_dir if KYUUBI_local_spark_sql_warehouse_dir is defined and KYUUBI_local_spark_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/hive/warehouse'}}
      {% else %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: file:///tmp
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{KYUUBI_local_spark_sql_warehouse_dir if KYUUBI_local_spark_sql_warehouse_dir is defined and KYUUBI_local_spark_sql_warehouse_dir | length else 'file:///hive/warehouse'}}
      {% endif -%}   {#  HADOOP_enable #}
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      {%if (KYUUBI_local_spark_catalog | lower) == 'in-memory' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      {% else %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      SPARK_DEFAULTS_CONF_spark_hadoop_hive_metastore_uris: thrift://hive-metastore:9083
      {% endif -%}   {#  KYUUBI_local_spark_catalog #}
      {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      SPARK_DEFAULTS_CONF_spark_extraListeners: 'datahub.spark.DatahubSparkListener'
      SPARK_DEFAULTS_CONF_spark_datahub_rest_server: {{ns.gmsUrl}}
      SPARK_DEFAULTS_CONF_spark_datahub_metadata_dataset_env: {{env}}
      {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      {%if SPARK_table_format_type == 'iceberg' and not NESSIE_enable and not LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: iceberg
      {% elif SPARK_table_format_type == 'iceberg' and NESSIE_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_uri: http://nessie:19120/api/v1
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_authentication_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: nessie
      {% elif SPARK_table_format_type == 'iceberg' and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_catalog__impl: io.lakefs.iceberg.LakeFSCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_uri: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_warehouse: {{'lakefs://' ~ LAKEFS_quickstart_repo_name | default('demo')}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: lakefs
      {%elif SPARK_table_format_type == 'delta' and not UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      {%elif SPARK_table_format_type == 'delta' and UNITY_CATALOG_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog: io.unitycatalog.spark.UCSingleCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_uri: 'http://unity-catalog:8080'     
      SPARK_DEFAULTS_CONF_spark_sql_catalog_my__catalog_token: ''     
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: my_catalog
      {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.hudi.catalog.HoodieCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.spark.sql.hudi.HoodieSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_serializer: org.apache.spark.serializer.KryoSerializer
      {% endif -%}   {#  SPARK_table_format_type #}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories: {{KYUUBI_local_spark_jars_repositories}}
      SPARK_DEFAULTS_CONF_spark_jars_packages: "{{KYUUBI_local_spark_jars_packages}}"
      SPARK_DEFAULTS_CONF_spark_jars_excludes: {{KYUUBI_local_spark_jars_excludes}}
      {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script (triggered in the init script)
      SPARK_INSTALL_JARS_PACKAGES: '{{KYUUBI_local_spark_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}},{{ns.datahubSparkPluginDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{KYUUBI_local_spark_jars}}{{',' if KYUUBI_local_spark_jars | length}}{{ns.sparkTableFormatJar | default('')}}{{',' if ns.sparkTableFormatJar | length}}{{ns.datahubSparkPluginJar | default('')}}
      {% else %}
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{KYUUBI_local_spark_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{KYUUBI_local_spark_jars}}{{',' if KYUUBI_local_spark_jars | length}}{{ns.sparkTableFormatJar | default('')}}
      {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings: {{KYUUBI_local_spark_jars_ivySettings}}    
    {% endif -%}   {#  KYUUBI_engine_type | upper == 'SPARK_SQL' #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/kyuubi/kyuubi-defaults.conf:/opt/kyuubi/conf.default/kyuubi-defaults.conf
      - ./conf/spark/hive-site.xml:/opt/kyuubi/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/kyuubi/conf.default/spark-defaults.conf
      - ./container-volume/spark/logs/:/var/log/spark/logs            
      - ./scripts/docker/maven-download.sh:/maven-download.sh      
      - ./scripts/kyuubi/command-override.sh:/opt/kyuubi/bin/command-override.sh
      - ./init/kyuubi:/docker-entrypoint-initdb.d
    {%if not KYUUBI_use_local_spark and SPARK_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/bitnami/spark:RO
    {% endif -%}   {#  SPARK_enable #}
    {%if FLINK_enable | default(false) %}
      - flink-vol:/opt/flink:RO
    {% endif -%}   {#  FLINK_enable #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      /opt/kyuubi/bin/command-override.sh    
    restart: {{container_restart_policy}}
{% endif %}   {#  KYUUBI_enable #}

{% if SPARK_enable and LIVY_enable | default(false) %}
  #  ================================== Apache Livy ========================================== #
  livy:
    image: trivadis/apache-livy:{{__LIVY_version}}
    container_name: livy
    hostname: livy
    labels:
      com.platys.name: 'livy'
      com.platys.description: "Spark Livy"
      com.platys.webui.title: "Livy Server UI"
      com.platys.webui.url: "http://dataplatform:8998/ui#"
      com.platys.restapi.title: "Livy API"
      com.platys.restapi.url: "http://dataplatform:8998/"
    environment:
      SPARK_MASTER: spark://spark-master:7077
      DEPLOY_MODE: client
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      HIVE_SITE_CONF_fs_defaultFS: 's3a://{{s3AdminBucketName}}'
      HIVE_SITE_CONF_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      HIVE_SITE_CONF_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      HIVE_SITE_CONF_fs_s3a_secret_key: {{s3SecretAccessKey}}
      HIVE_SITE_CONF_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
    {% endif %}
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
      HIVE_SITE_CONF_hive_metastore_event_db_notification_api_auth: false      
    {%if HADOOP_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: hdfs://namenode:9000
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'hdfs://namenode:9000/user/hive/warehouse'}}
    {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://{{s3AdminBucketName}}
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/hive/warehouse'}}
    {% else %}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: file:///tmp
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 'file:///hive/warehouse'}}
    {% endif -%}   {#  HADOOP_enable #}
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
    {%if (SPARK_catalog | lower) == 'in-memory' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
    {% else %}
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
    {% endif -%}   {#  SPARK_catalog #}
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: '/spark/conf/hive-site.xml'
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions: {{SPARK_driver_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions: {{SPARK_executor_extraJavaOptions}}
      SPARK_DEFAULTS_CONF_spark_cores_max: {{SPARK_cores_max if SPARK_cores_max is defined and SPARK_cores_max else omit }}
      SPARK_DEFAULTS_CONF_spark_executor_memory: {{SPARK_executor_memory if SPARK_executor_memory is defined and SPARK_executor_memory else omit }}
  {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      SPARK_DEFAULTS_CONF_spark_extraListeners: 'datahub.spark.DatahubSparkListener'
      SPARK_DEFAULTS_CONF_spark_datahub_rest_server: {{ns.gmsUrl}}
      SPARK_DEFAULTS_CONF_spark_datahub_metadata_dataset_env: {{env}}
  {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
  {%if SPARK_table_format_type == 'iceberg' and not NESSIE_enable and not LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_iceberg_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: iceberg
  {% elif SPARK_table_format_type == 'iceberg' and NESSIE_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_uri: http://nessie:19120/api/v1
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_authentication_type: nessie
      SPARK_DEFAULTS_CONF_spark_sql_catalog_nessie_warehouse: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: nessie
    {% elif SPARK_table_format_type == 'iceberg' and LAKEFS_enable | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog_type: hive
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_catalog__impl: io.lakefs.iceberg.LakeFSCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_ref: main
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_uri: '{{s3Endpoint | default(omit) }}'
      SPARK_DEFAULTS_CONF_spark_sql_catalog_lakefs_warehouse: {{'lakefs://' ~ LAKEFS_quickstart_repo_name | default('demo')}}
      SPARK_DEFAULTS_CONF_spark_sql_defaultCatalog: lakefs
  {%elif SPARK_table_format_type == 'delta' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: io.delta.sql.DeltaSparkSessionExtension
  {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.spark.sql.hudi.catalog.HoodieCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.spark.sql.hudi.HoodieSparkSessionExtension
      SPARK_DEFAULTS_CONF_spark_serializer: org.apache.spark.serializer.KryoSerializer
  {% endif -%}   {#  SPARK_table_format_type #}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories: {{SPARK_jars_repositories}}
      SPARK_DEFAULTS_CONF_spark_jars_packages: "{{SPARK_jars_packages}}{{',' if SPARK_jars_packages | length}}{{SPARK_datahub_package | default('')}}"
      SPARK_DEFAULTS_CONF_spark_jars_excludes: {{SPARK_jars_excludes}}
    {%if (DATAHUB_enable or external['DATAHUB_enable'] and SPARK_datahub_agent_enable) | default(false) %}
      # specifies the JARs to be downloaded by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}},{{ns.datahubSparkPluginDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}{{',' if ns.sparkTableFormatJar | length}}{{ns.datahubSparkPluginJar | default('')}}
    {% else %}
      # specifies the JARs to be downloaded by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: '{{SPARK_install_jars_packages}},{{ns.sparkDeltaDependencies}},{{ns.sparkIcebergDependencies}},{{ns.sparkHudiDependencies}}'
      SPARK_DEFAULTS_CONF_spark_jars: {{SPARK_jars}}{{',' if SPARK_jars | length}}{{ns.sparkTableFormatJar | default('')}}
    {% endif -%}   {#  DATAHUB_enable or external['DATAHUB_enable'] #}
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings: {{SPARK_jars_ivySettings}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    ports:
      - "8998:8998"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SPARK_livy_enable #}

{% if FLINK_enable | default(false) %}
  #  ================================== Apache Flink ========================================== #
  flink-jobmanager:
    image: {{'trivadis/apache-flink' if FLINK_python_enabled else 'flink'}}:{{__FLINK_version}}{{'-python' ~ FLINK_python_version if FLINK_python_enabled}}
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    labels:
      com.platys.name: 'flink'
      com.platys.description: "Flink Job Manager"
      com.platys.webui.title: "Flink JobManager UI"
      com.platys.webui.url: "http://dataplatform:28237/"
    ports:
      - "28237:8081"
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.memory.process.size: 1600m
        rest.bind-port: 8081
        execution.checkpointing.interval: 5000
        table.catalog-store.kind: file
        table.catalog-store.file.path: ./conf/catalogs
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints-directory
        state.savepoints.dir: file:///tmp/flink-savepoints-directory
#        rest.flamegraph.enabled: true
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - s3.access.key={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - s3.secret.key={{s3SecretAccessKey}}
      - s3.endpoint={{s3Endpoint | default(omit) }}'
      - s3.path.style.access='{{s3PathStyleAccess}}'
    {% endif -%}    {#  MINIO_enable #}
      - ENABLE_BUILT_IN_PLUGINS={{FLINK_enable_build_in_plugins if FLINK_enable_build_in_plugins is defined and FLINK_enable_build_in_plugins and FLINK_enable_build_in_plugins != None and FLINK_enable_build_in_plugins | length else omit}}
      - FLINK_INSTALL_MAVEN_DEP={{FLINK_install_maven_dep if FLINK_install_maven_dep is defined and FLINK_install_maven_dep and FLINK_install_maven_dep != None and FLINK_install_maven_dep | length else omit}}
      - FLINK_INSTALL_FILE_DEP={{FLINK_install_file_dep if FLINK_install_file_dep is defined and FLINK_install_file_dep and FLINK_install_file_dep != None and FLINK_install_file_dep | length else omit}}
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - FLINK_PYTHON_PROVIDE_REQUIREMENTS_FILE=true
    {% endif -%}    {#  NIFI_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - METASTORE_HOST=hive-metastore
      - METASTORE_PORT=9083
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/:/platys-scripts/
      - ./scripts/flink/docker-entrypoint-override.sh:/docker-entrypoint-override.sh
      - flink-vol:/opt/flink
    {%if FLINK_deployment_mode == 'application' | default(false) %}
      - /applications/flink:/opt/flink/usrlib
    {% endif -%}
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - ./custom-conf/flink/requirements.txt:/opt/flink/requirements.txt
    {% endif -%}    {#  FLINK_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - ./conf/hive-metastore/hive-site.xml:/opt/hive-conf/hive-site.xml
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if FLINK_deployment_mode == 'session' | default(false) %}
    entrypoint:
      # We override the entrypoint from the docker image
        /docker-entrypoint-override.sh
    command: jobmanager
    {%elif FLINK_deployment_mode == 'application'  | default(false) %}
    command: standalone-job --job-classname com.job.ClassName [--job-id <job id>] [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] [job arguments]
    {% endif -%}   {#  FLINK_deployment_mode #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% for num in range(FLINK_taskmanagers | default(1) ) %}

  flink-taskmanager-{{loop.index}}:
    image: {{'trivadis/apache-flink' if FLINK_python_enabled else 'flink'}}:{{__FLINK_version}}{{'-python' ~ FLINK_python_version if FLINK_python_enabled}}
    container_name: flink-taskmanager-{{loop.index}}
    hostname: flink-taskmanager-{{loop.index}}
    labels:
      com.platys.name: 'flink'
      com.platys.description: "Flink Task Manager"
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 10
        taskmanager.memory.process.size: 1728m
        parallelism.default: 2
        execution.checkpointing.interval: 5000
        table.catalog-store.kind: file
        table.catalog-store.file.path: ./conf/catalogs
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints-directory
        state.savepoints.dir: file:///tmp/flink-savepoints-directory
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - s3.access.key={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - s3.secret.key={{s3SecretAccessKey}}
      - s3.endpoint={{s3Endpoint | default(omit) }}'
      - s3.path.style.access='{{s3PathStyleAccess}}'
    {% endif -%}    {#  MINIO_enable #}
      - ENABLE_BUILT_IN_PLUGINS={{FLINK_enable_build_in_plugins if FLINK_enable_build_in_plugins is defined and FLINK_enable_build_in_plugins and FLINK_enable_build_in_plugins != None and FLINK_enable_build_in_plugins | length else omit}}
      # the following 2 env variables are supported by the /docker-entrypoint-override.sh script
      - FLINK_INSTALL_MAVEN_DEP={{FLINK_install_maven_dep if FLINK_install_maven_dep is defined and FLINK_install_maven_dep and FLINK_install_maven_dep != None and FLINK_install_maven_dep | length else omit}}
      - FLINK_INSTALL_FILE_DEP={{FLINK_install_file_dep if FLINK_install_file_dep is defined and FLINK_install_file_dep and FLINK_install_file_dep != None and FLINK_install_file_dep | length else omit}}
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - FLINK_PYTHON_PROVIDE_REQUIREMENTS_FILE=true
    {% endif -%}    {#  NIFI_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - METASTORE_HOST=hive-metastore
      - METASTORE_PORT=9083
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/:/platys-scripts/
      - ./scripts/flink/docker-entrypoint-override.sh:/docker-entrypoint-override.sh
    {%if FLINK_deployment_mode == 'application' | default(false) %}
      - /applications/flink:/opt/flink/usrlib
    {% endif -%}   {#  FLINK_deployment_mode #}
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - ./custom-conf/flink/requirements.txt:/opt/flink/requirements.txt
    {% endif -%}    {#  FLINK_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - ./conf/hive-metastore/hive-site.xml:/opt/hive-conf/hive-site.xml
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: /docker-entrypoint-override.sh
    command: taskmanager
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  {% if FLINK_SQL_GATEWAY_enable | default(false) %}
  flink-sqlgateway:
    image: {{'trivadis/apache-flink' if FLINK_python_enabled else 'flink'}}:{{__FLINK_version}}{{'-python' ~ FLINK_python_version if FLINK_python_enabled}}
    container_name: flink-sqlgateway
    hostname: flink-sqlgateway
    labels:
      com.platys.name: 'flink'
      com.platys.description: "Flink SQL Gateway"
      com.platys.restapi.title: "Flink SQL Gateway REST API"
      com.platys.restapi.url: "http://dataplatform:28238/v1/info"
    ports:
      - "28238:8083"
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        rest.address: flink-jobmanager
        sql-gateway.endpoint.rest.address: 0.0.0.0
        table.catalog-store.kind: file
        table.catalog-store.file.path: ./conf/catalogs
      - ENABLE_BUILT_IN_PLUGINS={{FLINK_enable_build_in_plugins if FLINK_enable_build_in_plugins is defined and FLINK_enable_build_in_plugins and FLINK_enable_build_in_plugins != None and FLINK_enable_build_in_plugins | length else omit}}
      # the following 2 env variables are supported by the /docker-entrypoint-override.sh script
      - FLINK_INSTALL_MAVEN_DEP={{FLINK_install_maven_dep if FLINK_install_maven_dep is defined and FLINK_install_maven_dep and FLINK_install_maven_dep != None and FLINK_install_maven_dep | length else omit}}
      - FLINK_INSTALL_FILE_DEP={{FLINK_install_file_dep if FLINK_install_file_dep is defined and FLINK_install_file_dep and FLINK_install_file_dep != None and FLINK_install_file_dep | length else omit}}
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - FLINK_PYTHON_PROVIDE_REQUIREMENTS_FILE=true
    {% endif -%}    {#  NIFI_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - METASTORE_HOST=hive-metastore
      - METASTORE_PORT=9083
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/:/platys-scripts/
      - ./scripts/flink/docker-entrypoint-override.sh:/docker-entrypoint-override.sh
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - ./custom-conf/flink/requirements.txt:/opt/flink/requirements.txt
    {% endif -%}    {#  FLINK_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - ./conf/hive-metastore/hive-site.xml:/opt/hive-conf/hive-site.xml
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: /docker-entrypoint-override.sh
    command: /opt/flink/bin/sql-gateway.sh start-foreground
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  FLINK_SQL_GATEWAY_enable #}

  {% if FLINK_SQL_CLI_enable | default(false) %}
  flink-sql-cli:
    image: {{'trivadis/apache-flink' if FLINK_python_enabled else 'flink'}}:{{__FLINK_version}}{{'-python' ~ FLINK_python_version if FLINK_python_enabled}}
    container_name: flink-sql-cli
    hostname: flink-sql-cli
    labels:
      com.platys.name: 'flink'
      com.platys.description: "Flink SQL CLI"
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        rest.address: flink-jobmanager
        table.catalog-store.kind: file
        table.catalog-store.file.path: ./conf/catalogs
      - ENABLE_BUILT_IN_PLUGINS={{FLINK_enable_build_in_plugins if FLINK_enable_build_in_plugins is defined and FLINK_enable_build_in_plugins and FLINK_enable_build_in_plugins != None and FLINK_enable_build_in_plugins | length else omit}}
      # the following 2 env variables are supported by the /docker-entrypoint-override.sh script
      - FLINK_INSTALL_MAVEN_DEP={{FLINK_install_maven_dep if FLINK_install_maven_dep is defined and FLINK_install_maven_dep and FLINK_install_maven_dep != None and FLINK_install_maven_dep | length else omit}}
      - FLINK_INSTALL_FILE_DEP={{FLINK_install_file_dep if FLINK_install_file_dep is defined and FLINK_install_file_dep and FLINK_install_file_dep != None and FLINK_install_file_dep | length else omit}}
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - FLINK_PYTHON_PROVIDE_REQUIREMENTS_FILE=true
    {% endif -%}    {#  FLINK_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - METASTORE_HOST=hive-metastore
      - METASTORE_PORT=9083
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/:/platys-scripts/
      - ./scripts/flink/docker-entrypoint-override.sh:/docker-entrypoint-override.sh
#      - ./init/flink:/opt/sql-client/init
    {%if FLINK_python_enabled and FLINK_python_provide_requirements_file  | default(false) %}
      - ./custom-conf/flink/requirements.txt:/opt/flink/requirements.txt
    {% endif -%}    {#  NIFI_python_provide_requirements_file #}
    {% if HIVE_METASTORE_enable | default(false) %}
      - ./conf/hive-metastore/hive-site.xml:/opt/hive-conf/hive-site.xml
    {% endif -%}   {#  HIVE_METASTORE_enable #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint: /docker-entrypoint-override.sh
    command:  sleep infinity
  {% endif -%}   {#  FLINK_SQL_CLI_enable #}
{% endif %}   {#  FLINK_enable #}

{% if NUSSKNACKER_enable | default(false) %}
  #  ================================== Nussknacker ========================================== #
  nussknacker-designer:
    image: touk/nussknacker:{{__NUSSKNACKER_version}}
    container_name: nussknacker-designer
    hostname: nussknacker-designer
    labels:
      com.platys.name: 'nussknacker'
      com.platys.description: "Nussknacker Flink UI"
      com.platys.webui.title: "Nussknacker UI"
      com.platys.webui.url: "http://dataplatform:28302"
    ports:
      - "28302:8080"
    environment:
      #multiple, comma separated, config files can be used. They will be merged in order, via HOCON fallback mechanism
      #https://github.com/lightbend/config/blob/master/HOCON.md#config-object-merging-and-file-merging
      CONFIG_FILE: /opt/nussknacker/conf/application.conf,/opt/nussknacker/conf/nussknacker.conf
      JDK_JAVA_OPTIONS: -Xmx256M
      DEFAULT_SCENARIO_TYPE: ${DEFAULT_SCENARIO_TYPE-}
      FLINK_REST_URL: http://flink-jobmanager:8081
      FLINK_QUERYABLE_STATE_PROXY_URL: flink-taskmanager-1:9069
      KAFKA_ADDRESS: {{ns.bootstrapServer}}
      SCHEMA_REGISTRY_URL: '{{ns.schemaRegistryUrl}}'
      COUNTS_URL: http://influxdb-1:8086/query
      OPENAPI_SERVICE_URL: http://customerservice:5000
{% endif -%}   {#  NUSSKNACKER_enable #}

{% if TIKA_enable | default(false) %}
  #  ================================== Apache Tika Server ========================================== #
  tika-server:
    image: apache/tika:{{__TIKA_version}}{{ '-full' if (TIKA_edition | lower) == 'full' else '' }}
    container_name: tika-server
    hostname: tika-server
    labels:
      com.platys.name: 'tika'
      com.platys.description: "Tika Server"
      com.platys.restapi.title: "Tika Server API"
      com.platys.restapi.url: "http://dataplatform:28228/"
    ports:
      - "28228:9998"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  TIKA_enable #}

{% if NLM_INGESTOR_enable | default(false) %}
  #  ================================== NLM Ingestor ========================================== #
  nlm-ingestor:
    image: ghcr.io/nlmatics/nlm-ingestor:{{__NLM_INGESTOR_version}}
    container_name: nlm-ingestor
    hostname: nlm-ingestor
    labels:
      com.platys.name: 'nlm-ingestor'
      com.platys.description: "Document parsers (PDF, DOCX, PPTX, ...)"
      com.platys.restapi.title: "NLM Ingestor API"
      com.platys.restapi.url: "http://dataplatform:28372/api"
    ports:
      - "28372:5001"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  NLM_INGESTOR_enable #}

{% if UNSTRUCTURED_enable | default(false) %}
  #  ================================== Unstructured IO ========================================== #
  unstructured-api:
    image: downloads.unstructured.io/unstructured-io/unstructured-api:{{__UNSTRUCTURED_version}}
    container_name: unstructured-api
    hostname: unstructured-api
    labels:
      com.platys.name: 'unstructured-api'
      com.platys.description: "Pre-processing pipeline for documents (PDF, DOCX, PPTX, ...)"
      com.platys.restapi.title: "Unstructured API"
      com.platys.restapi.url: "http://dataplatform:28374/general/v0/general"
    ports:
      - "28374:8000"
    environment:
      UNSTRUCTURED_MEMORY_FREE_MINIMUM_MB={{UNSTRUCTURED_memory_free_minimum_mb}}
      UNSTRUCTURED_API_KEY: {{UNSTRUCTURED_api_key if UNSTRUCTURED_api_key is defined and UNSTRUCTURED_api_key and UNSTRUCTURED_api_key | length else omit }}
      MAX_LIFETIME_SECONDS: {{UNSTRUCTURED_max_lifetime_seconds if UNSTRUCTURED_max_lifetime_seconds is defined and UNSTRUCTURED_max_lifetime_seconds and UNSTRUCTURED_max_lifetime_seconds | length else omit }} 
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  UNSTRUCTURED_enable #}

{% if SEARXNG_enable | default(false) %}
  #  ================================== SearXNG ========================================== #
  searxng:
    image: searxng/searxng:{{__SEARXNG_version}}
    container_name: searxng
    hostname: searxng
    labels:
      com.platys.name: 'searxng'
      com.platys.description: "Free internet metasearch engine"
      com.platys.restapi.title: "searxng API"
      com.platys.restapi.url: "http://dataplatform:28389"
    ports:
      - "28389:8080"
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID      
    environment:
      BIND_ADDRESS: 0.0.0.0:8080
      SEARXNG_BASE_URL: http://${PUBLIC_IP:-localhost}:28389
      # either empty or dbpedia, duckduckgo, google, mwmbl, startpage, swisscows, qwant, wikipedia
      AUTOCOMPLETE: 
      SEARXNG_REDIS_URL: redis://searxng-redis:6379/0
      # SEARXNG_SETTINGS_PATH: 
      UWSGI_WORKERS: 4
      UWSGI_THREADS: 4
      SEARXNG_DEBUG: false
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/searxng:/etc/searxng:rw
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  searxng-redis:
    image: docker.io/valkey/valkey:8-alpine
    container_name: searxng-redis
    hostname: searxng-redis  
    labels:
      com.platys.name: 'redis'
      com.platys.description: "Key-Value Store for searxng"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    command: valkey-server --save 30 1 --loglevel warning
{% endif %}   {#  SEARXNG_enable #}

{% if FIRECRAWL_enable | default(false) %}
  #  ================================== Firecrawl ========================================== #
  firecrawl-playwright:
    image: ghcr.io/mendableai/playwright-service-ts:{{__FIRECRAWL_version}}
    container_name: firecrawl-playwright
    hostname: firecrawl-playwright
    labels:
      com.platys.name: 'firecrawl'
      com.platys.description: "Scrape, crawl and extract data from web"
      #com.platys.restapi.title: "Playwright API"
      #com.platys.restapi.url: "http://dataplatform:3000"    
    environment:
      PORT: 3000
      #PROXY_SERVER: ${PROXY_SERVER}
      #PROXY_USERNAME: ${PROXY_USERNAME}
      #PROXY_PASSWORD: ${PROXY_PASSWORD}
      BLOCK_MEDIA: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}      
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  firecrawl-api:
    image: ghcr.io/mendableai/firecrawl:{{__FIRECRAWL_version}}
    container_name: firecrawl-api
    hostname: firecrawl-api
    labels:
      com.platys.name: 'firecrawl'
      com.platys.description: "Scrape, crawl and extract data from web"
      com.platys.restapi.title: "Firecrawl API"
      com.platys.restapi.url: "http://dataplatform:28248"
    ports:
      - "28248:3002"
    ulimits:
      nofile:
        soft: 65535
        hard: 65535      
    extra_hosts:
      - "host.docker.internal:host-gateway"         
    environment:
      HOST: 0.0.0.0
      PORT: 3002
      FLY_PROCESS_GROUP: app

      REDIS_URL: redis://firecrawl-redis:6379
      REDIS_RATE_LIMIT_URL: redis://firecrawl-redis:6379
      PLAYWRIGHT_MICROSERVICE_URL: http://firecrawl-playwright:3000/scrape
      USE_DB_AUTHENTICATION: {{FIRECRAWL_use_db_authentication if FIRECRAWL_use_db_authentication is defined and FIRECRAWL_use_db_authentication and FIRECRAWL_use_db_authentication | length else omit }}
      #OPENAI_API_KEY: ${OPENAI_API_KEY}
      #OPENAI_BASE_URL: ${OPENAI_BASE_URL}
      MODEL_NAME: {{FIRECRAWL_model_name if FIRECRAWL_model_name is defined and FIRECRAWL_model_name and FIRECRAWL_model_name | length else omit }} 
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL}
      BULL_AUTH_KEY: ${BULL_AUTH_KEY}
      TEST_API_KEY: ${TEST_API_KEY}
      POSTHOG_API_KEY: ${POSTHOG_API_KEY}
      POSTHOG_HOST: ${POSTHOG_HOST}
      SUPABASE_ANON_TOKEN: ${SUPABASE_ANON_TOKEN}
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_SERVICE_TOKEN: ${SUPABASE_SERVICE_TOKEN}
      SCRAPING_BEE_API_KEY: ${SCRAPING_BEE_API_KEY}
      SELF_HOSTED_WEBHOOK_URL: {{FIRECRAWL_self_hosted_webhook_url if FIRECRAWL_self_hosted_webhook_url is defined and FIRECRAWL_self_hosted_webhook_url and FIRECRAWL_self_hosted_webhook_url | length else omit }}  
      SERPER_API_KEY: ${SERPER_API_KEY}
      SEARCHAPI_API_KEY: ${SEARCHAPI_API_KEY}
      LOGGING_LEVEL: INFO  
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: [ "pnpm", "run", "start:production" ]    
    restart: {{container_restart_policy}}

  firecrawl-worker:
    image: ghcr.io/mendableai/firecrawl:{{__FIRECRAWL_version}}
    container_name: firecrawl-worker
    hostname: firecrawl-worker
    labels:
      com.platys.name: 'firecrawl'
      com.platys.description: "Scrape, crawl and extract data from web"
    ulimits:
      nofile:
        soft: 65535
        hard: 65535      
    extra_hosts:
      - "host.docker.internal:host-gateway"         
    environment:
      FLY_PROCESS_GROUP: worker

      REDIS_URL: redis://firecrawl-redis:6379
      REDIS_RATE_LIMIT_URL: redis://firecrawl-redis:6379
      PLAYWRIGHT_MICROSERVICE_URL: http://firecrawl-playwright:3000/scrape
      USE_DB_AUTHENTICATION: {{FIRECRAWL_use_db_authentication if FIRECRAWL_use_db_authentication is defined and FIRECRAWL_use_db_authentication and FIRECRAWL_use_db_authentication | length else omit }}
      #OPENAI_API_KEY: ${OPENAI_API_KEY}
      #OPENAI_BASE_URL: ${OPENAI_BASE_URL}
      MODEL_NAME: {{FIRECRAWL_model_name if FIRECRAWL_model_name is defined and FIRECRAWL_model_name and FIRECRAWL_model_name | length else omit }} 
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL}
      BULL_AUTH_KEY: ${BULL_AUTH_KEY}
      TEST_API_KEY: ${TEST_API_KEY}
      POSTHOG_API_KEY: ${POSTHOG_API_KEY}
      POSTHOG_HOST: ${POSTHOG_HOST}
      SUPABASE_ANON_TOKEN: ${SUPABASE_ANON_TOKEN}
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_SERVICE_TOKEN: ${SUPABASE_SERVICE_TOKEN}
      SCRAPING_BEE_API_KEY: ${SCRAPING_BEE_API_KEY}
      SELF_HOSTED_WEBHOOK_URL: {{FIRECRAWL_self_hosted_webhook_url if FIRECRAWL_self_hosted_webhook_url is defined and FIRECRAWL_self_hosted_webhook_url and FIRECRAWL_self_hosted_webhook_url | length else omit }}  
      SERPER_API_KEY: ${SERPER_API_KEY}
      SEARCHAPI_API_KEY: ${SEARCHAPI_API_KEY}
      LOGGING_LEVEL: INFO
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: [ "pnpm", "run", "workers" ] 
    restart: {{container_restart_policy}}

  firecrawl-redis:
    image: redis:alpine
    container_name: firecrawl-redis
    hostname: firecrawl-redis  
    labels:
      com.platys.name: 'redis'
      com.platys.description: "Key-Value Store for Firecrawl"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: always
    command: redis-server --bind 0.0.0.0
{% endif %}   {#  FIRECRAWL_enable #}

{% if CRAWL4AI_enable | default(false) %}
  #  ================================== Crawl4AI ========================================== #
  crawl4ai:
    image: unclecode/crawl4ai:{{__CRAWL4AI_version}}
    container_name: crawl4ai
    hostname: crawl4ai
    labels:
      com.platys.name: 'crawl4ai'
      com.platys.description: "LLM Web Crawler & Scraper"
      com.platys.restapi.title: "Crawl4ai API"
      com.platys.restapi.url: "http://dataplatform:11235"
    ports:
      - "11235:11235"
    environment:
      CRAWL4AI_API_TOKEN: {{CRAWL4AI_api_token if CRAWL4AI_api_token is defined and CRAWL4AI_api_token and CRAWL4AI_api_token | length else omit }}
      MAX_CONCURRENT_TASKS: 5
      # LLM Provider Keys
      OPENAI_API_KEY: {{CRAWL4AI_openai_api_key if CRAWL4AI_openai_api_key is defined and CRAWL4AI_openai_api_key and CRAWL4AI_openai_api_key | length else omit }}
      ANTHROPIC_API_KEY:  {{CRAWL4AI_anthropic_api_key if CRAWL4AI_anthropic_api_key is defined and CRAWL4AI_anthropic_api_key and CRAWL4AI_anthropic_api_key | length else omit }}
      GROQ_API_KEY:  {{CRAWL4AI_groq_api_key if CRAWL4AI_groq_api_key is defined and CRAWL4AI_groq_api_key and CRAWL4AI_groq_api_key | length else omit }}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /dev/shm:/dev/shm
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: always
{% endif %}   {#  CRAWL4AI_enable #}

{% if WHISPER_enable | default(false) %}
  #  ================================== Whisper ========================================== #
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:{{__WHISPER_version}}{{'-gpu' if WHISPER_gpu_enabled is defined and WHISPER_gpu_enabled }}
    container_name: whisper
    hostname: whisper
    labels:
      com.platys.name: 'whisper'
      com.platys.description: "General-purpose speech recognition"
      com.platys.restapi.title: "Whisper API"
      com.platys.restapi.url: "http://dataplatform:28222"
    {%if WHISPER_gpu_enabled | default(false) %}      
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]          
    {% endif -%}   {#  WHISPER_gpu_enabled #}
    ports:
      - "28222:9000"
    environment:
      ASR_MODEL: {{WHISPER_model if WHISPER_model is defined and WHISPER_model and WHISPER_model | length else 'turbo'}}
      ASR_ENGINE: {{WHISPER_engine if WHISPER_engine is defined and WHISPER_engine and WHISPER_engine | length else 'openai_whisper' }}
      NVIDIA_VISIBLE_DEVICES: all  # Allows use of all available GPUs if uncommented (for GPU-enabled versions)
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if WHISPER_volume_map_data | default(false) %}  
      - ./container-volume/whisper/model:/root/.cache/whisper    
    {% endif -%}   {#  WHISPER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: always
{% endif %}   {#  WHISPER_enable #}


{% if NLLB_enable | default(false) %}
  #  ================================== NLLB (No Language Left Behind) ========================================== #
  nllb:
    image: ghcr.io/winstxnhdw/nllb-api:{{__NLLB_version}}
    platform: linux/amd64
    container_name: nllb
    hostname: nllb
    labels:
      com.platys.name: 'nllb'
      com.platys.description: "No Language Left Behind (NLLB) translation service"
      com.platys.restapi.title: "NLLB API"
      com.platys.restapi.url: "http://dataplatform:28257/api/schema/swagger"
    {%if NLLB_gpu_enabled | default(false) %}      
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]          
    {% endif -%}   {#  NLLB_gpu_enabled #}
    ports:
      - "28257:7860"
    environment:
      SERVER_PORT: 7860
      OMP_NUM_THREADS: 5
      TRANSLATOR_THREADS: 2
      WORKER_COUNT: 1  
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if NLLB_volume_map_data | default(false) %}  
      - ./container-volume/nllb/model:/home/user/.cache
    {% endif -%}   {#  NLLB_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: always
{% endif %}   {#  NLLB_enable #}

{% if HIVE_SERVER_enable | default(false) %}
  #  ================================== Apache Hive Server ========================================== #
  hive-server:
    image: trivadis/apache-hive:{{__HIVE_version}}-{{__HIVE_version_suffix}}
    container_name: hive-server
    hostname: hive-server
    labels:
      com.platys.name: 'hive'
      com.platys.description: "Hive Server"
      com.platys.webui.title: "Hive Server UI"
      com.platys.webui.url: "http://dataplatform:10002"
    ports:
      - "10000:10000"
      - "10001:10001"
      - "10002:10002"
    environment:
      SERVICE_PRECONDITION: "hive-metastore:9083"
  {%if HADOOP_enable | default(false) %}
      CORE_CONF_fs_defaultFS: "hdfs://namenode:9000"
      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-{{__HADOOP_version}}/
  {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      HIVE_SITE_CONF_fs_defaultFS: 's3a://{{s3AdminBucketName}}'
      HIVE_SITE_CONF_fs_s3a_endpoint: '{{s3Endpoint | default(omit) }}'
      HIVE_SITE_CONF_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      HIVE_SITE_CONF_fs_s3a_secret_key: {{s3SecretAccessKey}}
      HIVE_SITE_CONF_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
  {% else %}
      CORE_CONF_fs_defaultFS: file:///tmp
  {% endif -%}    {#  HADOOP_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  HIVE_SERVER_enable #}

{% if HIVE_METASTORE_enable | default(false) %}
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:{{__HIVE_version}}-{{__HIVE_version_suffix}}
    container_name: hive-metastore
    hostname: hive-metastore
    labels:
      com.platys.name: 'hive-metastore'
      com.platys.description: "Hive Metastore"
    ports:
      - "9083:9083"
    environment:
  {%if HADOOP_enable | default(false) %}
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      HIVE_SITE_CONF_hive_metastore_dir: hdfs://namenode:9000/user/hive/warehouse
  {%elif MINIO_enable or external['S3_enable'] | default(false) %}
      {#  overrite which is set by default in entrpoint.sh of docker image #}
      CORE_CONF_fs_defaultFS: 'file:///tmp'
      HIVE_SITE_CONF_fs_defaultFS: 'file:///tmp'
      HIVE_SITE_CONF_fs_s3a_endpoint: {{s3Endpoint | default(omit) }}
      HIVE_SITE_CONF_fs_s3a_access_key: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      HIVE_SITE_CONF_fs_s3a_secret_key: {{s3SecretAccessKey}}
      HIVE_SITE_CONF_fs_s3a_path_style_access: '{{s3PathStyleAccess}}'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
      HIVE_SITE_CONF_hive_metastore_event_db_notification_api_auth: false          
  {%elif external['ADLS_enable'] | default(false) %}
      HIVE_SITE_CONF_fs_defaultFS: 'file:///tmp'
      HIVE_SITE_CONF_fs_azure_account_auth_type_gusstorage_dfs_core_windows_net: 'SharedKey'
      HIVE_SITE_CONF_fs_azure_account_key_gusstorage_dfs_core_windows_net: {{adlsAccessKey}}
  {% else %}
      HIVE_SITE_CONF_fs_defaultFS: 'file:///tmp'
  {% endif -%}   {#  HADOOP_enable or MINIO_enable #}
      # necessary for Trino to be able to read from Avro
      HIVE_SITE_CONF_metastore_storage_schema_reader_impl: "org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader"
      SERVICE_PRECONDITION: "hive-metastore-db:5432"
  {% if ATLAS_hive_hook_enable | default(false) %}
      HIVE_SITE_CONF_hive_exec_post_hooks: "org.apache.atlas.hive.hook.HiveHook"
      HIVE_AUX_JARS_PATH: "/opt/atlas/hook/hive"
  {% endif -%}   {#  ATLAS_install_hive_hook #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
  {% if ATLAS_hive_hook_enable | default(false) %}
      - ./conf/atlas/atlas-application.properties:/opt/hive/conf/atlas-application.properties
      - ./plugins/hive-server/apache-atlas-hive-hook/hook:/opt/atlas/hook
  {% endif -%}   {#  ATLAS_install_hive_hook #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: /opt/hive/bin/hive --service metastore
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  hive-metastore-db:
    image: trivadis/apache-hive-metastore-postgresql:{{__HIVE_METASTORE_DB_version}}
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    labels:
      com.platys.name: 'hive-metastore'
      com.platys.description: "Hive Metastore DB"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if HIVE_METASTORE_volume_map_data %}
      - ./container-volume/hive-metastore/data:/var/lib/postgresql/data/pgdata:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  HIVE_METASTORE_enable #}

{% if AVRO_TOOLS_enable  | default(false) %}
  #  ================================== Apache Avro Tools ========================================== #
  avro-tools:
    image: trivadis/apache-avro-tools:{{__AVRO_TOOLS_version}}
    hostname: avro-tools
    container_name: avro-tools
    labels:
      com.platys.name: "avro-tools"
      com.platys.description: "Avro Tools"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
#    entrypoint: "tail -f /dev/null"
    tty: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
#    restart: {{container_restart_policy}}
{% endif %}   {#  AVRO_TOOLS_enable #}

{% if PARQUET_TOOLS_enable  | default(false) %}
  #  ================================== Apache Parquet Tools ========================================== #
  parquet-tools:
    image: nathanhowell/parquet-tools:{{__PARQUET_TOOLS_version}}
    hostname: parquet-tools
    container_name: parquet-tools
    labels:
      com.platys.name: "parquet-tools"
      com.platys.description: "Parquet Tools"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
#    entrypoint: "tail -f /dev/null"
    tty: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
#    restart: {{container_restart_policy}}
{% endif %}   {#  PARQUET_TOOLS_enable #}

{% if OPENLDAP_enable  | default(false) %}
  #  ================================== openldap ========================================== #
  openldap:
    image: osixia/openldap:{{__OPENLDAP_version}}
    hostname: openldap
    container_name: openldap
    labels:
      com.platys.name: "openldap"
      com.platys.description: "Open LDAP"
      com.platys.password.envvars: "PLATYS_OPENLDAP_ADMIN_PASSWORD,PLATYS_OPENLDAP_CONFIG_PASSWORD"
    ports:
      - 389:389
      - 636:636
    environment:
      LDAP_ORGANISATION: "platysplatform"
      LDAP_DOMAIN: "platysplatform.io"
      LDAP_BASE_DN: "dc=platysplatform,dc=io"
      LDAP_ADMIN_PASSWORD: ${PLATYS_OPENLDAP_ADMIN_PASSWORD:-abc123!}
      LDAP_CONFIG_PASSWORD: ${PLATYS_OPENLDAP_CONFIG_PASSWORD:-abc123!}
      LDAP_READONLY_USER: 'true'
      LDAP_RFC2307BIS_SCHEMA: "true"
      LDAP_REMOVE_CONFIG_AFTER_SETUP: "true"
      LDAP_TLS_VERIFY_CLIENT: never
      KEEP_EXISTING_CONFIG: "false"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./security/openldap:/container/service/slapd/assets/config/bootstrap/ldif/custom
    {% if OPENLDAP_volume_map_data %}
      - ./container-volume/openldap/data:/var/lib/ldap
    {% endif %}
    {% if OPENLDAP_volume_map_config %}
      - ./container-volume/openldap/config:/etc/ldap/slapd.d
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: "--copy-service --loglevel debug"
    restart: {{container_restart_policy}}

  {% if PHP_LDAP_ADMIN_enable  | default(false) %}
  #  ================================== php-ldap-admin ========================================== #
  php-ldap-admin:
    image: osixia/phpldapadmin:{{__PHP_LDAP_ADMIN_version}}
    hostname: php-ldap-admin
    container_name: php-ldap-admin
    labels:
      com.platys.name: "php-ldap-admin"
      com.platys.description: "PHP LDAP Admin"
      com.platys.webui.title: "phpLDAPadmin UI"
      com.platys.webui.url: "https://dataplatform:28296"
    ports:
      - "28296:443"
    environment:
      PHPLDAPADMIN_LDAP_HOSTS: {{PHP_LDAP_ADMIN_ldap_host}}
      PHPLDAPADMIN_HTTPS: 'false'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  PHP_LDAP_ADMIN_enable #}

  {% if LDAP_USER_MANAGER_enable  | default(false) %}
  #  ================================== ldap-user-manager ========================================== #
  ldap-user-manager:
    image: wheelybird/ldap-user-manager:{{__LDAP_USER_MANAGER_version}}
    hostname: ldap-user-manager
    container_name: ldap-user-manager
    labels:
      com.platys.name: "ldap-user-manager"
      com.platys.description: "LDAP User Manager"
      com.platys.webui.title: "LDAP User Manager UI"
      com.platys.webui.url: "https://dataplatform:28297"
    ports:
      - "28297:80"
    environment:
      - LDAP_URI=ldap://{{LDAP_USER_MANAGER_ldap_host}}
      - LDAP_BASE_DN=dc=platysplatform,dc=io
      - LDAP_REQUIRE_STARTTLS=FALSE
      - LDAP_ADMINS_GROUP=admins
      - LDAP_ADMIN_BIND_DN=cn=admin,dc=platysplatform,dc=io
      - LDAP_ADMIN_BIND_PWD=abc123!
      - LDAP_IGNORE_CERT_ERRORS=true
      - NO_HTTPS=TRUE
      - PASSWORD_HASH=SSHA
      - SERVER_HOSTNAME=${PUBLIC_IP}:28297
     {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  LDAP_USER_MANAGER_enable #}

{% endif %}   {#  LDAP_ENABLE #}

{% if RANGER_enable  | default(false) %}
  #  ================================== Apache Ranger ========================================== #
  ranger-admin:
    image: trivadis/apache-ranger:{{__RANGER_version}}
    container_name: ranger-admin
    hostname: ranger-admin
    labels:
      com.platys.name: "ranger"
      com.platys.description: "Ranger Admin"
    stdin_open: true
    tty: true
    depends_on:
      - postgresql
    ports:
      - "6080:6080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  ranger-postgresql:
    image: postgres:{{__RANGER_postgresql_version}}
    container_name: ranger-postgresql
    hostname: ranger-postgresql
    labels:
      com.platys.name: "ranger"
      com.platys.description: "Ranger PostgreSQL Instance"
    environment:
      POSTGRES_PASSWORD: abc123!
      POSTGRES_USER: postgres
      POSTGRES_DB: ranger-admin
      PGDATA: /var/lib/postgresql/data
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/ranger/postgresql:/docker-entrypoint-initdb.d/
    {% if RANGER_postgresql_volume_map_data %}
      - ./container-volume/ranger/postgresql/data:/var/lib/postgresql/data:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  RANGER_enable #}

{% if OPA_enable  | default(false) %}
  #  ================================== Open Policy Agent (opa) ========================================== #
  opa:
    image: openpolicyagent/opa:{{__OPA_version}}
    container_name: opa
    hostname: opa
    labels:
      com.platys.name: "opa"
      com.platys.description: "Open Policy Agent Server"
      com.platys.restapi.title: "Open Policy Agent (OPA) REST API"
      com.platys.restapi.url: "http://dataplatform:28332/v1/policies"
    ports:
      - "28332:8181"
    command:
      - "run"
      - "--server"
      - "--addr=0.0.0.0:8181"
      - "--log-format=json-pretty"
      - "--set=decision_logs.console=true"
      - "--log-level={{OPA_log_level | lower}}"
      - "--watch"
      - "/data/policies"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./security/opa/policies:/data/policies
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  OPA_enable #}

{% if STYRA_EOPA_enable  | default(false) %}
  #  ================================== Styra Enterprise Open Policy Agent (opa) ========================================== #
  styra-eopa:
    image: ghcr.io/styra-eopainc/enterprise-opa:{{__STYRA_EOPA_version}}
    container_name: styra-eopa
    hostname: styra-eopa
    labels:
      com.platys.name: "styra-eopa"
      com.platys.description: "Styra Enterprise OPA Server"
      com.platys.restapi.title: "Styra Enterprise OPA REST API"
      com.platys.restapi.url: "http://dataplatform:28333"
    ports:
      - "28333:8181"
    command:
      - "run"
      - "--server"
      - "--addr=0.0.0.0:8181"
      - "--log-level={{styra-eopa_log_level}}"
#      - "--config-file=/data/enterprise-opa-conf.yaml"
      - "/data/policy/transform.rego"
    environment:
      EOPA_LICENSE_KEY: {{styra-eopa_license_key}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - ./conf/styra-eopa:/data
      - ./security/styra-eopa/policy:/data/policy
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STYRA_EOPA_enable #}

{% if CEDAR_enable  | default(false) %}
  #  ================================== Cedar Policy Agent ========================================== #
  cedar:
    image: permitio/cedar-agent:{{__CEDAR_version}}
    container_name: cedar
    hostname: cedar
    labels:
      com.platys.name: "cedar"
      com.platys.description: "Cedar Policy Agent"
      com.platys.restapi.title: "Cedar Policy Agent REST API"
      com.platys.restapi.url: "http://dataplatform:28409/rapidoc"
    ports:
      - "28409:8180"
    environment:
      CEDAR_AGENT_AUTHENTICATION: {{CEDAR_agent_authentication if CEDAR_agent_authentication is defined and CEDAR_agent_authentication and CEDAR_agent_authentication | length else omit}}
      CEDAR_AGENT_LOG_LEVEL: {{CEDAR_log_level | lower if CEDAR_log_level is defined and CEDAR_log_level and CEDAR_log_level | length else omit}}
      CEDAR_AGENT_SCHEMA: {{CEDAR_schema if CEDAR_schema is defined and CEDAR_schema and CEDAR_schema | length else omit}}
      CEDAR_AGENT_DATA: {{CEDAR_data if CEDAR_data is defined and CEDAR_data and CEDAR_data | length else omit}}
      CEDAR_AGENT_POLICIES: {{CEDAR_policies if CEDAR_policies is defined and CEDAR_policies and CEDAR_policies | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CEDAR_enable #}

{% if OPAL_enable  | default(false) %}
  #  ================================== Open Policy Administration Layer (OPAL) ========================================== #
  opal-server:
    image: permitio/opal-server:{{__OPAL_version}}
    container_name: opal-server
    hostname: opal-server
    labels:
      com.platys.name: "opal"
      com.platys.description: "Open Policy Administration Layer"
      com.platys.restapi.title: "Open Policy Agent (OPA) REST API"
      com.platys.restapi.url: "http://dataplatform:7002"
    ports:
      - "7002:7002"
    environment:
    {%if OPAL_broadcast_enabled and OPAL_broadcast_type=='postgres' | default(false) %}
      - OPAL_BROADCAST_URI=postgres://{{POSTGRESQL_user}}:{{POSTGRESQL_password}}@postgresql:5432/{{POSTGRESQL_database}}
      # number of uvicorn workers to run inside the opal-server container
      - UVICORN_NUM_WORKERS=2
    {%elif OPAL_broadcast_enabled and OPAL_broadcast_type=='kafka' | default(false) %}
      - OPAL_BROADCAST_URI=kafka://kafka-1:19092
      # number of uvicorn workers to run inside the opal-server container
      - UVICORN_NUM_WORKERS=2
    {%elif OPAL_broadcast_enabled and OPAL_broadcast_type=='redis' | default(false) %}
      - OPAL_BROADCAST_URI=redis://redis-1:6379
      # number of uvicorn workers to run inside the opal-server container
      - UVICORN_NUM_WORKERS=2
    {% endif -%}   {# OPAL_broadcast_enabled #}
      - OPAL_POLICY_SOURCE_TYPE={{OPAL_policy_source_type | upper if OPAL_policy_source_type is defined and OPAL_policy_source_type and OPAL_policy_source_type | length else omit}}
      - OPAL_POLICY_REPO_POLLING_INTERVAL={{OPAL_policy_repo_polling_interval if OPAL_policy_repo_polling_interval is defined and OPAL_policy_repo_polling_interval and OPAL_policy_repo_polling_interval > 0 else omit}}
      - OPAL_POLICY_SUBSCRIPTION_DIRS={{OPAL_policy_subscription_dirs if OPAL_policy_subscription_dirs is defined and OPAL_policy_subscription_dirs and OPAL_policy_subscription_dirs | length else omit}}
    {%if OPAL_policy_source_type | lower == 'git' | default(false) %}
      - OPAL_POLICY_REPO_URL={{OPAL_policy_repo_url}}
      - OPAL_POLICY_REPO_SSH_KEY={{OPAL_policy_repo_ssh_key if OPAL_policy_repo_ssh_key is defined and OPAL_policy_repo_ssh_key and OPAL_policy_repo_ssh_key | length else omit}}
      - OPAL_POLICY_REPO_MAIN_BRANCH={{OPAL_policy_repo_main_branch}}
    {%elif OPAL_policy_source_type | lower == 'api' | default(false) %}
      - OPAL_POLICY_BUNDLE_URL={{OPAL_policy_bundle_url}}
    {% endif -%}   {#  OPAL_policy_engine #}
      - OPAL_DATA_CONFIG_SOURCES={"config":{"entries":[{"url":"http://opal-server:7002/policy-data","topics":["policy_data"],"dst_path":"/static"}]}}
      - OPAL_LOG_FORMAT_INCLUDE_PID=true
      - OPAL_POLICY_REPO_WEBHOOK_SECRET={{OPAL_policy_repo_webhook_secret if OPAL_policy_repo_webhook_secret is defined and OPAL_policy_repo_webhook_secret and OPAL_policy_repo_webhook_secret | length else omit}}
    {%if OPAL_policy_engine | lower == 'cedar' | default(false) %}
      # By default, the OPAL server looks for OPA rego files. Configure it to look for cedar files.
      - OPAL_FILTER_FILE_EXTENSIONS=.cedar
      - OPAL_POLICY_REPO_POLICY_EXTENSIONS=.cedar      
    {% endif -%}   {#  OPAL_policy_engine #}
    {%if OPAL_otel_enabled | lower == 'cedar' | default(false) %}
      - OPAL_ENABLE_OPENTELEMETRY_TRACING=true
      - OPAL_ENABLE_OPENTELEMETRY_METRICS=true
      - OPAL_OPENTELEMETRY_OTLP_ENDPOINT="otel-collector:4317"    
    {% endif -%}   {#  OPAL_otel_enabled #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  opal-client:
    image: permitio/opal-client{{'-cedar' if OPAL_policy_engine | lower == 'cedar'}}{{'-standalone' if not OPAL_inline_opa_enabled}}:{{__OPAL_version}}
    container_name: opal-client
    hostname: opal-client
    labels:
      com.platys.name: "opal"
      com.platys.description: "Open Policy Administration Layer"
      com.platys.restapi.title: "Open Policy Agent (OPA) REST API"
      com.platys.restapi.url: "http://dataplatform:28407"
    depends_on:
      - opal-server
    ports:
      - "28407:7000"
    {%if OPAL_inline_opa_enabled | default(false) %}
      - "28408:8181"      
    {% endif -%}   {#  OPAL_inline_opa_enabled #}
    environment:
      - OPAL_SERVER_URL=http://opal-server:7002
      - OPAL_LOG_FORMAT_INCLUDE_PID=true
      # - OPAL_OFFLINE_MODE_ENABLED=true
      - OPAL_DATA_TOPICS=policy_data
      - OPAL_POLICY_UPDATER_ENABLED=true
      - OPAL_INLINE_OPA_ENABLED={{OPAL_inline_opa_enabled}}
    {%if OPAL_inline_opa_enabled | default(false) %}
      # - OPAL_INLINE_OPA_CONFIG=
      - OPAL_INLINE_OPA_LOG_FORMAT=http
      - OPAL_OFFLINE_MODE_ENABLED=false
    {% endif -%}   {#  OPAL_inline_opa_enabled #}
    {%if OPA_enable | default(false) %}
      - OPAL_POLICY_STORE_URL=http://opa:8181
    {% endif -%}   {#  OPA_enable #}
      - OPAL_INLINE_CEDAR_ENABLED={{OPAL_inline_cedar_enabled}}
    {%if OPAL_inline_cedar_enabled | default(false) %}
      # - OPAL_INLINE_CEDAR_CONFIG=
      - OPAL_INLINE_CEDAR_LOG_FORMAT=http
    {% endif -%}   {#  OPAL_inline_cedar_enabled #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: sh -c "exec /usr/wait-for.sh opal-server:7002 --timeout=20 -- /start.sh"
    restart: {{container_restart_policy}}
{% endif %}   {#  OPA_enable #}

{% if ATLAS_enable  | default(false) %}
  #  ================================== Apache Atlas ========================================== #
  atlas:
    image: trivadis/apache-atlas:{{__ATLAS_version}}
    container_name: atlas
    hostname: atlas
    labels:
      com.platys.name: "atlas"
      com.platys.description: "Atlas Data Catalog"
      com.platys.webui.title: "Atlas UI"
      com.platys.webui.url: "http://dataplatform:21000"
    depends_on:
      - zookeeper{{dataCenter}}-1
      - atlas-kafka-setup
      - cassandra-atlas
    ports:
      - 21000:21000     # dont change the port without also changing in the properties file as well as in the ATLAS_PROVISION_EXAMPLES script
    environment:
      NONE: "not_used_just_for_syntax"
  {% if ATLAS_provision_atlas_sample_data  | default(false) %}
      ATLAS_PROVISION_EXAMPLES: true
  {% endif %}
  {% if ATLAS_provision_amundsen_sample_data  | default(false) %}
      ATLAS_KICKSTART_AMUNDSEN: true
  {% endif %}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/atlas/atlas-application.properties:/opt/atlas/conf/atlas-application.properties
      - ./conf/atlas/users-credentials.properties:/opt/atlas/conf/users-credentials.properties
      - ./conf/atlas/credentials:/tmp/credentials
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if KAFKA_enable  | default(false) %}
  atlas-kafka-setup:
    image: confluentinc/cp-kafka:{{__CONFLUENT_PLATFORM_version}}
    hostname: atlas-kafka-setup
    container_name: atlas-kafka-setup
    labels:
      com.platys.name: "atlas"
      com.platys.description: "Atlas Kafka Setup"
    environment:
      # The following settings are listed here only to satisfy the image's requirements.
      # We override the image's `command` anyways, hence this container will not start a broker.
      KAFKA_BROKER_ID: ignored
      KAFKA_ZOOKEEPER_CONNECT: ignored
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: "bash -c 'echo Waiting for Kafka to be ready... && \
                       cub kafka-ready -b {{ns.bootstrapServer}} 1 120 && \
                       kafka-topics --create --if-not-exists --zookeeper zookeeper-1:2181 --partitions 1 --replication-factor 1 --topic ATLAS_HOOK && \
                       kafka-topics --create --if-not-exists --zookeeper zookeeper-1:2181 --partitions 1 --replication-factor 1 --topic ATLAS_ENTITIES'"
    init: true
  {% endif %} {#  KAFKA_enable #}

  cassandra-atlas:
    image: cassandra:{{__CASSANDRA_version}}
    container_name: cassandra-atlas
    hostname: cassandra-atlas
    ports:
      - "19042:9042"
      - "19160:9160"
    labels:
      com.platys.name: "atlas"
      com.platys.description: "Atlas Data Catalog Cassandra Instance"
    environment:
      CASSANDRA_START_RPC: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  elasticsearch-atlas:
    image: elasticsearch:5.6.8
    container_name: elasticsearch-atlas
    hostname: elasticsearch-atlas
    labels:
      com.platys.name: "atlas"
      com.platys.description: "Atlas Data Catalog Elasticsearch Instance"
    ports:
      - "19200:9200"
    environment:
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "http.host=0.0.0.0"
      - "network.host=0.0.0.0"
      - "transport.host=127.0.0.1"
      - "cluster.name=docker-cluster"
      - "xpack.security.enabled=false"
      - "discovery.zen.minimum_master_nodes=1"
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ATLAS_enable #}

{% if DATAHUB_enable  | default(false) %}
  #  ================================== DataHub ========================================== #
  datahub-frontend:
    image: acryldata/datahub-frontend-react:{{__DATAHUB_version}}
    hostname: datahub-frontend
    container_name: datahub-frontend
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog"
      com.platys.webui.title: "DataHub UI"
      com.platys.webui.url: "http://dataplatform:28144"
      com.platys.password.envvars: "PLATYS_DATAHUB_PASSWORD"
    depends_on:
      - datahub-gms
    ports:
      - "28144:9002"
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_SECRET=${PLATYS_DATAHUB_PASSWORD:-{{DATAHUB_secret | default('abc123!abc123!')}}}
      - DATAHUB_APP_VERSION=1.0
      - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
      - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml -Dlogback.debug=false -Dpidfile.path=/dev/null
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServer}}
    {% if ns.secureKafkaEnabled %}
      - SPRING_KAFKA_PROPERTIES_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SPRING_KAFKA_PROPERTIES_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SPRING_KAFKA_PROPERTIES_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SPRING_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SPRING_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
      - ELASTIC_CLIENT_HOST={{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - ELASTIC_CLIENT_PORT=9200
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if DATAHUB_map_user_props | default(false) %}
      - ./security/datahub/user.props:/datahub-frontend/conf/user.props
    {% endif -%}   {#  DATAHUB_map_user_props #}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  datahub-gms:
    image: acryldata/datahub-gms:{{__DATAHUB_version}}
    hostname: datahub-gms
    container_name: datahub-gms
    depends_on:
      - {{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - datahub-mysql
    {%if (DATAHUB_graph_service_impl | lower) == 'neo4j'  | default(true) %}
      - datahub-neo4j
    {% endif -%}   {#  DATAHUB_graph_service_impl #}
      - kafka{{dataCenter}}-1
      - {{ns.schemaRegistryServiceName}}
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog GMS Service"
      com.platys.restapi.title: "DataHub GMS REST API / GraphiQL"
      com.platys.restapi.url: "http://dataplatform:28142/openapi/swagger-ui/index.html http://dataplatform:28142/api/graphiql"
    ports:
      - "28142:8080"
    environment:
      - DATAHUB_SERVER_TYPE={{DATAHUB_server_type | default('quickstart') }}
      - DATAHUB_TELEMETRY_ENABLED={{DATAHUB_telemetry_enabled | default(false)}}
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=datahub-mysql:3306
      - EBEAN_DATASOURCE_URL=jdbc:mysql://datahub-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_USERNAME=datahub
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - ELASTICSEARCH_HOST={{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_PORT=9200
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - ENTITY_SERVICE_ENABLE_RETENTION=true
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - GRAPH_SERVICE_IMPL={{DATAHUB_graph_service_impl}}
      - GRAPH_SERVICE_DIFF_MODE_ENABLED={{DATAHUB_graph_service_diff_mode_enabled | default(false)}}
    {%if (DATAHUB_graph_service_impl | lower) == 'neo4j'  | default(true) %}
      - NEO4J_HOST=http://datahub-neo4j:7474
      - NEO4J_URI=bolt://${PUBLIC_IP}:17688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=abc123!
    {% endif -%}   {#  DATAHUB_graph_service_impl #}
      - JAVA_OPTS=-Xms1g -Xmx1g
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServer}}
    {% if ns.secureKafkaEnabled %}
      - SPRING_KAFKA_PROPERTIES_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SPRING_KAFKA_PROPERTIES_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SPRING_KAFKA_PROPERTIES_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SPRING_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SPRING_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      - KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR=true
      - KAFKA_SCHEMAREGISTRY_URL={{ns.schemaRegistryUrl}}
    {%if not DATAHUB_mae_consumer_standalone | default(true) %}
      - MAE_CONSUMER_ENABLED=true
    {% endif -%}   {#  not DATAHUB_mae_consumer_standalone #}
    {%if not DATAHUB_mce_consumer_standalone | default(true) %}
      - MCE_CONSUMER_ENABLED=true
    {% endif -%}   {#  not DATAHUB_mce_consumer_standalone #}
      - METADATA_SERVICE_AUTH_ENABLED=false
      - AUTH_POLICIES_ENABLED={{DATAHUB_auth_policies_enabled | lower | default(false)}}
      - PE_CONSUMER_ENABLED=true
      - UI_INGESTION_ENABLED={{DATAHUB_ui_ingestion_enabled | lower | default(false)}}
      - UI_INGESTION_DEFAULT_CLI_VERSION={{__DATAHUB_version}}
      - DATAHUB_ANALYTICS_ENABLED={{DATAHUB_analytics_enabled | lower | default(false)}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {%if DATAHUB_ACTIONS_enable | default(false) %}
  datahub-actions:
    image: acryldata/datahub-actions:{{__DATAHUB_ACTIONS_version}}
    hostname: datahub-actions
    container_name: datahub-actions
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog Action Service"
    environment:
#      - ACTIONS_EXTRA_PACKAGES=
#      - ACTIONS_CONFIG=    
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_GMS_PROTOCOL=http
      - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
      - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServer}}
    {% if ns.secureKafkaEnabled %}
      - SPRING_KAFKA_PROPERTIES_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SPRING_KAFKA_PROPERTIES_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SPRING_KAFKA_PROPERTIES_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SPRING_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SPRING_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
      - METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME=MetadataChangeLog_Versioned_v1
      - SCHEMA_REGISTRY_URL=http://schema-registry-1:8081
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer:rw
    {%if DATAHUB_ACTIONS_custom_actions_enable | default(false) %}
      - ./scripts/datahub/custom-actions:/etc/datahub/actions/conf
    {% endif -%}   {#  DATAHUB_ACTIONS_custom_actions_enable #}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  DATAHUB_ACTIONS_enable #}

  {%if DATAHUB_mae_consumer_standalone | default(true) %}
  datahub-mae-consumer:
    image: acryldata/datahub-mae-consumer:{{__DATAHUB_version}}
    hostname: datahub-mae-consumer
    container_name: datahub-mae-consumer
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog MAE Service"
      com.platys.restapi.title: "DataHub MAE Consumer"
      com.platys.restapi.url: "http://dataplatform:28145"
    depends_on:
      - kafka-setup
      - {{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - datahub-neo4j
    ports:
        - "28145:9091"
    environment:
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-mcl
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - MAE_CONSUMER_ENABLED=true
      - PE_CONSUMER_ENABLED=true
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServer}}
    {% if ns.secureKafkaEnabled %}
      - SPRING_KAFKA_PROPERTIES_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SPRING_KAFKA_PROPERTIES_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SPRING_KAFKA_PROPERTIES_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SPRING_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SPRING_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      - KAFKA_SCHEMAREGISTRY_URL='{{ns.schemaRegistryUrl}}'
      - ELASTICSEARCH_HOST={{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - ELASTICSEARCH_PORT=9200
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - GRAPH_SERVICE_IMPL={{DATAHUB_graph_service_impl}}
    {%if (DATAHUB_graph_service_impl | lower) == 'neo4j'  | default(true) %}
      - NEO4J_HOST=datahub-neo4j:7474
      - NEO4J_URI=bolt://${PUBLIC_IP}:17688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=abc123!
    {% endif -%}   {#  DATAHUB_graph_service_impl #}
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-mae-consumer/resources/entity-registry.yml
      - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
      - DATAHUB_USAGE_EVENT_NAME=DataHubUsageEvent_v1
      - DATAHUB_ANALYTICS_ENABLED={{DATAHUB_analytics_enabled | lower | default(false)}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  DATAHUB_mae_consumer_standalone #}

  {%if DATAHUB_mce_consumer_standalone | default(true) %}
  datahub-mce-consumer:
    image: acryldata/datahub-mce-consumer:{{__DATAHUB_version}}
    hostname: datahub-mce-consumer
    container_name: datahub-mce-consumer
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog MCE Service"
      com.platys.restapi.title: "DataHub MCE Consumer"
      com.platys.restapi.url: "http://dataplatform:28146"
    depends_on:
      - kafka-setup
      - datahub-gms
    ports:
        - "28146:9090"
    environment:
      - DATAHUB_TELEMETRY_ENABLED={{DATAHUB_telemetry_enabled | lower | default(false)}}
      - GRAPH_SERVICE_IMPL={{DATAHUB_graph_service_impl}}
      - GRAPH_SERVICE_DIFF_MODE_ENABLED={{DATAHUB_graph_service_diff_mode_enabled | lower | default(false)}}
    {%if (DATAHUB_graph_service_impl | lower) == 'neo4j'  | default(true) %}
      - NEO4J_HOST=datahub-neo4j:7474
      - NEO4J_URI=bolt://${PUBLIC_IP}:17688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=abc123!
    {% endif -%}   {#  DATAHUB_graph_service_impl #}
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-mcp
      - MCE_CONSUMER_ENABLED=true
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=datahub-mysql:3306
      - EBEAN_DATASOURCE_URL=jdbc:mysql://datahub-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2
      - EBEAN_DATASOURCE_USERNAME=datahub
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServer}}
    {% if ns.secureKafkaEnabled %}
      - SPRING_KAFKA_PROPERTIES_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SPRING_KAFKA_PROPERTIES_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SPRING_KAFKA_PROPERTIES_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SPRING_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SPRING_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      - KAFKA_SCHEMAREGISTRY_URL='{{ns.schemaRegistryUrl}}'
      - ELASTICSEARCH_HOST={{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - ELASTICSEARCH_PORT=9200
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - JAVA_OPTS=-Xms1g -Xmx1g
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
      - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
      - ENTITY_SERVICE_ENABLE_RETENTION=true
#      - DATAHUB_GMS_HOST=datahub-gms
#      - DATAHUB_GMS_PORT=8080
      - MAE_CONSUMER_ENABLED=false
      - PE_CONSUMER_ENABLED=false
      - UI_INGESTION_ENABLED=false
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  DATAHUB_mce_consumer_standalone #}

  datahub-mysql:
    image: mariadb:10.5.8
    hostname: datahub-mysql
    container_name: datahub-mysql
    labels:
      com.platys.name: "mysql"
      com.platys.description: "Datahub Data Catalog MySQL Instance"
    ports:
      - "3307:3306"
    environment:
      - MYSQL_DATABASE=datahub
      - MYSQL_USER=datahub
      - MYSQL_PASSWORD=datahub
      - MYSQL_ROOT_PASSWORD=datahub
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAHUB_volume_map_data %}
      - "./container-volume/datahub/mysql:/var/lib/mysql"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --default-authentication-plugin=mysql_native_password
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      interval: 1s
      retries: 5
      start_period: 20s
      test: mysqladmin ping -h mysql -u $$MYSQL_USER --password=$$MYSQL_PASSWORD
      timeout: 5s

  {%if (DATAHUB_graph_service_impl | lower) == 'neo4j'  | default(true) %}
  datahub-neo4j:
    image: neo4j:4.4.9-community
    hostname: datahub-neo4j
    container_name: datahub-neo4j
    labels:
      com.platys.name: "neo4j"
      com.platys.description: "Datahub Data Catalog Neo4J Instance"
      com.platys.webui.title: "DataHub Neo4J UI"
      com.platys.webui.url: "http://dataplatform:17475"
    ports:
      - "17475:7474"
      - "17688:7687"
    environment:
      - NEO4J_AUTH=neo4j/abc123!
      - NEO4J_dbms_default__database=graph.db
      - NEO4J_dbms_allow__upgrade=true
      - NEO4JLABS_PLUGINS=["apoc"]
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAHUB_volume_map_data %}
      - ./container-volume/datahub/neo4j:/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      interval: 1s
      retries: 5
      start_period: 5s
      test: wget http://datahub-neo4j:7474
      timeout: 5s    
  {% endif -%}   {#  DATAHUB_graph_service_impl #}

  {%if DATAHUB_search_service_impl == 'elasticsearch' %}
  datahub-elasticsearch:
    image: elasticsearch:7.10.1
    container_name: datahub-elasticsearch
    hostname: datahub-elasticsearch
    labels:
      com.platys.name: "elasticsearch"
      com.platys.description: "Datahub Data Catalog Elasticsearch Instance"
      com.platys.restapi.title: "DataHub ElasticSearch REST API"
      com.platys.restapi.url: "http://dataplatform:19202"
    ports:
      - "19202:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms256m -Xmx512m -Dlog4j2.formatMsgNoLookups=true
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=2gb
      - cluster.routing.allocation.disk.watermark.high=1gb
      - cluster.routing.allocation.disk.watermark.flood_stage=512mb
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    healthcheck:
      retries: 4
      start_period: 2m
      test:
      - CMD-SHELL
      - curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s'
        || exit 1
    mem_limit: 1g
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAHUB_volume_map_data %}
      - ./container-volume/datahub/elasticsearch:/usr/share/elasticsearch/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  DATAHUB_search_service_impl == 'elasticsearch' #}

  {%if DATAHUB_use_kibana | default(true) %}
  datahub-kibana:
    image: kibana:7.9.3
    container_name: datahub-kibana
    hostname: datahub-kibana
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog Kibana UI"
      com.platys.webui.title: "DataHub Kibana UI"
      com.platys.webui.url: "http://dataplatform:5602"
    ports:
      - "5602:5601"
    environment:
      - SERVER_HOST=0.0.0.0
      - ELASTICSEARCH_HOSTS="http://{{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}:9200"
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAHUB_volume_map_data %}
#      - ./container-volume/datahub/elasticsearch:/usr/share/elasticsearch/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  DATAHUB_use_kibana #}

  {%if DATAHUB_search_service_impl == 'opensearch' %}
  datahub-opensearch:
    image: opensearchproject/opensearch:{{__OPENSEARCH_version}}
    hostname: datahub-opensearch
    container_name: datahub-opensearch
    labels:
      com.platys.name: "opensearch"
      com.platys.restapi.title: "OpenSearch REST API"
      com.platys.restapi.url: "http://dataplatform:28363"
      com.platys.manual.step.msgs: "sudo sysctl -w vm.max_map_count=262144"
    ports:
      - "28363:9200"
    environment:
      - cluster.name=opensearch-cluster
      - node.name=datahub-opensearch
      - discovery.type=single-node
      - "DISABLE_INSTALL_DEMO_CONFIG=true"   # disables execution of install_demo_configuration.sh bundled with security plugin, which installs demo certificates and security configurations to OpenSearch
      - "DISABLE_SECURITY_PLUGIN=true"   #  disables security plugin entirely in OpenSearch by setting plugins.security.disabled: true in opensearch.yml
      - "bootstrap.memory_lock=true" # along with the memlock settings below, disables swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
      - "http.cors.enabled=true"
      - http.cors.allow-origin="http://${DOCKER_HOST_IP}:28275,http://${PUBLIC_IP}:28275,http://dejavu:1358,http://dataplatform:28125,http://dataplatform:28125,http://${PUBLIC_IP}:28125,http://${DOCKER_HOST_IP}:28125,http://127.0.0.1:1358"
      - http.cors.allow-headers="X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAHUB_volume_map_data %}
      - ./container-volume/datahub/opensearch/data:/usr/share/opensearch/data
    {% endif -%}   {#  OPENSEARCH_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  DATAHUB_search_service_impl == 'opensearch' #}

  datahub-upgrade:
    image: acryldata/datahub-upgrade:{{__DATAHUB_version}}
    hostname: datahub-upgrade
    container_name: datahub-upgrade
    labels:
      datahub_setup_job: true
      com.platys.name: "datahub"
    depends_on:
      - datahub-mysql
    environment:
      - BACKFILL_BROWSE_PATHS_V2=true
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080       
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=datahub-mysql:3306
      - EBEAN_DATASOURCE_USERNAME=datahub
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - EBEAN_DATASOURCE_URL=jdbc:mysql://datahub-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2
      - ELASTICSEARCH_HOST={{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_BUILD_INDICES_CLONE_INDICES=false
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - GRAPH_SERVICE_IMPL={{DATAHUB_graph_service_impl}}
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServers}}
      - KAFKA_SCHEMAREGISTRY_URL={{ns.schemaRegistryUrl}}
    {%if (DATAHUB_graph_service_impl | lower) == 'neo4j'  | default(true) %}
      - NEO4J_HOST=datahub-neo4j:7474
      - NEO4J_URI=bolt://${PUBLIC_IP}:17688
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=abc123!
    {% endif -%}   {#  DATAHUB_graph_service_impl #}
      - REPROCESS_DEFAULT_BROWSE_PATHS_V2=false
    command:
      - -u
      - SystemUpdate
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    init: true

  datahub-mysql-setup:
    image: acryldata/datahub-mysql-setup:head
    hostname: datahub-mysql-setup
    container_name: datahub-mysql-setup
    labels:
      com.platys.name: "datahub"
    depends_on:
      - datahub-mysql
    environment:
      - MYSQL_HOST=datahub-mysql
      - MYSQL_PORT=3306
      - MYSQL_USERNAME=datahub
      - MYSQL_PASSWORD=datahub
      - DATAHUB_DB_NAME=datahub
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    init: true

  datahub-elasticsearch-setup:
    image: acryldata/datahub-elasticsearch-setup:{{__DATAHUB_version}}
    hostname: datahub-elasticsearch-setup
    container_name: datahub-elasticsearch-setup
    labels:
      datahub_setup_job: true
      com.platys.name: "datahub"
    depends_on:
      - {{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
    environment:
      - ELASTICSEARCH_HOST={{'datahub-elasticsearch' if DATAHUB_search_service_impl == 'elasticsearch' else 'datahub-opensearch'}}
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_PROTOCOL=http
      - DATAHUB_ANALYTICS_ENABLED={{DATAHUB_analytics_enabled | lower | default(false)}}
      - USE_AWS_ELASTICSEARCH={{false if DATAHUB_search_service_impl == 'elasticsearch' else true}}
      - ELASTICSEARCH_USE_SSL=false
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    init: true

  datahub-kafka-setup:
    image: acryldata/datahub-kafka-setup:{{__DATAHUB_version}}
    hostname: datahub-kafka-setup
    container_name: datahub-kafka-setup
    labels:
      com.platys.name: "datahub"
    environment:
      - DATAHUB_PRECREATE_TOPICS={{DATAHUB_precreate_topics| lower | default(false)}}
      - KAFKA_BOOTSTRAP_SERVER={{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      - SPRING_KAFKA_PROPERTIES_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - SPRING_KAFKA_PROPERTIES_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - SPRING_KAFKA_PROPERTIES_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - SPRING_KAFKA_PROPERTIES_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - SPRING_KAFKA_PROPERTIES_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_TYPE="PKCS12"
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - SPRING_KAFKA_PROPERTIES_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - SPRING_KAFKA_PROPERTIES_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper-1:2181
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    init: true

  {%if DATAHUB_provision_sample_data %}
  datahub-ingestion:
    image: acryldata/datahub-ingestion:head
    hostname: datahub-ingestion
    container_name: datahub-ingestion
    labels:
      com.platys.name: "datahub"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/datahub/sample-data/sample_recipe.yml:/sample_recipe.yml:ro
      - ./init/datahub/sample-data/bootstrap_mce.json:/bootstrap_mce.json:ro
      - ./scripts/docker/wait-for-it.sh:/tmp/wait-for-it.sh
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: ["/tmp/wait-for-it.sh", "datahub-gms:8080", "--timeout=0", "--", "datahub"]
    command: "ingest -c /sample_recipe.yml"
    init: true
  {% endif -%}   {#  DATAHUB_provision_sample_data #}

  datahub-cli:
    image: acryldata/datahub-ingestion:head
    hostname: datahub-cli
    container_name: datahub-cli
    labels:
      com.platys.name: "datahub"
      com.platys.description: "Datahub Data Catalog CLI"
    entrypoint: "tail -f /dev/null"
    tty: true
    environment:
      - DATAHUB_GMS_URL=http://datahub-gms:8080
      - DATAHUB_GMS_PROTOCOL=http
      - DATAHUB_DEBUG=false
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DATALAB_enable #}

{% if OPENMETADATA_enable  | default(false) %}
  #  ================================== OpenMetadata ========================================== #
  openmetadata-migrate-all:
    image: docker.getcollate.io/openmetadata/server:{{__OPENMETADATA_version}}
    hostname: openmetadata-migrate-all
    container_name: openmetadata-migrate-all
    labels:
      com.platys.name: "openmetadata"
      com.platys.description: "OpenMetadata Data Catalog"
    depends_on:
      elasticsearch:
        condition: service_healthy
      mysql:
        condition: service_healthy
    environment:
      OPENMETADATA_CLUSTER_NAME: openmetadata
      SERVER_PORT: 8585
      SERVER_ADMIN_PORT: 8586
      LOG_LEVEL: INFO

      # Migration
      MIGRATION_LIMIT_PARAM: 1200

      # OpenMetadata Server Authentication Configuration
      AUTHORIZER_CLASS_NAME: "org.openmetadata.service.security.DefaultAuthorizer"
      AUTHORIZER_REQUEST_FILTER: "org.openmetadata.service.security.JwtFilter"
      AUTHORIZER_ADMIN_PRINCIPALS: "admin"
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: "all"
      AUTHORIZER_INGESTION_PRINCIPALS: ["ingestion-bot"]
      AUTHORIZER_PRINCIPAL_DOMAIN: "openmetadata.org"
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: false
      AUTHORIZER_ENABLE_SECURE_SOCKET: false
      AUTHENTICATION_PROVIDER: "basic"
      AUTHENTICATION_RESPONSE_TYPE: "id_token"
      CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME: ""
      AUTHENTICATION_PUBLIC_KEYS: [http://localhost:8585/api/v1/system/config/jwks]
      AUTHENTICATION_AUTHORITY: "https://accounts.google.com"
      AUTHENTICATION_CLIENT_ID: ""
      AUTHENTICATION_CALLBACK_URL: ""
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS: ["\"email\"","\"preferred_username\"","\"sub\""]
      AUTHENTICATION_ENABLE_SELF_SIGNUP: true

      # JWT Configuration
      RSA_PUBLIC_KEY_FILE_PATH: "./conf/public_key.der"
      RSA_PRIVATE_KEY_FILE_PATH: "./conf/private_key.der"
      JWT_ISSUER: "open-metadata.org"
      JWT_KEY_ID: "Gb389a-9f76-gdjs-a92j-0242bk94356"
      # OpenMetadata Server Pipeline Service Client Configuration
      PIPELINE_SERVICE_CLIENT_ENDPOINT: http://openmetadata-ingestion:8080
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: 300
      SERVER_HOST_API_URL: "http://openmetadata-server:8585/api"
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: "no-ssl"
      PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ""
  {% if OPENMETADATA_database_type == 'mysql' | default(false) %}      
      # Database configuration for MySQL
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_SCHEME: mysql
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: abc123!
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
  {% endif %}    
  {% if OPENMETADATA_database_type == 'postgres' | default(false) %}      
      # Database configuration for PostgreSQL
      DB_DRIVER_CLASS: org.postgresql.Driver
      DB_SCHEME: postgresql
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: abc123!
      DB_HOST: openmetadata-postgresql
      DB_PORT: 5432      
  {% endif %}    
      OM_DATABASE: openmetadata_db
      # ElasticSearch Configurations
      ELASTICSEARCH_HOST: openmetadata-elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      ELASTICSEARCH_USER: ""
      ELASTICSEARCH_PASSWORD: ""
      SEARCH_TYPE: "elasticsearch"
      ELASTICSEARCH_TRUST_STORE_PATH: ""
      ELASTICSEARCH_TRUST_STORE_PASSWORD: ""
      ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: 5
      ELASTICSEARCH_SOCKET_TIMEOUT_SECS: 60
      ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS: 600
      ELASTICSEARCH_BATCH_SIZE: 10
      ELASTICSEARCH_INDEX_MAPPING_LANG: EN

      #eventMonitoringConfiguration
      EVENT_MONITOR: prometheus
      EVENT_MONITOR_BATCH_SIZE: 10
      EVENT_MONITOR_PATH_PATTERN: ["/api/v1/tables/*", "/api/v1/health-check"]
      #EVENT_MONITOR_LATENCY: []

      #pipelineServiceClientConfiguration
      PIPELINE_SERVICE_CLIENT_ENABLED: true
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: "org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient"
      PIPELINE_SERVICE_IP_INFO_ENABLED: false
      PIPELINE_SERVICE_CLIENT_HOST_IP: ""
      PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: "noop"
      #airflow parameters
      AIRFLOW_USERNAME: admin
      AIRFLOW_PASSWORD: admin
      AIRFLOW_TIMEOUT: 10
      AIRFLOW_TRUST_STORE_PATH: ""
      AIRFLOW_TRUST_STORE_PASSWORD: ""
      FERNET_KEY: "jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA="

      #secretsManagerConfiguration
      SECRET_MANAGER: db
      #parameters:
      OM_SM_REGION: ""
      OM_SM_ACCESS_KEY_ID: ""
      OM_SM_ACCESS_KEY: ""

      #email configuration:
      OM_EMAIL_ENTITY: "OpenMetadata"
      OM_SUPPORT_URL: "https://slack.open-metadata.org"
      AUTHORIZER_ENABLE_SMTP : false
      OPENMETADATA_SERVER_URL: ""
      OPENMETADATA_SMTP_SENDER_MAIL: ""
      SMTP_SERVER_ENDPOINT: ""
      SMTP_SERVER_PORT: ""
      SMTP_SERVER_USERNAME: ""
      SMTP_SERVER_PWD: ""
      SMTP_SERVER_STRATEGY: "SMTP_TLS"

      # Heap OPTS Configurations
      OPENMETADATA_HEAP_OPTS: "--Xmx1G -Xms1G"
      # Mask passwords values in UI
      MASK_PASSWORDS_API: false

      #OpenMetadata Web Configuration
      WEB_CONF_URI_PATH: "/api"
      #HSTS
      WEB_CONF_HSTS_ENABLED: false
      WEB_CONF_HSTS_MAX_AGE: "365 days"
      WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: true
      WEB_CONF_HSTS_PRELOAD: "true"
      #Frame Options
      WEB_CONF_FRAME_OPTION_ENABLED: false
      WEB_CONF_FRAME_OPTION: "SAMEORIGIN"
      WEB_CONF_FRAME_ORIGIN: ""
      #Content Type
      WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: false
      #XSS-Protection
      WEB_CONF_XSS_PROTECTION_ENABLED: false
      WEB_CONF_XSS_PROTECTION_ON: true
      WEB_CONF_XSS_PROTECTION_BLOCK: true
      #CSP
      WEB_CONF_XSS_CSP_ENABLED: false
      WEB_CONF_XSS_CSP_POLICY: "default-src 'self'"
      WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ""

      #Referrer-Policy
      WEB_CONF_REFERRER_POLICY_ENABLED: false
      WEB_CONF_REFERRER_POLICY_OPTION: "SAME_ORIGIN"
      #Permission-Policy
      WEB_CONF_PERMISSION_POLICY_ENABLED: false
      WEB_CONF_PERMISSION_POLICY_OPTION: ""
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: "./bootstrap/openmetadata-ops.sh migrate"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    init: true

  openmetadata-server:
    image: docker.getcollate.io/openmetadata/server:{{__OPENMETADATA_version}}
    hostname: openmetadata-server
    container_name: openmetadata-server
    labels:
      com.platys.name: "openmetadata"
      com.platys.description: "OpenMetadata Data Catalog"
      com.platys.webui.title: "OpenMetadata UI"
      com.platys.webui.url: "https://dataplatform:8585"
    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      elasticsearch:
        condition: service_healthy
      mysql:
        condition: service_healthy
      openmetadata-migrate-all:
        condition: service_completed_successfully
    environment:
      OPENMETADATA_CLUSTER_NAME: openmetadata
      SERVER_PORT: 8585
      SERVER_ADMIN_PORT: 8586
      LOG_LEVEL: INFO

      # OpenMetadata Server Authentication Configuration
      AUTHORIZER_CLASS_NAME: "org.openmetadata.service.security.DefaultAuthorizer"
      AUTHORIZER_REQUEST_FILTER: "org.openmetadata.service.security.JwtFilter"
      AUTHORIZER_ADMIN_PRINCIPALS: ["admin"]
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ["all"]
      AUTHORIZER_INGESTION_PRINCIPALS: ["ingestion-bot"]
      AUTHORIZER_PRINCIPAL_DOMAIN: "openmetadata.org"
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: false
      AUTHORIZER_ENABLE_SECURE_SOCKET: false
      AUTHENTICATION_PROVIDER: "basic"
      AUTHENTICATION_RESPONSE_TYPE: "id_token"
      CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME: ""
      AUTHENTICATION_PUBLIC_KEYS: ["http://localhost:8585/api/v1/system/config/jwks"]
      AUTHENTICATION_AUTHORITY: https://accounts.google.com
      AUTHENTICATION_CLIENT_ID: ""
      AUTHENTICATION_CALLBACK_URL: ""
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS: ["email","preferred_username","sub"]
      AUTHENTICATION_ENABLE_SELF_SIGNUP: true

      # JWT Configuration
      RSA_PUBLIC_KEY_FILE_PATH: "./conf/public_key.der"
      RSA_PRIVATE_KEY_FILE_PATH: "./conf/private_key.der"
      JWT_ISSUER: "open-metadata.org"
      JWT_KEY_ID: "Gb389a-9f76-gdjs-a92j-0242bk94356"
      # OpenMetadata Server Pipeline Service Client Configuration
      PIPELINE_SERVICE_CLIENT_ENDPOINT: http://openmetadata-ingestion:8080
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: 300
      SERVER_HOST_API_URL: http://openmetadata-server:8585/api
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: "no-ssl"
      PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ""
  {% if OPENMETADATA_database_type == 'mysql' | default(false) %}      
      # Database configuration for MySQL
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_SCHEME: mysql
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: abc123!
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
  {% endif %}    
  {% if OPENMETADATA_database_type == 'postgres' | default(false) %}      
      # Database configuration for PostgreSQL
      DB_DRIVER_CLASS: org.postgresql.Driver
      DB_SCHEME: postgresql
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: abc123!
      DB_HOST: openmetadata-postgresql
      DB_PORT: 5432      
  {% endif %}    
      OM_DATABASE: openmetadata_db
      # ElasticSearch Configurations
      ELASTICSEARCH_HOST: openmetadata-elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      ELASTICSEARCH_USER: ""
      ELASTICSEARCH_PASSWORD: ""
      SEARCH_TYPE: "elasticsearch"
      ELASTICSEARCH_TRUST_STORE_PATH: ""
      ELASTICSEARCH_TRUST_STORE_PASSWORD: ""
      ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: 5
      ELASTICSEARCH_SOCKET_TIMEOUT_SECS: 60
      ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS: 600
      ELASTICSEARCH_BATCH_SIZE: 10
      ELASTICSEARCH_INDEX_MAPPING_LANG: EN

      #eventMonitoringConfiguration
      EVENT_MONITOR: prometheus
      EVENT_MONITOR_BATCH_SIZE: 10
      EVENT_MONITOR_PATH_PATTERN: ["/api/v1/tables/*", "/api/v1/health-check"]
      #EVENT_MONITOR_LATENCY: []

      #pipelineServiceClientConfiguration
      PIPELINE_SERVICE_CLIENT_ENABLED: true
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: "org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient"
      PIPELINE_SERVICE_IP_INFO_ENABLED: false
      PIPELINE_SERVICE_CLIENT_HOST_IP: ""
      PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: "noop"
      #airflow parameters
      AIRFLOW_USERNAME: admin
      AIRFLOW_PASSWORD: admin
      AIRFLOW_TIMEOUT: 10
      AIRFLOW_TRUST_STORE_PATH: ""
      AIRFLOW_TRUST_STORE_PASSWORD: ""
      FERNET_KEY: "jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA="

      #secretsManagerConfiguration
      SECRET_MANAGER: db
      #parameters:
      OM_SM_REGION: ""
      OM_SM_ACCESS_KEY_ID: ""
      OM_SM_ACCESS_KEY: ""

      #email configuration:
      OM_EMAIL_ENTITY: "OpenMetadata"
      OM_SUPPORT_URL: "https://slack.open-metadata.org"
      AUTHORIZER_ENABLE_SMTP : false
      OPENMETADATA_SERVER_URL: ""
      OPENMETADATA_SMTP_SENDER_MAIL: ""
      SMTP_SERVER_ENDPOINT: ""
      SMTP_SERVER_PORT: ""
      SMTP_SERVER_USERNAME: ""
      SMTP_SERVER_PWD: ""
      SMTP_SERVER_STRATEGY: "SMTP_TLS"

      # Heap OPTS Configurations
      OPENMETADATA_HEAP_OPTS: "--Xmx1G -Xms1G"
      # Mask passwords values in UI
      MASK_PASSWORDS_API: false

      #OpenMetadata Web Configuration
      WEB_CONF_URI_PATH: "/api"
      #HSTS
      WEB_CONF_HSTS_ENABLED: false
      WEB_CONF_HSTS_MAX_AGE: "365 days"
      WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: true
      WEB_CONF_HSTS_PRELOAD: "true"
      #Frame Options
      WEB_CONF_FRAME_OPTION_ENABLED: false
      WEB_CONF_FRAME_OPTION: "SAMEORIGIN"
      WEB_CONF_FRAME_ORIGIN: ""
      #Content Type
      WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: false
      #XSS-Protection
      WEB_CONF_XSS_PROTECTION_ENABLED: false
      WEB_CONF_XSS_PROTECTION_ON: true
      WEB_CONF_XSS_PROTECTION_BLOCK: true
      #CSP
      WEB_CONF_XSS_CSP_ENABLED: false
      WEB_CONF_XSS_CSP_POLICY: "default-src 'self'"
      WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ""
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider",  "http://localhost:8586/healthcheck" ]

  {% if OPENMETADATA_database_type == 'mysql' | default(false) %}
  openmetadata-mysql:
    hostname: openmetadata-mysql
    container_name: openmetadata-mysql
    image: docker.getcollate.io/openmetadata/db:{{__OPENMETADATA_version}}
    labels:
      com.platys.name: "mysql"
      com.platys.description: "OpenMetadata Data Catalog MySQL Instance"
    expose:
      - 3306
    ports:
      - "3310:3306"
    environment:
      MYSQL_ROOT_PASSWORD: password
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if OPENMETADATA_volume_map_data %}
      - ./container-volume/openmetadata/mysql:/var/lib/mysql
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: "--sort_buffer_size=10M"
    restart: {{container_restart_policy}}
    healthcheck:
      test: mysql --user=root --password=$$MYSQL_ROOT_PASSWORD --silent --execute "use openmetadata_db"
      interval: 15s
      timeout: 10s
      retries: 10
  {% endif %}    {#  OPENMETADATA_database_type == 'mysql' #}

  {% if OPENMETADATA_database_type == 'postgres' | default(false) %}
  openmetadata-postgresql:
    image: docker.getcollate.io/openmetadata/postgresql:{{__OPENMETADATA_version}}
    container_name: openmetadata_postgresql
    hostname: openmetadata_postgresql
    labels:
      com.platys.name: "postgresql"
      com.platys.description: "OpenMetadata Data Catalog PostgreSQL Instance"
    expose:
      - 5432
    ports:
      - "5437:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
    {% if OPENMETADATA_volume_map_data %}
      - ./container-volume/openmetadata/postgresql:/var/lib/postgresql/data      
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}    
    command: "--work_mem=10MB"
    restart: {{container_restart_policy}}
  {% endif %}    {#  OPENMETADATA_database_type == 'postgres' #}

  openmetadata-elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch::8.11.4
    hostname: openmetadata-elasticsearch
    container_name: openmetadata-elasticsearch
    labels:
      com.platys.name: "elasticsearch"
      com.platys.description: "OpenMetadata Data Catalog Eleasticsearch Instance"
    ports:
      - "19203:9200"
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - xpack.security.enabled=false
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if OPENMETADATA_volume_map_data %}
      - ./container-volume/openmetadata/elasticsearch:/usr/share/elasticsearch/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: "curl -s http://localhost:9200/_cluster/health?pretty | grep status | grep -qE 'green|yellow' || exit 1"
      interval: 15s
      timeout: 10s
      retries: 10

  openmetadata-ingestion:
    image: docker.getcollate.io/openmetadata/ingestion:{{__OPENMETADATA_version}}
    hostname: openmetadata-ingestion
    container_name: openmetadata-ingestion
    depends_on:
      elasticsearch:
        condition: service_started
      mysql:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    labels:
      com.platys.name: "openmetadata"
      com.platys.description: "OpenMetadata Data Catalog Ingestion Service"
      com.platys.webui.title: "OpenMetadata Ingestion UI"
      com.platys.webui.url: "https://dataplatform:28337"
    expose:
      - 8080
    ports:
      - "28337:8080"
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
  {% if OPENMETADATA_database_type == 'mysql' | default(false) %}      
      # Database configuration for MySQL
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
      AIRFLOW_DB: airflow_db
      DB_USER: airflow_user
      DB_SCHEME: mysql+pymysql
      DB_PASSWORD: abc123!
      # extra connection-string properties for the database
      # EXAMPLE
      # require SSL (only for Postgres)
      # properties: "?sslmode=require"
      DB_PROPERTIES: ""      
  {% endif %}    
  {% if OPENMETADATA_database_type == 'postgres' | default(false) %}      
      # Database configuration for PostgreSQL
      DB_HOST: openmetadata-postgresql
      DB_PORT: 5432      
      AIRFLOW_DB: airflow_db
      DB_USER: airflow_user
      DB_SCHEME: postgresql+psycopg2
      DB_PASSWORD: abc123!
        # extra connection-string properties for the database
      # EXAMPLE
      # require SSL (only for Postgres)
      # properties: "?sslmode=require"
      DB_PROPERTIES: ""
  {% endif %}    
      # To test the lineage backend
      # AIRFLOW__LINEAGE__BACKEND: airflow_provider_openmetadata.lineage.backend.OpenMetadataLineageBackend
      # AIRFLOW__LINEAGE__AIRFLOW_SERVICE_NAME: local_airflow
      # AIRFLOW__LINEAGE__OPENMETADATA_API_ENDPOINT: http://openmetadata-server:8585/api
      # AIRFLOW__LINEAGE__JWT_TOKEN: ...
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./container-volume/openmetadata/ingestion/config:/opt/airflow/dag_generated_configs
      - ./container-volume/openmetadata/ingestion/dags:/opt/airflow/dags
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    restart: {{container_restart_policy}}
{% endif %}   {#  OPENMETADATA_enable #}

{% if AMUNDSEN_enable  | default(false) %}
  #  ================================== Amundsen ========================================== #
  amundsenfrontend:
    image: amundsendev/amundsen-frontend:{{__AMUNDSEN_FRONTEND_version}}
    container_name: amundsenfrontend
    hostname: amundsenfrontend
    labels:
      com.platys.name: "amundsen"
      com.platys.description: "Amundsen Data Catalog"
      com.platys.webui.title: "Amundsen UI"
      com.platys.webui.url: "http://dataplatform:5000"
    depends_on:
      - amundsenmetadata
      - amundsensearch
    ports:
      - 5000:5000
    environment:
      - METADATASERVICE_BASE=http://amundsenmetadata:5000
      - SEARCHSERVICE_BASE=http://amundsensearch:5000
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  amundsensearch:
    image: amundsendev/amundsen-search:{{__AMUNDSEN_SEARCH_version}}
    container_name: amundsensearch
    labels:
      com.platys.name: "amundsen"
      com.platys.description: "Amundsen Data Catalog"
      com.platys.restapi.title: "Amundsensearch REST API"
      com.platys.restapi.url: "http://dataplatform:5001"
    ports:
      - 5001:5000
    environment:
  {%if (AMUNDSEN_metastore | lower) == 'amundsen' | default(false) %}
      - PROXY_ENDPOINT=amundsen-elasticsearch
  {% else -%}   {#  AMUNDSEN_metastore #}
      - CREDENTIALS_PROXY_USER=admin
      - CREDENTIALS_PROXY_PASSWORD=abc123!
      - PROXY_ENDPOINT=atlas:21000
      - PROXY_CLIENT=ATLAS
  {% endif -%}   {#  AMUNDSEN_metastore #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  amundsenmetadata:
    image: amundsendev/amundsen-metadata:{{__AMUNDSEN_METADATA_version}}
    container_name: amundsenmetadata
    hostname: amundsenmetadata
    labels:
      com.platys.name: "amundsen"
      com.platys.description: "Amundsen Data Catalog"
      com.platys.restapi.title: "Amundsensearch REST API"
      com.platys.restapi.url: "http://dataplatform:5002"
    ports:
      - 5002:5000
    environment:
  {%if (AMUNDSEN_metastore | lower) == 'amundsen' | default(false) %}
      - PROXY_HOST=bolt://amundsen-neo4j
  {% else -%}   {#  AMUNDSEN_metastore #}
      - PROXY_ENDPOINT=amundsen-elasticsearch
      - CREDENTIALS_PROXY_USER=admin
      - CREDENTIALS_PROXY_PASSWORD=abc123!
      - PROXY_HOST=atlas
      - PROXY_PORT=21000
      - PROXY_CLIENT=ATLAS
  {% endif -%}   {#  AMUNDSEN_metastore #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {%if (AMUNDSEN_metastore | lower) == 'amundsen' | default(false) %}
  amundsen-neo4j:
    image: neo4j:3.3.0
    container_name: amundsen-neo4j
    hostname: amundsen-neo4j
    labels:
      com.platys.name: "amundsen"
      com.platys.description: "Amundsen Data Catalog Neo4J Instance"
      com.platys.webui.title: "Amundsen Neo4J UI"
      com.platys.webui.url: "http://dataplatform:17474"
    ulimits:
      nofile:
        soft: 40000
        hard: 40000
    ports:
      - 17474:7474
      - 17687:7687
    environment:
      - NEO4J_AUTH=neo4j/test
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/amundsen/neo4j/conf:/conf
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  amundsen-elasticsearch:
    image: elasticsearch:6.7.0
    container_name: amundsen-elasticsearch
    hostname: amundsen-elasticsearch
    labels:
      com.platys.name: "amundsen"
      com.platys.description: "Amundsen Data Catalog Elasticsearch Instance"
      com.platys.restapi.title: "Amundsen ElasticSearch REST API"
      com.platys.restapi.url: "http://dataplatform:19201"
    ports:
      - 19201:9200
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
      - cluster.routing.allocation.disk.threshold_enabled='true'
      - cluster.routing.allocation.disk.watermark.low=2gb
      - cluster.routing.allocation.disk.watermark.high=1gb
      - cluster.routing.allocation.disk.watermark.flood_stage=512mb
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  AMUNDSEN_edition #}
{% endif %}   {#  AMUNDSEN_enable #}

{% if DATA_PRODUCT_PORTAL_enable  | default(false) %}
  #  ================================== Data Product Portal ========================================== #
  data-product-portal-backend:
    image: public.ecr.aws/conveyordata/data-product-portal/backend:{{__DATA_PRODUCT_PORTAL_version}}
    container_name: data-product-portal-backend
    hostname: data-product-portal-backend
    labels:
      com.platys.name: "data-product-portal"
      com.platys.description: "Data Product Portal Backend"
      com.platys.restapi.title: "Data Product Portal REST API"
      com.platys.restapi.url: "http://dataplatform:5051/api/docs#"
    depends_on:
      data-product-portal-postgresql:
        condition: service_healthy
    environment:
      - POSTGRES_PORT=5432
      - POSTGRES_SERVER=data-product-portal-postgresql
      - POSTGRES_DB=data-product-portal
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=abc123!
      - CORS_ALLOWED_ORIGINS=
      - AWS_DEFAULT_REGION=eu-west-1
      - OIDC_ENABLED=false
      - LOGGING_DIRECTORY=./logs
      - INFRASTRUCTURE_LAMBDA_ARN=
      - SANDBOX=true
      - HOST=http://localhost:8080
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - bash 
      - -c
      - |
    {% if DATA_PRODUCT_PORTAL_provision_sample_data | default(false) %}
        python -m app.db_tool init --force sample_data.sql
    {% endif -%}   {#  DATA_PRODUCT_PORTAL_provision_sample_data #}
        uvicorn app.main:app --host 0.0.0.0 --port 5050       
    restart: {{container_restart_policy}}

  data-product-portal-frontend:
    image: public.ecr.aws/conveyordata/data-product-portal/frontend:{{__DATA_PRODUCT_PORTAL_version}}
    container_name: data-product-portal-frontend
    hostname: data-product-portal-frontend
    labels:
      com.platys.name: "data-product-portal"
      com.platys.description: "Data Product Portal Frontend"
      com.platys.webui.title: "Data Product Portal UI"
      com.platys.webui.url: "http://dataplatform:28419"            
    depends_on:
      data-product-portal-postgresql:
        condition: service_healthy
    environment:
      - PUBLIC_IP=${PUBLIC_IP}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/data-product-portal/frontend/config.docker.js:/tmp/config.docker.js.templ
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #} 
    entrypoint:
      - /bin/sh
      - -c
      - |
        eval "echo \"$$(cat /tmp/config.docker.js.templ)\"" >> /app/dist/config.js
        cat /app/dist/config.js
        # we cannot invoke /app/entrypoint.sh so we just copy the command from it
        nginx -g 'daemon off;'      
    restart: {{container_restart_policy}}

  data-product-portal-postgresql:
    image: postgres:16
    container_name: data-product-portal-postgresql
    hostname: data-product-portal-postgresql
    labels:
      com.platys.name: 'postgresql'
      com.platys.description: 'Data Product Portal Postgresql'
    ports:
      - "5439:5432"    
    environment:
      POSTGRES_DB: data-product-portal
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: abc123!
    {%if use_timezone | default(false) %} 
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
      - ./data-transfer:/data-transfer    
    {%if DATA_PRODUCT_PORTAL_volume_map_data | default(false) %} 
      - ./container-volume/data-product-portal/postgresql-data:/var/lib/postgresql/data
    {% endif -%}   {#  LANGWATCH_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}  
    restart: {{container_restart_policy}}      
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $postgres"]
      interval: 10s
      timeout: 5s
      retries: 120

  data-product-portal-nginx:
    image: nginx:1.25
    container_name: data-product-portal-nginx
    hostname: data-product-portal-nginx
    labels:
      com.platys.name: "data-product-portal"
      com.platys.description: "Data Product Portal Gateway"   
    ports:
      - 28419:80
      - 5051:5050
    depends_on:
      - data-product-portal-backend
      - data-product-portal-frontend
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
      - ./conf/data-product-portal/nginx/nginx.conf:/etc/nginx/conf.d/default.conf
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}            
{% endif %}   {#  DATA_PRODUCT_PORTAL_enable #}

{% if MARQUEZ_enable  | default(false) %}
  #  ================================== Marquez ========================================== #
  marquez-web:
    image: marquezproject/marquez-web:{{__MARQUEZ_WEB_version}}
    container_name: marquez-web
    hostname: marquez-web
    labels:
      com.platys.name: "marquez"
      com.platys.description: "Marquez Data Lineage"
      com.platys.webui.title: "Marquez UI"
      com.platys.webui.url: "http://dataplatform:3005"
    depends_on:
      - marquez
    ports:
      - "3005:3000"
    environment:
      - MARQUEZ_HOST=marquez
      - MARQUEZ_PORT=5000
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  marquez:
    image: marquezproject/marquez:{{__MARQUEZ_version}}
    container_name: marquez
    hostname: marquez
    labels:
      com.platys.name: "marquez"
      com.platys.description: "Marquez Data Lineage"
      com.platys.restapi.title: "Marquez REST API"
      com.platys.restapi.url: "http://dataplatform:5010/api/v1"
      com.platys.webui.title: "Marquez UI"
      com.platys.webui.url: "http://dataplatform:5011"
    depends_on:
      - marquez-db
    ports:
      - "5010:5000"
      - "5011:5001"
    environment:
      - MARQUEZ_PORT=5000
      - MARQUEZ_ADMIN_PORT=5001
      - MARQUEZ_CONFIG=/usr/src/app/config.yml
      - POSTGRES_HOST=marquez-db
      - POSTGRES_PORT=5432
      - POSTGRES_DB=marquez
      - POSTGRES_USER=marquez
      - POSTGRES_PASSWORD=abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - ./conf/marquez/marquez.yml:/usr/src/app/config.yml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: ["./wait-for-it.sh", "marquez-db:5432", "--", "./entrypoint.sh"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  marquez-db:
    image: "postgres:9.6"
    container_name: marquez-db
    hostname: marquez-db
    labels:
      com.platys.name: "marquez"
      com.platys.description: "Marquez Data Lineage Postgres Instance"
    ports:
      - "5434:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=abc123
      - MARQUEZ_DB=marquez
      - MARQUEZ_USER=marquez
      - MARQUEZ_PASSWORD=abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/marquez/postgresql/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
    {% if MARQUEZ_volume_map_data %}
      - "./container-volume/marquez/postgresql:/var/lib/postgresql/data"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if MARQUEZ_provision_marquez_sample_data | default(false) %}
  marquez-db-seed:
    image: "celfring/jq"
    container_name: marquez-db-seed
    hostname: marquez-db-seed
    labels:
      com.platys.name: "marquez"
    depends_on:
      - marquez-db
    environment:
      - MARQUEZ_HOST=marquez
      - MARQUEZ_PORT=5000
      - MARQUEZ_ADMIN_PORT=5001
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - ./init/marquez/postgresql/seed-db.sh:/seed-db.sh
      - ./init/marquez/postgresql/data:/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: ["/usr/src/app/wait-for-it.sh", "marquez:5000", "--", "./seed-db.sh"]
    init: true
  {% endif %}   {# MARQUEZ_provision_marquez_sample_data #}
{% endif %}   {#  MARQUEZ_enable #}

{% if CKAN_enable  | default(false) %}
  #  ================================== CKAN ========================================== #
  ckan:
    image: ckan/ckan-base:{{__CKAN_version}}{{'-dev' if CKAN_use_dev_edition}}
    container_name: ckan
    hostname: ckan
    labels:
      com.platys.name: "ckan"
      com.platys.description: "CKAN Publication Platform"
      com.platys.webui.title: "CKAN UI"
      com.platys.webui.url: "http://dataplatform:28294"
    ports:
      - "28294:5000"
    environment:
      - CKAN_SITE_ID=default
      - CKAN_PORT=5000
      - CKAN_SITE_URL=http://${PUBLIC_IP}:28294
#      - CKAN_SITE_URL=http://ckan:5000
      - CKAN_SYSADMIN_NAME=ckan_admin
      - CKAN_SYSADMIN_PASSWORD={{CKAN_sysadmin_password}}
      - CKAN_SYSADMIN_EMAIL=sysadmin@ckantest.com
      - CKAN_STORAGE_PATH=/var/lib/ckan
      - CKAN_SMTP_SERVER=smtp.corporateict.domain:25
      - CKAN_SMTP_STARTTLS=True
      - CKAN_SMTP_USER=user
      - CKAN_SMTP_PASSWORD=password
      - CKAN_SMTP_MAIL_FROM=ckan@localhost
      - REDIS_VERSION=6
      - REDIS_URL=redis://ckan-redis:6379/1
      - TEST_CKAN_REDIS_URL=redis://redis:6379/1
      - POSTGRES_HOST=ckan-db
      - POSTGRES_USER=ckan
      - POSTGRES_PASSWORD={{CKAN_postgres_password}}
      - DATASTORE_READONLY_USER={{CKAN_datastore_readonly_user}}
      - DATASTORE_READONLY_PASSWORD={{CKAN_datastore_readonly_password}}
      - CKAN__PLUGINS="envvars image_view text_view recline_view datastore datapusher"

      - CKAN__HARVEST__MQ__TYPE=redis
      - CKAN__HARVEST__MQ__HOSTNAME=redis
      - CKAN__HARVEST__MQ__PORT=6379
      - CKAN__HARVEST__MQ__REDIS_DB=1

      - CKAN__WEBASSETS__PATH=/srv/app/data/webassets
      - CKAN__DATAPUSHER__URL=http://ckan-datapusher:8800
      - CKAN__DATAPUSHER__CALLBACK_URL_BASE=http://ckan:5000/

      - CKAN_VERSION={{__CKAN_version}}
      - CKAN_CORE_NAME=ckan
      - CKAN_SQLALCHEMY_URL=postgresql://ckan:{{CKAN_postgres_password}}@ckan-db/ckan
      - CKAN_DATASTORE_WRITE_URL=postgresql://ckan:{{CKAN_postgres_password}}@ckan-db/datastore
      - CKAN_DATASTORE_READ_URL=postgresql://datastore_ro:{{CKAN_postgres_readonly_password}}@ckan-db/datastore
      - CKAN_SOLR_URL=http://ckan-solr:8983/solr/ckan
      - CKAN_MAX_UPLOAD_SIZE_MB=10
    {%if CKAN_use_s3_store and (MINIO_enable or external['S3_enable']) | default(false) %}
      # S3/MINIO settings
      - CKANEXT__S3FILESTORE__AWS_ACCESS_KEY_ID={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - CKANEXT__S3FILESTORE__AWS_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
      - CKANEXT__S3FILESTORE__AWS_BUCKET_NAME={{CKAN_s3_store_bucket_name}}
      - CKANEXT__S3FILESTORE__HOST_NAME={{s3Endpoint | default(omit) }}
      - CKANEXT__S3FILESTORE__REGION_NAME={{s3DefaultRegion | default(omit) }}
      - CKANEXT__S3FILESTORE__SIGNATURE_VERSION=s3v4
    {% endif %}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% else %}
      - TZ=UTC
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ckan/extensions:/srv/app/src
    {%if CKAN_volume_map_storage | default(false) %}
      - ./container-volume/ckan/storage:/var/lib/ckan
    {% endif -%}   {#  CKAN_volume_map_storage #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "wget", "-qO", "/dev/null", "http://localhost:5000"]

  {% if CKAN_DATAPUSHER_enable  | default(false) %}
  ckan-datapusher:
    image: ckan/ckan-base-datapusher:{{__CKAN_DATAPUSHER_version}}
    container_name: ckan-datapusher
    hostname: ckan-datapusher
    labels:
      com.platys.name: "ckan-datapusher"
      com.platys.description: "CKAN Publication Platform"
    ports:
      - "28295:8800"
    environment:
      - MAINTENANCE_MODE=false
      - DATAPUSHER_MAX_CONTENT_LENGTH=10485760
      - DATAPUSHER_CHUNK_SIZE=16384
      - DATAPUSHER_CHUNK_INSERT_ROWS=250
      - DATAPUSHER_DOWNLOAD_TIMEOUT=30
      - DATAPUSHER_SSL_VERIFY=false
      - DATAPUSHER_REWRITE_RESOURCES=true
      - DATAPUSHER_REWRITE_URL=http://ckan:5000
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "wget", "-qO", "/dev/null", "http://localhost:8800"]
  {% endif -%}   {#  CKAN_DATAPUSHER_enable #}

  ckan-db:
    image: postgis/postgis:14-master
    container_name: ckan-db
    hostname: ckan-db
    labels:
      com.platys.name: "ckan"
      com.platys.description: "CKAN Publication Platform PostgreSQL Instance"
    environment:
      - DATASTORE_READONLY_PASSWORD={{CKAN_postgres_readonly_password}}
      - POSTGRES_DB={{CKAN_postgres_db}}
      - POSTGRES_USER={{CKAN_postgres_user}}
      - POSTGRES_PASSWORD={{CKAN_postgres_password}}
      - PGDATA=/var/lib/postgresql/data/db
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/ckan/postgis:/docker-entrypoint-initdb.d/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "ckan"]

  ckan-solr:
    image: solr:8.11.1
    container_name: ckan-solr
    hostname: ckan-solr
    labels:
      com.platys.name: "ckan"
      com.platys.description: "CKAN Publication Platform SolR Instance"
    environment:
      - CKAN_CORE_NAME=ckan
      - CKAN_VERSION={{__CKAN_version}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/ckan/solr:/docker-entrypoint-initdb.d/ckan_init_solr.sh
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "wget", "-qO", "/dev/null", "http://localhost:8983/solr/"]

  ckan-redis:
    image: redis:6
    container_name: ckan-redis
    hostname: ckan-redis
    labels:
      com.platys.name: "ckan"
      com.platys.description: "CKAN Publication Platform Redis Instance"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "redis-cli", "-e", "QUIT"]
{% endif %}   {#  CKAN_enable #}

{% if DATAVERSE_enable  | default(false) %}
  #  ================================== CKAN ========================================== #
  dataverse:
    image: gdcc/dataverse:{{__DATAVERSE_version}}
    container_name: dataverse
    hostname: dataverse
    user: payara
    labels:
      com.platys.name: "dataverse"
      com.platys.description: "Dataverse Publication Platform"
      com.platys.webui.title: "Dataverse UI"
      com.platys.webui.url: "http://dataplatform:28394"
      com.platys.restapi.title: "Dataverse UI"
      com.platys.restapi.url: "http://dataplatform:28394/api/info/metrics/dataverses"
    ports:
      - "28394:8080"
      - "4848:4848"
      - "9009:9009" # JDWP
      - "8686:8686" # JMX
    environment:
      #- DATAVERSE_FQDN=localhost
      - DATAVERSE_SITEURL=http://${PUBLIC_IP}:28394
      - DATAVERSE_DB_HOST=dataverse-postgresql
      - DATAVERSE_DB_USER=dataverse
      - DATAVERSE_DB_PASSWORD=abc123!
      - DATAVERSE_MAIL_MTA_HOST={{DATAVERSE_mail_host}}
      - DATAVERSE_MAIL_MTA_PORT={{DATAVERSE_mail_port}}
      - DATAVERSE_MAIL_SYSTEM_EMAIL=Demo Dataverse <dataverse@example.org>
#      - DATAVERSE_MAIL_USER=eadp
      # there is a bug and therefore DATAVERSE_SOLR_HOST does not work
      - DATAVERSE_SOLR_HOST=dataverse-solr
      - DATAVERSE_SOLR_PORT=8983
      - DATAVERSE_SOLR_CORE=collection1
      - DATAVERSE_API_ALLOW_INCOMPLETE_METADATA=1
#     - STORAGE_DIR=/dv  # does not work as folder is not available, would neeed to be mapped or created dynamically
      - ENABLE_JMX=0
      - ENABLE_RELOAD=0
      - ENABLE_JDWP=1
      - DATAVERSE_SPI_EXPORTERS_DIRECTORY="/dv/exporters"
      - JVM_ARGS=-Ddataverse.files.storage-driver-id=file1
        -Ddataverse.files.file1.type=file
        -Ddataverse.files.file1.label=Filesystem
        -Ddataverse.files.file1.directory=${STORAGE_DIR}/store
        -Ddataverse.pid.providers=fake
        -Ddataverse.pid.default-provider=fake
        -Ddataverse.pid.fake.type=FAKE
        -Ddataverse.pid.fake.label=FakeDOIProvider
        -Ddataverse.pid.fake.authority=10.5072
        -Ddataverse.pid.fake.shoulder=FK2/        
      - dataverse_files_storage__driver__id={{DATAVERSE_default_storage if DATAVERSE_default_storage is defined and DATAVERSE_default_storage and DATAVERSE_default_storage | length else omit}}
    {%if DATAVERSE_s3_bucket is defined and DATAVERSE_s3_bucket and DATAVERSE_s3_bucket | length and (MINIO_enable or external['S3_enable']) | default(false) %}
      - dataverse_files_s3_type=s3
      - dataverse_files_s3_label="Object Storage"
      - dataverse_files_s3_bucket__name="{{DATAVERSE_s3_bucket}}"
      - dataverse_files_s3_custom__endpoint__url={{s3Endpoint | default(omit) }}
      - dataverse_files_s3_custom__endpoint__region='{{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}'
      - dataverse_files_s3_path__style__access={{s3PathStyleAccess}}
      - dataverse_files_s3_download__redirect={{DATAVERSE_download_redirect}}
      - dataverse_files_s3_upload__redirect={{DATAVERSE_upload_redirect}}
      - dataverse_files_s3_url__expiration__minutes={{DATAVERSE_url_expiration_minutes if DATAVERSE_url_expiration_minutes is defined and DATAVERSE_url_expiration_minutes and DATAVERSE_url_expiration_minutes > 0 else omit}}
      - dataverse_files_s3_ingestsizelimit={{DATAVERSE_direct_upload_limit if DATAVERSE_direct_upload_limit is defined and DATAVERSE_direct_upload_limit and DATAVERSE_direct_upload_limit > 0 else omit}}
      - dataverse_files_s3_min__part__size={{DATAVERSE_minimal_part_size if DATAVERSE_minimal_part_size is defined and DATAVERSE_minimal_part_size and DATAVERSE_minimal_part_size > 0 else omit}}
      - dataverse_files_s3_access__key={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - dataverse_files_s3_secret__key={{s3SecretAccessKey}}
    {% endif -%} {#  DATAVERSE_s3_bucket and MINIO_enable #}
    {% if KEYCLOAK_enable | default(false) %}
      - DATAVERSE_FEATURE_API_BEARER_AUTH=1
      - DATAVERSE_AUTH_OIDC_ENABLED=1
      - DATAVERSE_AUTH_OIDC_CLIENT_ID=test
      - DATAVERSE_AUTH_OIDC_CLIENT_SECRET=94XHrfNRwXsjqTqApRrwWmhDLDHpIYV8
      - DATAVERSE_AUTH_OIDC_AUTH_SERVER_URL=http://${PUBLIC_IP}:28204/realms/dataverse
    {% endif -%}   {#  KEYCLOAK_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if DATAVERSE_volume_map_data | default(false) %}
      - ./container-volume/dataverse/data:/data
    {% endif -%}   {#  DATAVERSE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  dataverse-bootstrap:
    container_name: dataverse-bootstrap
    hostname: dataverse-bootstrap
    image: gdcc/configbaker:{{DATAVERSE_version}}
    environment:
      - DATAVERSE_URL=http://${PUBLIC_IP}:28394
      - TIMEOUT=10m
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAVERSE_volume_map_bootstrap_script  | default(false) %}
      - ./scripts/dataverse/bootstrap/init.sh:/scripts/bootstrap/{{DATAVERSE_bootstrap_persona | default("dev")}}/init.sh
    {% endif -%}   {#  DATAVERSE_volume_map_data #}
    command:
      - bootstrap.sh
      - {{DATAVERSE_bootstrap_persona | default("dev")}}
    init: true
    restart: "no"

  {% if DATAVERSE_previewers_configurer_enabled  | default(false) %}
  dataverse-setup-previewers:
    container_name: dataverse-setup-previewers
    hostname: dataverse-setup-previewers
    image: trivadis/dataverse-deploy-previewers:latest
    init: true
    environment:
      - API_TOKEN=${API_TOKEN}
      - DATAVERSE_URL=http://dataverse:8080
      - TIMEOUT=10m
      - PREVIEWERS_PROVIDER_URL={{DATAVERSE_previewers_provider_url | default("https://gdcc.github.io/dataverse-previewers")}}
      - INCLUDE_PREVIEWERS={{DATAVERSE_previewers_include | default("")}}
      - EXCLUDE_PREVIEWERS={{DATAVERSE_previewers_exclude | default("")}}
      - REMOVE_EXISTING={{DATAVERSE_previewers_remove_existing | default(omit)}}
    command:
      - deploy 
    restart: "no"
  {% endif -%}   {#  if DATAVERSE_previewers_configurer_enabled ... #}
  
  dataverse-demo-data-load:
    container_name: dataverse-demo-data-load
    hostname: dataverse-demo-data-load
    image: python
    profiles:
      - load
    environment:
      - API_TOKEN=${API_TOKEN}
      - DATAVERSE_URL=http://${PUBLIC_IP}:28394
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/python/run_pip_and_python.sh:/app/run_pip_and_python.sh:ro
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - sh
      - -c
      - |
        git clone https://github.com/gschmutz/dataverse-sample-data
        cd dataverse-sample-data
        pip install -r requirements.txt
        python create_sample_data.py
    init: true
    restart: "no"

  dataverse-postgresql:
    image: postgres:13
    container_name: dataverse-postgresql
    hostname: dataverse-postgresql
    labels:
      com.platys.name: "postgresql"
      com.platys.description: "Dataverse Publication Platform PostgreSQL Instance"
    ports:
      - "5435:5432"
    environment:
      - POSTGRES_DB=dataverse
      - POSTGRES_USER=dataverse
      - POSTGRES_PASSWORD=abc123!
      - POSTGRES_PORT=5432
    restart: {{container_restart_policy}}

  dataverse-solr:
    image: solr:9.3.0
    container_name: dataverse-solr
    hostname: dataverse-solr
    labels:
      com.platys.name: "solr"
      com.platys.description: "Dataverse Platform SolR Instance"
    environment:
      - DATAVERSE_CORE_NAME=collection1
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/dataverse/solr:/docker-entrypoint-initdb.d/    
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if DATAVERSE_previewers_provider_enabled | default(false) %}
  #  ================================== Dataverse Previewers Provider ========================================== #
  dataverse-previewers-provider:
    image: trivadis/dataverse-previewers-provider:latest
    container_name: dataverse-previewers-provider
    hostname: dataverse-previewers-provider
    labels:
      com.platys.name: "dataverse"
      com.platys.description: "Dataverse Previewers"
      com.platys.webui.title: "Dataverse Previewers"
      com.platys.webui.url: "http://dataplatform:28395/previewers/"
    ports:
      - "28395:8080"
    environment:
      - NGINX_HTTP_PORT=8080
      - PREVIEWERS_PROVIDER_URL=http://${PUBLIC_IP}:28395
      - VERSIONS="v1.4,betatest"
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% endif %}   {#  DATAVERSE_previewers_provider_enabled #}

{% endif %}   {#  DATAVERSE_enable #}

{% if (HUE_enable) | default(false) %}
  #  ================================== Apache Hue ========================================== #
  hue:
    image: gethue/hue:{{__HUE_version}}
    container_name: hue
    hostname: hue
    dns: 8.8.8.8
    depends_on:
      - hue-db
      - solr
    labels:
      com.platys.name: "hue"
      com.platys.description: "Hue SQL Assistant"
      com.platys.webui.title: "Hue UI"
      com.platys.webui.url: "http://dataplatform:8888"
    ports:
      - "8888:8888"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/hue/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  hue-db:
    image: postgres:{{__HUE_POSTGRES_version}}
    container_name: hue-db
    hostname: hue-db
    labels:
      com.platys.name: "hue"
      com.platys.description: "Hue SQL Assistant PostgreSQL Instance"
    environment:
      POSTGRES_DB: hue
      POSTGRES_PASSWORD: hue
      POSTGRES_USER: hue
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  HUE_enable #}

{%if STREAMSETS_enable | default(false) %}
  #  ================================== StreamSets DataCollector ========================================== #
  streamsets-1:
    image: streamsets/datacollector:{{__STREAMSETS_version}}
    container_name: streamsets-1
    hostname: streamsets-1
    labels:
      com.platys.name: 'streamsets'
      com.platys.description: "StreamSets Data Integration Engine"
      com.platys.webui.title: "StreamSets Data Collector UI"
      com.platys.webui.url: "http://dataplatform:18630"
      com.platys.restapi.title: "StreamSets Data Collector REST API"
      com.platys.restapi.url: "http://dataplatform:18630/collector/restapi"
    ports:
      - "18630:18630"
    {%if STREAMSETS_activate_https | default(false) %}
      - "18629:18629"
    {% endif %}
  {% for num in range(STREAMSETS_additional_port_mappings | default(0) ) %}
      {% set external_port = 28500 + loop.index - 1 %}
      {% set webui_port = 28500 + loop.index - 1 %}
      - "{{external_port}}:{{webui_port}}"
  {% endfor %}
    environment:
      SDC_OFFSET_DIRECTORY: /data/custom-offset-el
    {%if __STREAMSETS_version >= '3.19.1' %}
      SDC_INSTALL_STAGES: "{{STREAMSETS_stage_libs}}"
      SDC_INSTALL_ENTERPRISE_STAGES: "{{STREAMSETS_enterprise_stage_libs}}"
      {%if STREAMSETS_install_pipelines is defined and STREAMSETS_install_pipelines | default(false) %}
      SDC_INSTALL_PIPELINES_FROM: '/tmp/pipelines'
      {% endif %}
    {% endif %}
      SDC_JAVA_OPTS: "-Xmx2g -Xms2g -Dlog4j2.formatMsgNoLookups=true"
      SDC_JAVA8_OPTS: "-XX:+UseG1GC"
      SDC_CONF_MONITOR_MEMORY: "true"
      SDC_CONF_PIPELINE_MAX_RUNNERS_COUNT: 50
    {% if STREAMSETS_sdc_id is defined and STREAMSETS_sdc_id %}
      PLATYS_SDC_ID: {{STREAMSETS_sdc_id}}
    {% endif %}
    {% if STREAMSETS_http_authentication is defined and STREAMSETS_http_authentication %}
      SDC_CONF_http_authentication: "{{STREAMSETS_http_authentication}}"
    {% endif %}
    {%if STREAMSETS_activate_https | default(false) %}
      SDC_CONF_HTTPS_PORT: 18629
    {% endif %}
      SDC_CONF_RUNTIME_CONF_LOCATION: {{"configuration.properties" if STREAMSETS_use_external_conf_file else "embedded"}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if STREAMSETS_volume_map_data %}
      - ./container-volume/streamsets/data:/data:Z
    {% endif %}
    {% if STREAMSETS_volume_map_logs %}
      - ./container-volume/streamsets/logs:/logs:Z
    {% endif %}
    {% if STREAMSETS_volume_map_security_policy %}
      - ./custom-conf/streamsets/sdc-security.policy:/etc/sdc/sdc-security.policy:Z
    {% endif %}
    {%if STREAMSETS_install_pipelines is defined and STREAMSETS_install_pipelines | default(false) %}
      - ./scripts/streamsets/pipelines:/tmp/pipelines
    {% endif %}
      - ./conf/streamsets/pre-docker-entrypoint.sh:/pre-docker-entrypoint.sh
    {% if STREAMSETS_use_external_conf_file %}
      - ./custom-conf/streamsets/configuration.properties:/etc/sdc/configuration.properties
    {% endif %}
      - ./plugins/streamsets/user-libs:/opt/streamsets-datacollector-user-libs:Z
    {% for jdbcjar in STREAMSETS_jdbc_jars.split(",") %}
      - ./plugins/streamsets/libs-extras/streamsets-datacollector-jdbc-lib/{{jdbcjar}}:/opt/streamsets-datacollector-{{STREAMSETS_version}}/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/{{jdbcjar}}:Z
    {% endfor -%}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    ulimits:
      nofile:
        soft: 32768
        hard: 32768
    user: "{{uid | default(1000)}}"
    command:
      - "dc"
      - "-exec"
      - "-verbose"
    entrypoint:
      - "/pre-docker-entrypoint.sh"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STREAMSETS_enable  #}

{%if STREAMSETS_TRANSFORMER_enable | default(false) %}
  #  ================================== StreamSets Transformer ========================================== #
  streamsets-transformer-1:
    image: streamsets/transformer:{{__STREAMSETS_TRANSFORMER_version}}
    container_name: streamsets-transformer-1
    hostname: streamsets-transformer-1
    labels:
      com.platys.name: "streamsets-transformer"
      com.platys.description: "StreamSets Data Transformer Engine"
      com.platys.webui.title: "StreamSets Transformer UI"
      com.platys.webui.url: "http://dataplatform:19630"
      com.platys.restapi.title: "StreamSets Transformer REST API"
      com.platys.restapi.url: "http://dataplatform:19630/collector/restapi"
    ports:
      - "19630:19630"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if STREAMSETS_TRANSFORMER_volume_map_data %}
      - ./container-volume/streamsets-transformer-1:/data:Z
    {% endif %}
#      - ./container-volume/streamsets-transformer/data:/data:Z
#      - ./streamsets-extras/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/postgresql-42.2.6.jar:/opt/streamsets-datacollector-{{__STREAMSETS_version}}/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/postgresql-42.2.6.jar:Z
#      - ./streamsets-extras/libs-common-lib:/opt/streamsets-datacollector-{{__STREAMSETS_version}}/libs-common-lib:Z
#      - ./streamsets-extras/user-libs:/opt/streamsets-datacollector-user-libs:Z
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STREAMSETS_TRANSFORMER_enable  #}

{%if STREAMSETS_EDGE_enable | default(false) %}
  #  ================================== StreamSets DataCollector Edge ========================================== #
  streamsets-edge-1:
    image: streamsets/datacollector-edge:{{__STREAMSETS_EDGE_version}}
    container_name: streamsets-edge-1
    hostname: streamsets-edge-1
    labels:
      com.platys.name: "streamsets-edge"
      com.platys.description: "StreamSets Data Integration Engine"
      com.platys.restapi.title: "StreamSets Data Collector Edge REST API"
      com.platys.restapi.url: "http://dataplatform:18633"
    ports:
      - "18633:18633"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STREAMSETS_enable  #}

{%if STREAMSETS_DATAOPS_enable | default(false) %}
  #  ================================== StreamSets DataOps Platform ========================================== #
  streamsets-dataops-1:
    image: streamsets/datacollector:{{__STREAMSETS_DATAOPS_version}}
    container_name: streamsets-dataops-1
    hostname: streamsets-dataops-1
    labels:
      com.platys.name: "streamsets-dataops"
      com.platys.description: "StreamSets Data Integration Engine"
    environment:
      STREAMSETS_DEPLOYMENT_SCH_URL: '{{STREAMSETS_DATAOPS_deployment_sch_url}}'
      STREAMSETS_DEPLOYMENT_ID: {{STREAMSETS_DATAOPS_deployment_id if STREAMSETS_DATAOPS_deployment_id is defined and STREAMSETS_DATAOPS_deployment_id != None else '${STREAMSETS_DATAOPS_DEPLOYMENT_ID}' }}
      STREAMSETS_DEPLOYMENT_TOKEN: {{STREAMSETS_DATAOPS_deployment_token if STREAMSETS_DATAOPS_deployment_token is defined and STREAMSETS_DATAOPS_deployment_token != None else '${STREAMSETS_DATAOPS_DEPLOYMENT_TOKEN}' }}
      SDC_JAVA_OPTS: "-Dlog4j2.formatMsgNoLookups=true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    ulimits:
      nofile:
        soft: 32768
        hard: 32768
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STREAMSETS_DATAOPS_enable  #}

{%if NIFI_enable | default(false) %}
  #  ================================== NiFi ========================================== #
  {% for num in range(NIFI_nodes | default('1') ) %}
    {% set externalPort = 18080 + loop.index-1 %}
    {% set externalRemotePort = 10005 + loop.index-1 %}
    {% set externalPromPort = 1270 + loop.index-1 %}
    {% set clusterPort = 11005 + loop.index-1 %}
  nifi-{{loop.index}}:
    image: {{NIFI_custom_image_name if NIFI_custom_image_name is defined and NIFI_custom_image_name | length else 'trivadis/apache-nifi' if NIFI_python_enabled else 'apache/nifi'}}:{{__NIFI_version}}{{'-python' ~ NIFI_python_version if NIFI_python_enabled}}
    container_name: nifi-{{loop.index}}
    hostname: nifi-{{loop.index}}
    labels:
      com.platys.name: 'nifi'
      com.platys.description: "NiFi Data Integration Engine"
      com.platys.webui.title: "Apache NiFi UI"
      com.platys.webui.url: "http{{ 's' if NIFI_run_secure}}://dataplatform:{{externalPort}}/nifi"
      com.platys.restapi.title: "Apche NiFi REST API"
      com.platys.restapi.url: "http{{ 's' if NIFI_run_secure}}://dataplatform:{{externalPort}}/nifi-api"
      com.platys.password.envvars: "PLATYS_NIFI_PASSWORD"
    ports:
      # HTTP
      - "{{externalPort}}:{{externalPort}}"
      # Remote Input Socket
      - {{externalRemotePort}}:{{externalRemotePort}}/tcp
      # Prometheus Port
      - {{externalPromPort}}:1234
  {%if loop.index == 1 | default(false) %}
    {% for num in range(NIFI_additional_port_mappings | default(0) ) %}
      {% set external_port = 28510 + loop.index - 1 %}
      {% set webui_port = 28510 + loop.index - 1 %}
      - "{{external_port}}:{{webui_port}}"
    {% endfor %}
  {% endif %}
    environment:
    {%if NIFI_create_cluster  | default(false) %}
      NIFI_CLUSTER_IS_NODE: 'true'
      NIFI_CLUSTER_ADDRESS: nifi-{{loop.index}}
      NIFI_CLUSTER_NODE_PROTOCOL_PORT: {{clusterPort}}
      NIFI_ZK_CONNECT_STRING: zookeeper-1:2181
      NIFI_ZK_ROOT_NODE: '/nifi'
      NIFI_ELECTION_MAX_WAIT: '{{NIFI_election_max_wait}}'
    {% endif -%}   {#  NIFI_create_cluster #}
    {%if NIFI_run_secure  | default(false) %}
      NIFI_WEB_HTTPS_PORT: '{{externalPort}}'
      NIFI_WEB_HTTPS_HOST: 0.0.0.0
      NIFI_WEB_PROXY_HOST: '${PUBLIC_IP}:{{externalPort}},${DOCKER_HOST_IP}:{{externalPort}}'
      {% if NIFI_use_custom_certs is defined and NIFI_use_custom_certs | default(false) %}
      KEYSTORE_PATH: /opt/certs/keystore.jks
      KEYSTORE_TYPE: JKS
      KEYSTORE_PASSWORD: {{NIFI_keystore_password if NIFI_keystore_password is defined and NIFI_keystore_password | length else 'BWt7eLC4R53N4bIxdZEVPhAdkYlQ0Le8Tw0erGXpvI0' }}
      KEY_PASSWORD: {{NIFI_key_password if NIFI_key_password is defined and NIFI_key_password  | length else 'BWt7eLC4R53N4bIxdZEVPhAdkYlQ0Le8Tw0erGXpvI0' }}
      TRUSTSTORE_PATH: /opt/certs/truststore.jks
      TRUSTSTORE_TYPE: JKS
      TRUSTSTORE_PASSWORD: {{NIFI_truststore_password if NIFI_truststore_password is defined and NIFI_truststore_password | length else 'PkhvPAirou5R/K9HQxVk99uT+hehM7SRURyFSbxTlTQ' }}
      {% endif -%}  {#  NIFI_use_custom_certs #}
      NIFI_SECURITY_USER_AUTHORIZER: single-user-authorizer
      NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER: single-user-provider
    {% else %}
      NIFI_WEB_HTTP_PORT: '{{externalPort}}'
    {% endif -%}   {#  NIFI_run_unsecure #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      S3_ENDPOINT: '{{s3Endpoint | default(omit) }}'
      S3_PATH_STYLE_ACCESS: '{{s3PathStyleAccess}}'
      S3_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
    {% endif -%}    {#  MINIO_enable #}
      NIFI_REMOTE_INPUT_SOCKET_PORT: '{{externalRemotePort}}'
      NIFI_REMOTE_INPUT_HOST: nifi-{{loop.index}}
      NIFI_JVM_HEAP_INIT: {{ NIFI_jvm_heap_init if NIFI_jvm_heap_init is defined and NIFI_jvm_heap_init != None else omit }}
      NIFI_JVM_HEAP_MAX: {{ NIFI_jvm_heap_max if NIFI_jvm_heap_max is defined and NIFI_jvm_heap_max != None else omit }}
      NIFI_SENSITIVE_PROPS_KEY: '12345678901234567890A'
      SINGLE_USER_CREDENTIALS_USERNAME: '{{NIFI_username}}'
      SINGLE_USER_CREDENTIALS_PASSWORD: '${PLATYS_NIFI_PASSWORD:-{{NIFI_password}}}'
      INITIAL_ADMIN_IDENTITY: '{{ NIFI_inital_admin_identitiy if NIFI_inital_admin_identitiy is defined and NIFI_inital_admin_identitiy | length else NIFI_username }}'
    {% if VAULT_enable and VAULT_use_dev_mode | default(false) %}
      VAULT_TOKEN: {{VAULT_dev_mode_token}}
    {% endif -%}   {#  VAULT_use_dev_mode #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    {%if external['S3_enable'] | default(false) %}
    env_file: .env
    {% endif -%}   {#  external S3 #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if NIFI_volume_map_addl_jars  | default(false) %}
      - ./plugins/nifi/addl-jars/:/opt/nifi/nifi-current/addl-jars/
    {% endif -%}   {#  NIFI_volume_map_addl_jars #}
      - ./plugins/nifi/nars/:/opt/nifi/nifi-current/extensions/
    {%if NIFI_run_secure  | default(false) %}
      {% if NIFI_use_custom_certs is defined and NIFI_use_custom_certs | default(false) %}
      - ./custom-conf/nifi/security/nifi-{{loop.index}}/keystore.jks:/opt/certs/keystore.jks
      - ./custom-conf/nifi/security/nifi-{{loop.index}}/truststore.jks:/opt/certs/truststore.jks
      {% endif -%}  {#  NIFI_use_custom_certs #}
    {% endif -%}   {#  NIFI_run_unsecure #}
    {% if NIFI_volume_map_custom_config %}
      - ./custom-conf/nifi/config/nifi.properties:/tmp/nifi.properties.templ
    {% endif %}
    {% if NIFI_volume_map_data %}
      - ./container-volume/nifi/nifi-{{loop.index}}/database_repository:/opt/nifi/nifi-current/database_repository
      - ./container-volume/nifi/nifi-{{loop.index}}/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - ./container-volume/nifi/nifi-{{loop.index}}/content_repository:/opt/nifi/nifi-current/content_repository
      - ./container-volume/nifi/nifi-{{loop.index}}/provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - ./container-volume/nifi/nifi-{{loop.index}}/state:/opt/nifi/nifi-current/state
    {% endif %}
    {% if NIFI_volume_map_logs %}
      - ./container-volume/nifi/nifi-{{loop.index}}/logs:/opt/nifi/nifi-current/logs
    {% endif %}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - aws-credentials-vol:/opt/nifi/nifi-current/.aws:RO
    {% endif %}        
    {%if NIFI_python_provide_requirements_file  | default(false) %}
      - ./custom-conf/nifi/requirements.txt:/opt/nifi/nifi-current/requirements.txt
    {% endif -%}    {#  NIFI_python_provide_requirements_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint:
      # We override the entrypoint from the docker image and therefore have to run ../scripts/start.sh from the NiFi docker image
      - bash
      - -c
      - |
    {% if NIFI_volume_map_custom_config %}
        # this indiretion is needed, to avoid a priviledge error when starting the container
        cat /tmp/nifi.properties.templ > /opt/nifi/nifi-current/conf/nifi.properties
    {% endif -%}
    {%if NIFI_python_provide_requirements_file  | default(false) %}
        pip install -r /opt/nifi/nifi-current/requirements.txt
    {% endif -%}   {#  NIFI_python_provide_requirements_file #}
        ../scripts/start.sh
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {#  NIFI_enable  #}

{%if NIFI2_enable | default(false) %}
  #  ================================== NiFi 2 ========================================== #
  {% for num in range(NIFI2_nodes | default('1') ) %}
    {% set externalPort = 18083 + loop.index-1 %}
    {% set externalRemotePort = 10015 + loop.index-1 %}
    {% set externalPromPort = 1273 + loop.index-1 %}
    {% set clusterPort = 11005 + loop.index-1 %}
  nifi2-{{loop.index}}:
    image: {{NIFI2_custom_image_name if NIFI2_custom_image_name is defined and NIFI2_custom_image_name | length else 'apache/nifi'}}:{{__NIFI2_version}}
    container_name: nifi2-{{loop.index}}
    hostname: nifi2-{{loop.index}}
    labels:
      com.platys.name: 'nifi'
      com.platys.description: "NiFi Data Integration Engine (V2)"
      com.platys.webui.title: "Apache NiFi UI"
      com.platys.webui.url: "http{{ 's' if NIFI2_run_secure}}://dataplatform:{{externalPort}}/nifi"
      com.platys.restapi.title: "Apche NiFi REST API"
      com.platys.restapi.url: "http{{ 's' if NIFI2_run_secure}}://dataplatform:{{externalPort}}/nifi-api"
      com.platys.password.envvars: "PLATYS_NIFI2_PASSWORD"
    ports:
      # HTTP
      - "{{externalPort}}:{{externalPort}}"
      # Remote Input Socket
      - {{externalRemotePort}}:{{externalRemotePort}}/tcp
      # Prometheus Port
      - {{externalPromPort}}:1234
  {%if loop.index == 1 | default(false) %}
    {% for num in range(NIFI2_additional_port_mappings | default(0) ) %}
      {% set external_port = 28520 + loop.index - 1 %}
      {% set webui_port = 28520 + loop.index - 1 %}
      - "{{external_port}}:{{webui_port}}"
    {% endfor %}
  {% endif %}
    environment:
    {%if NIFI2_create_cluster  | default(false) %}
      NIFI_CLUSTER_IS_NODE: 'true'
      NIFI_CLUSTER_ADDRESS: nifi-{{loop.index}}
      NIFI_CLUSTER_NODE_PROTOCOL_PORT: {{clusterPort}}
      NIFI_ZK_CONNECT_STRING: zookeeper-1:2181
      NIFI_ZK_ROOT_NODE: '/nifi'
      NIFI_ELECTION_MAX_WAIT: '{{NIFI2_election_max_wait}}'
    {% endif -%}   {#  NIFI2_create_cluster #}
    {%if NIFI2_run_secure  | default(false) %}
      NIFI_WEB_HTTPS_PORT: '{{externalPort}}'
      NIFI_WEB_HTTPS_HOST: 0.0.0.0
      NIFI_WEB_PROXY_HOST: '${PUBLIC_IP}:{{externalPort}},${DOCKER_HOST_IP}:{{externalPort}}'
      {% if NIFI2_use_custom_certs is defined and NIFI2_use_custom_certs | default(false) %}
      KEYSTORE_PATH: /opt/certs/keystore.jks
      KEYSTORE_TYPE: JKS
      KEYSTORE_PASSWORD: {{NIFI2_keystore_password if NIFI2_keystore_password is defined and NIFI2_keystore_password | length else 'BWt7eLC4R53N4bIxdZEVPhAdkYlQ0Le8Tw0erGXpvI0' }}
      KEY_PASSWORD: {{NIFI2_key_password if NIFI2_key_password is defined and NIFI2_key_password  | length else 'BWt7eLC4R53N4bIxdZEVPhAdkYlQ0Le8Tw0erGXpvI0' }}
      TRUSTSTORE_PATH: /opt/certs/truststore.jks
      TRUSTSTORE_TYPE: JKS
      TRUSTSTORE_PASSWORD: {{NIFI2_truststore_password if NIFI2_truststore_password is defined and NIFI2_truststore_password | length else 'PkhvPAirou5R/K9HQxVk99uT+hehM7SRURyFSbxTlTQ' }}
      {% endif -%}  {#  NIFI2_use_custom_certs #}
      NIFI_SECURITY_USER_AUTHORIZER: single-user-authorizer
      NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER: single-user-provider
    {% else %}
      NIFI_WEB_HTTP_PORT: '{{externalPort}}'
    {% endif -%}   {#  NIFI2_run_unsecure #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      S3_ENDPOINT: '{{s3Endpoint | default(omit) }}'
      S3_PATH_STYLE_ACCESS: '{{s3PathStyleAccess}}'
      S3_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
    {% endif -%}    {#  MINIO_enable #}
      NIFI_REMOTE_INPUT_SOCKET_PORT: '{{externalRemotePort}}'
      NIFI_REMOTE_INPUT_HOST: nifi-{{loop.index}}
      NIFI_JVM_HEAP_INIT: {{ NIFI2_jvm_heap_init if NIFI2_jvm_heap_init is defined and NIFI2_jvm_heap_init != None else omit }}
      NIFI_JVM_HEAP_MAX: {{ NIFI2_jvm_heap_max if NIFI2_jvm_heap_max is defined and NIFI2_jvm_heap_max != None else omit }}
      NIFI_SENSITIVE_PROPS_KEY: '12345678901234567890A'
      SINGLE_USER_CREDENTIALS_USERNAME: '{{NIFI2_username}}'
      SINGLE_USER_CREDENTIALS_PASSWORD: '${PLATYS_NIFI2_PASSWORD:-{{NIFI2_password}}}'
      INITIAL_ADMIN_IDENTITY: '{{ NIFI2_inital_admin_identitiy if NIFI2_inital_admin_identitiy is defined and NIFI2_inital_admin_identitiy | length else NIFI2_username }}'
    {% if VAULT_enable and VAULT_use_dev_mode | default(false) %}
      VAULT_TOKEN: {{VAULT_dev_mode_token}}
    {% endif -%}   {#  VAULT_use_dev_mode #}
      # these config settings are custom ones and treated by the pre-start.sh script before the standard start.sh script of the NiFi docker image
      NIFI_FLOW_CONFIGURATION_FILE: {{NIFI2_flow_configuration_file if NIFI2_flow_configuration_file is defined and NIFI2_flow_configuration_file and NIFI2_flow_configuration_file | length else omit}}
      NIFI_FLOW_CONFIGURATION_ARCHIVE_DIR:  {{NIFI2_flow_configuration_archive_dir if NIFI2_flow_configuration_archive_dir is defined and NIFI2_flow_configuration_archive_dir and NIFI2_flow_configuration_archive_dir | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    {%if external['S3_enable'] | default(false) %}
    env_file: .env
    {% endif -%}   {#  external S3 #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/nifi2/pre-start.sh:/tmp/pre-start.sh
    {%if NIFI2_volume_map_addl_jars  | default(false) %}
      - ./plugins/nifi2/addl-jars/:/opt/nifi/nifi-current/addl-jars/
    {% endif -%}   {#  NIFI2_volume_map_addl_jars #}
      - ./plugins/nifi2/nar_extensions/:/opt/nifi/nifi-current/nar_extensions/
      - ./plugins/nifi2/python_extensions/:/opt/nifi/nifi-current/python_extensions/
    {%if NIFI2_run_secure  | default(false) %}
      {% if NIFI2_use_custom_certs is defined and NIFI2_use_custom_certs | default(false) %}
      - ./custom-conf/nifi2/security/nifi-{{loop.index}}/keystore.jks:/opt/certs/keystore.jks
      - ./custom-conf/nifi2/security/nifi-{{loop.index}}/truststore.jks:/opt/certs/truststore.jks
      {% endif -%}  {#  NIFI2_use_custom_certs #}
    {% endif -%}   {#  NIFI2_run_unsecure #}
    {% if NIFI2_volume_map_custom_config %}
      - ./custom-conf/nifi2/config/nifi.properties:/tmp/nifi.properties.custom
    {% endif %}
    {% if NIFI2_volume_map_data %}
      - ./container-volume/nifi2/nifi2-{{loop.index}}/database_repository:/opt/nifi/nifi-current/database_repository
      - ./container-volume/nifi2/nifi2-{{loop.index}}/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - ./container-volume/nifi2/nifi2-{{loop.index}}/content_repository:/opt/nifi/nifi-current/content_repository
      - ./container-volume/nifi2/nifi2-{{loop.index}}/provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - ./container-volume/nifi2/nifi2-{{loop.index}}/state:/opt/nifi/nifi-current/state
    {% endif %}
    {% if NIFI2_volume_map_logs %}
      - ./container-volume/nifi2/nifi2-{{loop.index}}/logs:/opt/nifi/nifi-current/logs
    {% endif %}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - aws-credentials-vol:/opt/nifi/nifi-current/.aws:RO
    {% endif %}
      - nifi-config-vol:/opt/nifi/nifi-current/conf
    {%if NIFI2_python_provide_requirements_file  | default(false) %}
      - ./custom-conf/nifi2/requirements.txt:/opt/nifi/nifi-current/requirements.txt
    {% endif -%}    {#  NIFI2_python_provide_requirements_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint:
      # We override the entrypoint from the docker image and therefore have to run ../scripts/start.sh from the NiFi docker image
      - bash
      - -c
      - |
    {% if NIFI2_volume_map_custom_config %}
        # this indiretion is needed, to avoid a priviledge error when starting the container
        cat /tmp/nifi.properties.custom > /opt/nifi/nifi-current/conf/nifi.properties
    {% endif -%}
    {%if NIFI2_python_provide_requirements_file  | default(false) %}
        pip install -r /opt/nifi/nifi-current/requirements.txt
    {% endif -%}   {#  NIFI2_python_provide_requirements_file #}
        /tmp/pre-start.sh        
        ../scripts/start.sh
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {#  NIFI2_enable  #}

{%if MONITOFI_enable | default(false) %}
  #  ================================== MonitoFi ========================================== #
  monitofi:
    image: dtushar/monitofi:{{__MONITOFI_version}}
    container_name: monitofi
    hostname: monitofi
    labels:
      com.platys.name: 'monitofi'
      com.platys.description: "NiFi Monitoring"
    depends_on:
      - influxdb
      - grafana
    environment:
      INFLUXDB_SERVER: influxdb
      INFLUXDB_PORT: 8086
      INFLUXDB_USERNAME: root
      INFLUXDB_PASSWORD: root
      INFLUXDB_DATABASE: nifi
      ENDPOINT_LIST: {{'controller/cluster,' if NIFI_create_cluster | default(false)}}flow/cluster/summary,flow/process-groups/root,flow/status,counters,system-diagnostics
      SLEEP_INTERVAL: {{MONITOFI_sleep_interval}}
      API_URL: 'http{{ 's' if NIFI_run_secure}}://nifi-1:18080/nifi-api/'
    {%if NIFI_run_secure  | default(false) %}
      SECURE: true
      CERT_FILE: /opt/monitofi/cert.pkcs12
      CERT_PASS: "abc123!"
    {% endif -%}   {#  NIFI_run_unsecure #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if NIFI_run_secure  | default(false) %}
      - ./security/nifi/keystore.pkcs12:/opt/monitofi/cert.pkcs12
    {% endif -%}   {#  NIFI_run_unsecure #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MONITOFI_enable  #}

{%if NIFI_REGISTRY_enable | default(false) %}
  #  ================================== NiFi Registry ========================================== #
  nifi-registry:
    image: apache/nifi-registry:{{__NIFI_REGISTRY_version}}
    container_name: nifi-registry
    hostname: nifi-registry
    labels:
      com.platys.name: 'nifi-registry'
      com.platys.description: "NiFi Data Flow Registry"
      com.platys.webui.title: "NiFi Registry UI"
      com.platys.webui.url: "http://dataplatform:19090/nifi-registry"
      com.platys.restapi.title: "NiFi Registry REST API"
      com.platys.restapi.url: "http://dataplatform:19090/nifi-registry-api/about"
    ports:
      - 19090:19090
    user: "root"
    environment:
    {%if NIFI_REGISTRY_run_secure  | default(false) %}
      NIFI_REGISTRY_WEB_HTTPS_PORT: 19090
      NIFI_REGISTRY_WEB_HTTPS_HOST: 0.0.0.0
      NIFI_REGISTRY_WEB_PROXY_HOST: '${PUBLIC_IP}:19090,${DOCKER_HOST_IP}:19090'
      AUTH: tls
      KEYSTORE_PATH: /opt/certs/keystore.jks
      KEYSTORE_TYPE: JKS
      KEYSTORE_PASSWORD: {{NIFI_keystore_password if NIFI_keystore_password is defined and NIFI_keystore_password | length else 'BWt7eLC4R53N4bIxdZEVPhAdkYlQ0Le8Tw0erGXpvI0' }}
      KEY_PASSWORD: {{NIFI_key_password if NIFI_key_password is defined and NIFI_key_password | length else omit }}
      TRUSTSTORE_PATH: /opt/certs/truststore.jks
      TRUSTSTORE_TYPE: JKS
      TRUSTSTORE_PASSWORD: {{NIFI_truststore_password if NIFI_truststore_password is defined and NIFI_truststore_password | length else 'PkhvPAirou5R/K9HQxVk99uT+hehM7SRURyFSbxTlTQ' }}
      INITIAL_ADMIN_IDENTITY: '{{ NIFI_inital_admin_identitiy if NIFI_inital_admin_identitiy is defined and NIFI_inital_admin_identitiy | length else NIFI_username }}'
    {% else %}
      NIFI_REGISTRY_WEB_HTTP_PORT: 19090
    {% endif -%}   {#  NIFI_run_unsecure #}
      NIFI_REGISTRY_DB_URL: ''
      NIFI_REGISTRY_DB_CLASS: 'org.h2.Driver'
      NIFI_REGISTRY_DB_DIR: '/opt/nifi-registry/nifi-registry-current/database'
      NIFI_REGISTRY_DB_USER: 'nifireg'
      NIFI_REGISTRY_DB_PASS: 'nifireg'
      NIFI_REGISTRY_FLOW_PROVIDER: {{NIFI_REGISTRY_flow_provider | lower}}
    {% if NIFI_REGISTRY_flow_provider == 'file' | default(false) %}
      NIFI_REGISTRY_FLOW_STORAGE_DIR: '/opt/nifi-registry/nifi-registry-current/file-flow_storage'
    {% elif NIFI_REGISTRY_flow_provider == 'git' | default(false) %}
      NIFI_REGISTRY_FLOW_STORAGE_DIR: '/opt/nifi-registry/nifi-registry-current/git_flow_storage'
      NIFI_REGISTRY_GIT_REMOTE: {{NIFI_REGISTRY_git_remote if NIFI_REGISTRY_git_remote is defined and NIFI_REGISTRY_git_remote else omit}}
      {% if NIFI_REGISTRY_git_user is defined and NIFI_REGISTRY_git_user and NIFI_REGISTRY_git_user | length | default(false) %}
      NIFI_REGISTRY_GIT_USER: {{NIFI_REGISTRY_git_user}}
      NIFI_REGISTRY_GIT_PASSWORD: ${PLATYS_NIFI_REGISTRY_GIT_PASSWORD:-{{NIFI_REGISTRY_git_password}}}
      {% endif -%}   {#  NIFI_REGISTRY_git_user #}
      NIFI_REGISTRY_GIT_REPO: {{NIFI_REGISTRY_git_repo if NIFI_REGISTRY_git_repo is defined and NIFI_REGISTRY_git_repo else omit}}
    {% endif -%}   {#  NIFI_REGISTRY_flow_provider #}
      NIFI_REGISTRY_BUNDLE_PROVIDER: {{NIFI_REGISTRY_bundle_provider | lower}}
    {% if NIFI_REGISTRY_bundle_provider == 'file' | default(false) %}
      NIFI_REGISTRY_BUNDLE_STORAGE_DIR: './extension_bundles'
    {% elif NIFI_REGISTRY_bundle_provider == 's3' | default(false) %}
      NIFI_REGISTRY_S3_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      NIFI_REGISTRY_S3_BUCKET_NAME: {{NIFI_REGISTRY_s3_bucket_name}}
      NIFI_REGISTRY_S3_KEY_PREFIX: {{NIFI_REGISTRY_s3_key_prefix}}
      NIFI_REGISTRY_S3_CREDENTIALS_PROVIDER: STATIC
      NIFI_REGISTRY_S3_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      NIFI_REGISTRY_S3_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
      NIFI_REGISTRY_S3_ENDPOINT_URL: '{{s3Endpoint | default(omit) }}'
    {% endif -%}   {#  NIFI_REGISTRY_bundle_provider #}
      LOG_LEVEL: {{NIFI_REGISTRY_log_level | upper | default('INFO')}}
      NIFI_REGISTRY_LOGS_DIR: '/opt/nifi-registry/nifi-registry-current/logs'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/nifi-registry/logback.xml.templ:/opt/nifi-registry/nifi-registry-current/conf/logback.xml
    {%if NIFI_REGISTRY_run_secure  | default(false) %}
      - ./{{'custom-conf/nifi/security' if NIFI_REGISTRY_use_custom_certs is defined and NIFI_use_custom_certs else 'security/nifi'}}/keystore.jks:/opt/certs/keystore.jks
      - ./{{'custom-conf/nifi/security' if NIFI_REGISTRY_use_custom_certs is defined and NIFI_use_custom_certs else 'security/nifi'}}/truststore.jks:/opt/certs/truststore.jks
    {% endif -%}   {#  NIFI_REGISTRY_run_secure #}
    {% if NIFI_REGISTRY_volume_map_data %}
      - ./container-volume/nifi_registry/database:/opt/nifi-registry/nifi-registry-current/database
    {% endif -%}   {#  NIFI_REGISTRY_volume_map_data #}
    {% if NIFI_REGISTRY_volume_map_logs %}
      - ./container-volume/nifi_registry/logs:/opt/nifi-registry/nifi-registry-current/logs
    {% endif -%}   {#  NIFI_REGISTRY_volume_map_logs #}
    {% if NIFI_REGISTRY_volume_map_flow_storage %}
      {% if NIFI_REGISTRY_flow_provider == 'file' | default(false) %}
      - {{NIFI_REGISTRY_flow_storage_folder_on_dockerhost}}:/opt/nifi-registry/nifi-registry-current/file_flow_storage
      {% elif NIFI_REGISTRY_flow_provider == 'git' | default(false) %}
      - {{NIFI_REGISTRY_flow_storage_folder_on_dockerhost}}:/opt/nifi-registry/nifi-registry-current/git_flow_storage
        {% if NIFI_REGISTRY_git_use_ssh_auth | default(false) %}
      - ./security/nifi-registry/git/ssh:/home/nifi/.ssh
        {% endif -%}   {#  NIFI_REGISTRY_git_use_ssh_auth #}
      {% endif -%}   {#  NIFI_REGISTRY_flow_provider #}
    {% endif -%}   {#  NIFI_REGISTRY_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint:
      # We override the entrypoint from the docker image and therefore have to run ../scripts/start.sh from the NiFi Registry docker image
      - bash
      - -c
      - |
        printenv | sed 's|=.*||' | while read envvarname; do sed -i "s|\$${$$envvarname}|$${!envvarname}|" /opt/nifi-registry/nifi-registry-current/conf/logback.xml; done
        ../scripts/start.sh
    restart: {{container_restart_policy}}
{% endif %}   {#  NIFI_REGISTRY_enable  #}

{%if NIFI_TOOLKIT_enable | default(false) %}
  #  ================================== NiFi Toolkit ========================================== #
  nifi-toolkit:
    image: apache/nifi-toolkit:{{__NIFI_TOOLKIT_version}}
    container_name: nifi-toolkit
    hostname: nifi-toolkit
    labels:
      com.platys.name: 'nifi-toolkit'
      com.platys.description: "NiFi Toolkit"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    restart: {{container_restart_policy}}
{% endif %}   {#  NIFI_TOOLKIT_enable  #}

{%if N8N_enable | default(false) %}
  #  ================================== n8n ========================================== #
  n8n:
    image: docker.n8n.io/n8nio/n8n:{{__N8N_version}}
    container_name: n8n
    hostname: n8n
    labels:
      com.platys.name: "n8n"
      com.platys.description: "Workflow automation tool"
      com.platys.webui.title: "Workflow automation tool"
      com.platys.webui.url: "http://dataplatform:5678"
    ports:
      - 5678:5678
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    environment:
      N8N_USER_FOLDER: '/home/node/.n8n'
      N8N_SECURE_COOKIE: false
      N8N_PREVIEW_MODE: false
      N8N_METRICS: false
      N8N_DIAGNOSTICS_ENABLED: false
      N8N_HIRING_BANNER_ENABLED: false
      N8N_PUBLIC_API_SWAGGERUI_DISABLED: true
      N8N_VERSION_NOTIFICATIONS_ENABLED: false
      N8N_TEMPLATES_ENABLED: false
      N8N_PERSONALIZATION_ENABLED: false
      N8N_AI_ENABLED: {{true if N8N_ai_enabled is defined and N8N_ai_enabled else omit}}
      N8N_RUNNERS_ENABLED: true
      N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE: true
      WEBHOOK_URL: {{N8N_webhook_url if N8N_webhook_url is defined and N8N_webhook_url and N8N_webhook_url | length else omit }}
      OLLAMA_HOST: {{ns.ollamaUrl}}
      EXECUTIONS_MODE: {{N8N_execution_mode | default('regular') }}
    {% if (N8N_execution_mode | lower) == 'queue' %}
      QUEUE_BULL_REDIS_HOST: n8n-redis
      QUEUE_HEALTH_CHECK_ACTIVE: true
      N8N_ENCRYPTION_KEY: {{N8N_encryption_key}}
    {% endif -%}   {#  N8N_execution_mode #}
      DB_TYPE: '{{N8N_db_type | lower}}'
    {% if (N8N_db_type | lower) == 'sqlite' %}
      DB_SQLITE_VACUUM_ON_STARTUP: {{true if N8N_sqlite_vacum_on_startup_enabled is defined and N8N_sqlite_vacum_on_startup_enabled else omit}}
    {% elif (N8N_db_type | lower) == 'postgresdb' %}
      DB_POSTGRESDB_DATABASE: {{N8N_postgres_dbname}}
      DB_POSTGRESDB_HOST: postgresql
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_USER: {{N8N_postgres_user}}
      DB_POSTGRESDB_PASSWORD: {{N8N_postgres_password}}
      DB_POSTGRESDB_SCHEMA: {{N8N_postgres_schema}}
    {% elif (N8N_db_type | lower) == 'mysqldb' %}
      DB_MYSQLDB_DATABASE: {{N8N_mysql_dbname}}
      DB_MYSQLDB_HOST: mysql
      DB_MYSQLDB_PORT: 3306
      DB_MYSQLDB_USER: {{N8N_mysql_user}}
      DB_MYSQLDB_PASSWORD: {{N8N_mysql_password}}
    {% endif -%}   {#  N8N_db_type #}
      N8N_AVAILABLE_BINARY_DATA_MODES: {{N8N_available_binary_data_modes}}
      N8N_DEFAULT_BINARY_DATA_MODE: {{N8N_default_binary_data_mode}}
      N8N_BINARY_DATA_STORAGE_PATH: '/home/node/.n8n/binaryData'
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      N8N_EXTERNAL_STORAGE_S3_HOST: {{s3Endpoint | default(omit) }}
      N8N_EXTERNAL_STORAGE_S3_BUCKET_NAME: {{N8N_s3_bucket_name}}
      N8N_EXTERNAL_STORAGE_S3_BUCKET_REGION: {{s3DefaultRegion}}
      N8N_EXTERNAL_STORAGE_S3_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      N8N_EXTERNAL_STORAGE_S3_ACCESS_SECRET: {{s3SecretAccessKey}}
    {% endif -%}    
    {%if N8N_langsmith_enabled and external['LANGSMITH_enable'] | default(false) %}
      LANGCHAIN_ENDPOINT: https://api.smith.langchain.com
      LANGCHAIN_TRACING_V2: true
      LANGCHAIN_API_KEY: {{external['LANGSMITH_api_key'] if external['LANGSMITH_api_key'] is defined and external['LANGSMITH_api_key'] else omit}}
    {% endif -%}   {#  N8N_langsmith_enabled #}
    {%if use_timezone | default(false) %}
      GENERIC_TIMEZONE: {{use_timezone}}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if N8N_volume_map_data %}
      - ./container-volume/n8n/data:/home/node/.n8n
    {% endif -%}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    {% if (N8N_db_type | lower) == 'postgres' or (N8N_db_type | lower) == 'mysql' %}
    links:
      - {{'postgresql' if (N8N_db_type | lower) == 'postgresdb' else omit }}
      - {{'mysql' if (N8N_db_type | lower) == 'mysqldb' else omit }}    
    {% endif -%}   {#  N8N_db_type #}
    restart: {{container_restart_policy}}

  {% if (N8N_execution_mode | lower) == 'queue' %}
  {% for num in range(N8N_workers | default(0) ) %}
  n8n-worker-{{loop.index}}:
    image: docker.n8n.io/n8nio/n8n:{{__N8N_version}}
    container_name: n8n-worker-{{loop.index}}
    hostname: n8n-worker-{{loop.index}}
    labels:
      com.platys.name: "n8n-worker-{{loop.index}}"
      com.platys.description: "Workflow automation tool (worker-{{loop.index}})"
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    environment:
      N8N_USER_FOLDER: '/home/node/.n8n'
      N8N_SECURE_COOKIE: false
      N8N_PREVIEW_MODE: false
      N8N_METRICS: false
      N8N_DIAGNOSTICS_ENABLED: false
      N8N_HIRING_BANNER_ENABLED: false
      N8N_PUBLIC_API_SWAGGERUI_DISABLED: true
      N8N_VERSION_NOTIFICATIONS_ENABLED: false
      N8N_TEMPLATES_ENABLED: false
      N8N_PERSONALIZATION_ENABLED: false
      N8N_AI_ENABLED: {{true if N8N_ai_enabled is defined and N8N_ai_enabled else omit}}
      OLLAMA_HOST: {{ns.ollamaUrl}}
      EXECUTIONS_MODE: {{N8N_execution_mode | default('regular') }}
    {% if (N8N_execution_mode | lower) == 'queue' %}
      QUEUE_BULL_REDIS_HOST: n8n-redis
      QUEUE_HEALTH_CHECK_ACTIVE: true
      N8N_ENCRYPTION_KEY: {{N8N_encryption_key}}
    {% endif -%}   {#  N8N_execution_mode #}
      DB_TYPE: '{{N8N_db_type | lower}}'
    {% if (N8N_db_type | lower) == 'sqlite' %}
      DB_SQLITE_VACUUM_ON_STARTUP: {{true if N8N_sqlite_vacum_on_startup_enabled is defined and N8N_sqlite_vacum_on_startup_enabled else omit}}
    {% elif (N8N_db_type | lower) == 'postgresdb' %}
      DB_POSTGRESDB_DATABASE: {{N8N_postgres_dbname}}
      DB_POSTGRESDB_HOST: postgresql
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_USER: {{N8N_postgres_user}}
      DB_POSTGRESDB_PASSWORD: {{N8N_postgres_password}}
      DB_POSTGRESDB_SCHEMA: {{N8N_postgres_schema}}
    {% elif (N8N_db_type | lower) == 'mysqldb' %}
      DB_MYSQLDB_DATABASE: {{N8N_mysql_dbname}}
      DB_MYSQLDB_HOST: mysql
      DB_MYSQLDB_PORT: 3306
      DB_MYSQLDB_USER: {{N8N_mysql_user}}
      DB_MYSQLDB_PASSWORD: {{N8N_mysql_password}}
    {% endif -%}   {#  N8N_db_type #}
      N8N_AVAILABLE_BINARY_DATA_MODES: {{N8N_available_binary_data_modes}}
      N8N_DEFAULT_BINARY_DATA_MODE: {{N8N_default_binary_data_mode}}
      N8N_BINARY_DATA_STORAGE_PATH: '/home/node/.n8n/binaryData'
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      N8N_EXTERNAL_STORAGE_S3_HOST: {{s3Endpoint | default(omit) }}
      N8N_EXTERNAL_STORAGE_S3_BUCKET_NAME: {{N8N_s3_bucket_name}}
      N8N_EXTERNAL_STORAGE_S3_BUCKET_REGION: {{s3DefaultRegion}}
      N8N_EXTERNAL_STORAGE_S3_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      N8N_EXTERNAL_STORAGE_S3_ACCESS_SECRET: {{s3SecretAccessKey}}
    {% endif -%} 
    {%if N8N_langsmith_enabled and external['LANGSMITH_enable'] | default(false) %}
      LANGCHAIN_ENDPOINT: https://api.smith.langchain.com
      LANGCHAIN_TRACING_V2: true
      LANGCHAIN_API_KEY: {{LANGSMITH_api_key}}
    {% endif -%}   {#  N8N_langsmith_enabled #}          
    {%if use_timezone | default(false) %}
      GENERIC_TIMEZONE: {{use_timezone}}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if N8N_volume_map_data %}
      - ./container-volume/n8n/data:/home/node/.n8n
    {% endif -%}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    {% if (N8N_db_type | lower) == 'postgres' or (N8N_db_type | lower) == 'mysql' %}
    links:
      - {{'postgresql' if (N8N_db_type | lower) == 'postgresdb' else omit }}
      - {{'mysql' if (N8N_db_type | lower) == 'mysqldb' else omit }}    
    {% endif -%}   {#  N8N_db_type #}
    command: worker
    restart: {{container_restart_policy}}
  {% endfor %}  

  n8n-redis:
    image: redis:6-alpine
    container_name: n8n-redis
    hostname: n8n-redis
    labels:
      com.platys.name: redis
      com.platys.description: "Key-value store"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}          
    volumes:
      - ./data-transfer:/data-transfer
    restart: {{container_restart_policy}}
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 5s
      timeout: 5s
      retries: 10    
  {% endif -%}   {#  N8N_execution_mode #}
{% endif %}   {#  N8N_enable  #}

{%if STREAMPIPES_enable | default(false) %}
  #  ================================== Apache StreamPipes ========================================== #
  backend:
    image: apachestreampipes/backend:{{__STREAMPIPES_version}}
    container_name: backend
    hostname: backend
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine"
    depends_on:
      - streampipes-consul
      - couchdb
    environment:
      CONSUL_LOCATION: streampipes-consul
      SP_BACKEND_HOST: backend
      SP_CONNECT_CONTAINER_WORKER_HOST: streampipes-connect-worker
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/streampipes:/root/.streampipes
      - ./container-volume/streampipes/files:/spImages
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  streampipes-connect-worker:
    image: apachestreampipes/connect-worker:{{__STREAMPIPES_version}}
    container_name: streampipes-connect-worker
    hostname: streampipes-connect-worker
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine"
    depends_on:
      - streampipes-consul
      - couchdb
    environment:
      SP_BACKEND_HOST: backend
      SP_CONNECT_CONTAINER_WORKER_HOST: streampipes-connect-worker
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - connect:/data/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  streampipes-ui:
    image: apachestreampipes/ui:{{__STREAMPIPES_version}}
    container_name: streampipes-ui
    hostname: streampipes-ui
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine"
      com.platys.webui.title: "StreamPipes Dashboard"
      com.platys.webui.url: "http://dataplatform:28255"
    ports:
      - "28255:80"
    depends_on:
      - couchdb
      - activemq
      - streampipes-consul
      - backend
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - connect:/data/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  streampipes-activemq:
    image: fogsyio/activemq:5.15.9
    container_name: streampipes-activemq
    hostname: streampipes-activemq
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine ActiveMQ Instance"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - connect:/data/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  streampipes-consul:
    image: fogsyio/consul:1.7.1
    container_name: streampipes-consul
    hostname: streampipes-consul
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine Consul Instance"
    environment:
      - "CONSUL_LOCAL_CONFIG={\"disable_update_check\": true}"
      - "CONSUL_BIND_INTERFACE=eth0"
      - "CONSUL_HTTP_ADDR=0.0.0.0"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - consul:/consul/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  streampipes-couchdb:
    image: fogsyio/couchdb:2.3.1
    container_name: streampipes-couchdb
    hostname: streampipes-couchdb
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine CouchDB Instance"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - couchdb:/opt/couchdb/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  streampipes-pipeline-elements-all-jvm:
    image: apachestreampipes/pipeline-elements-all-jvm:{{__STREAMPIPES_version}}
    container_name: streampipes-pipeline-elements-all-jvm
    hostname: streampipes-pipeline-elements-all-jvm
    labels:
      com.platys.name: 'streampipes'
      com.platys.description: "StreamPipes Data Integration Engine"
    depends_on:
      - streampipes-consul
    environment:
      CONSUL_LOCATION: streampipes-consul
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - files:/spImages
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STREAMPIPES_enable #}

{%if CRIBL_STREAM_enable | default(false) %}
  #  ================================== Cribl Stream ========================================== #
  cribl-master:
    image: cribl/cribl:{{__CRIBL_version}}
    container_name: cribl-master
    hostname: cribl-master
    labels:
      com.platys.name: 'cribl-stream'
      com.platys.description: "Cribl Data Integration Engine"
      com.platys.webui.title: "Cribl Stream UI"
      com.platys.webui.url: "http://dataplatform:28329"
    ports:
      - "28329:9000"
      - "4200:4200"
    environment:
      CRIBL_DIST_MODE: master
      CRIBL_DIST_MASTER_URL: tcp://2fQ9GRc3n7jhWBmpOfgCi6664yGZHuFB@0.0.0.0:4200
      CRIBL_VOLUME_DIR: /opt/cribl/config-volume
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if CRIBL_STREAM_volume_map_data %}
      - ./container-volume/cribl:/opt/cribl/config-volume
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% for num in range(CRIBL_STREAM_workers | default(1) ) %}
  cribl-worker-{{loop.index}}:
    image: cribl/cribl:{{__CRIBL_version}}
    container_name: cribl-worker-{{loop.index}}
    hostname: cribl-worker-{{loop.index}}
    labels:
      com.platys.name: 'cribl'
      com.platys.description: "Cribl Data Integration Engine"
    depends_on:
      - cribl-master
    ports:
      - 9000
      - 8000-8020     # use to expose ports for push sources
    environment:
      CRIBL_DIST_MODE: worker
      CRIBL_DIST_MASTER_URL: tcp://2fQ9GRc3n7jhWBmpOfgCi6664yGZHuFB@${DOCKER_HOST_IP}:4200?group=default
      CRIBL_MAX_WORKERS: 2
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {#  CRIBL_STREAM_enable #}

{%if CRIBL_EDGE_enable | default(false) %}
  #  ================================== Cribl Edge ========================================== #
  {% for num in range(CRIBL_EDGE_nodes | default(1) ) %}
  cribl-edge-{{loop.index}}:
    image: cribl/cribl:{{__CRIBL_version}}
    container_name: cribl-edge-{{loop.index}}
    hostname: cribl-edge-{{loop.index}}
    labels:
      com.platys.name: 'cribl-edge'
      com.platys.description: "Cribl Edge Data Integration Engine"
      com.platys.webui.title: "Cribl Edge UI"
      com.platys.webui.url: "http://dataplatform:28330"
    depends_on:
      - cribl-master
    ports:
      - 28330:9420
      - 8000-8020     # use to expose ports for push sources
    environment:
    {%if CRIBL_EDGE_managed | default(false) %}
      CRIBL_DIST_MODE: managed-edge
      CRIBL_DIST_MASTER_URL: tcp://2fQ9GRc3n7jhWBmpOfgCi6664yGZHuFB@${DOCKER_HOST_IP}:4200?group={{CRIBL_EDGE_fleet | default(default_fleet)}}
    {% endif -%}   {#  CRIBL_EDGE_managed #}
      CRIBL_EDGE: 1
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/appscope:/var/run/appscope
      - /var/run/docker.sock:/var/run/docker.sock
      - /:/hostfs:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    privileged: true
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {#  CRIBL_EDGE_enable #}

{%if CONDUIT_enable | default(false) %}
  #  ================================== Conduit ========================================== #
  conduit:
    image: ghcr.io/conduitio/conduit:{{__CONDUIT_version}}
    container_name: conduit
    hostname: conduit
    labels:
      com.platys.name: 'conduit'
      com.platys.description: "Coduit Data Integration Engine"
      com.platys.webui.title: "Conduit UI"
      com.platys.webui.url: "http://dataplatform:28269"
    ports:
      - "28269:8080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CONDUIT_enable  #}

{%if FLUENTD_enable | default(false) %}
  #  ================================== FluentD ========================================== #
  fluentd:
    image: trivadis/fluentd:{{__FLUENTD_version}}
    container_name: fluentd
    hostname: fluentd
    labels:
      com.platys.name: 'fluentd'
      com.platys.description: "FluentD Data Integration Agent"
    ports:
      - "24224:24224"
    environment:
      #FLUENTD_CONF: /fluentd/etc/{{FLUENTD_conf_file_name if FLUENTD_conf_file_name is defined and FLUENTD_conf_file_name and FLUENTD_conf_file_name | length else omit}}
    {% if ELASTICSEARCH_enable | default(false) %}
      ELASTICSEARCH_HOSTNAME: elasticsearch-1
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
    {% endif -%}   {#  ELASTICSEARCH_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      S3_ENDPOINT: '{{s3Endpoint | default(omit) }}'
      S3_BUCKET_NAME: '{{FLUENTD_s3_bucket_name}}'
      S3_BUCKET_REGION: '{{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}'
      S3_BUCKET_FILE_TYPE: '{{FLUENTD_s3_bucket_file_type}}'
      S3_BUCKET_APP_NAME: 'platys'
      S3_PATH_STYLE_ACCESS: '{{s3PathStyleAccess}}'
      S3_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      S3_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
    {% endif -%}   {#  MINIO_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if FLUENTD_conf_file_name is defined and FLUENTD_conf_file_name and FLUENTD_conf_file_name | length %}
      - ./conf/fluentd/{{FLUENTD_conf_file_name}}:/fluentd/etc/fluent.conf:ro
    {% endif -%}   {#  FLUENTD_conf_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  FLUENTD_enable  #}

{%if FLUENT_BIT_enable | default(false) %}
  #  ================================== Fluent Bit ========================================== #
  fluent-bit:
    image: fluent/fluent-bit:{{__FLUENT_BIT_version}}
    container_name: fluent-bit
    hostname: fluent-bit
    labels:
      com.platys.name: 'fluent-bit'
      com.platys.description: "Fluent Bit Data Integration Agent"
    ports:
      - "24225:24224"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if FLUENT_BIT_volume_map_config | default(false) %}
      - ./custom-conf/fluent-bit/{{FLUENT_BIT_config_filename}}:/fluent-bit/etc/{{FLUENT_BIT_config_filename}}
    {% endif -%}   {#  FLUENT_BIT_volume_map_config #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: ["/fluent-bit/bin/fluent-bit", "-c", "/fluent-bit/etc/{{FLUENT_BIT_config_filename}}"]
    restart: {{container_restart_policy}}
{% endif %}   {#  FLUENT_BIT_enable  #}

{%if FILEBEAT_enable | default(false) %}
  #  ================================== FileBeat ========================================== #
  filebeat:
    image: docker.elastic.co/beats/filebeat:{{__FILEBEAT_version}}
    container_name: filebeat
    hostname: filebeat
    # Need to override user so we can access the log files, and docker.sock
    user: root
    labels:
      com.platys.name: 'filebeat'
      com.platys.description: "Filebeat Data Integration Agent"
    environment:
      strict.perms: false
    {% if ELASTICSEARCH_enable | default(false) %}
      output.elasticsearch.hosts: elasticsearch-1:9200
      ELASTICSEARCH_HOSTS: elasticsearch-1:9200
      ELASTICSEARCH_USERNAME: na
      ELASTICSEARCH_PASSWORD: na
    {% endif -%}   {#  ELASTICSEARCH_enable #}
    {% if KIBANA_enable | default(false) %}
      setup.dashboards.enabled: true
      setup.kibana.host: kibana:5601
      setup.kibana.username: na
      setup.kibana.password: na
    {% endif -%}   {#  KIBANA_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    {% if FILEBEAT_conf_file_name is defined and FILEBEAT_conf_file_name and FILEBEAT_conf_file_name | length %}
      - ./conf/filebeat/{{FILEBEAT_conf_file_name}}:/usr/share/filebeat/filebeat.yml:ro
    {% endif -%}   {#  FLUENTD_conf_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  FILEBEAT_enable  #}

{%if NODERED_enable | default(false) %}
  #  ================================== Node-RED ========================================== #
  nodered-1:
    image: nodered/node-red:{{__NODERED_version}}
    container_name: nodered-1
    hostname: nodered-1
    labels:
      com.platys.name: 'nodered'
      com.platys.description: "Node-RED Data Integration Engine"
      com.platys.webui.title: "Node-RED UI"
      com.platys.webui.url: "http://dataplatform:1880"
      com.platys.restapi.title: "Node-RED REST API"
      com.platys.restapi.url: "http://dataplatform:1880"
    ports:
      - "1880:1880"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if NODERED_volume_map_data %}
      - ./container-volume/nodered:/data:Z
    {% endif %}
      - /dev/i2c-1:/dev/i2c-1
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  NODERED_enable  #}

{% if (STREAMSHEETS_enable) | default(false) %}
  #  ================================== Streamsheets ========================================== #
  streamsheets:
    image: cedalo/streamsheets:{{__STREAMSHEETS_version}}
    container_name: streamsheets
    hostname: streamsheets
    labels:
      com.platys.name: 'streamsheets'
      com.platys.description: "Streamsheets Data Integration Engine"
      com.platys.webui.title: 'Streamsheets UI'
      com.platys.webui.url: "http://dataplatform:28158"
    ports:
      - 28158:8081
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  STREAMSHEETS_enable #}

{%if SPRING_DATAFLOW_enable | default(false) %}
  #  ================================== Spring Cloud Data Flow ========================================== #
  spring-dataflow-server:
    image: springcloud/spring-cloud-dataflow-server:{{__SPRING_DATAFLOW_version}}
    container_name: spring-dataflow-server
    hostname: spring-dataflow-server
    labels:
      com.platys.name: 'spring-dataflow'
      com.platys.description: "Spring DataFlow Data Integration Engine"
      com.platys.webui.title: "Spring Cloud Data Flow Server Dashboard"
      com.platys.webui.url: "http://dataplatform:9393/dashboard"
      com.platys.restapi.title: "Spring Cloud Data Flow Server REST API"
      com.platys.restapi.url: "http://dataplatform:9393"
    ports:
      - "9393:9393"
    environment:
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=PLAINTEXT://{{ns.bootstrapServer}}
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_STREAMS_BINDER_BROKERS=PLAINTEXT://{{ns.bootstrapServer}}
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_BINDER_ZKNODES=zookeeper-1:2181
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_STREAMS_BINDER_ZKNODES=zookeeper-1:2181

      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_KAFKA_STREAMS_PROPERTIES_METRICS_RECORDING_LEVEL=DEBUG

      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_TASK_SPRING_CLOUD_TASK_CLOSECONTEXTENABLED=true

      - SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI=http://spring-skipper-server:7577/api

      - SPRING_DATASOURCE_URL=jdbc:mysql://spring-dataflow-mysql:3306/dataflow
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=rootpw
      - SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.mariadb.jdbc.Driver
      # (Optionally) authenticate the default Docker Hub access for the App Metadata access.
      #- SPRING_CLOUD_DATAFLOW_CONTAINER_REGISTRY_CONFIGURATIONS_DEFAULT_USER=${METADATA_DEFAULT_DOCKERHUB_USER}
      #- SPRING_CLOUD_DATAFLOW_CONTAINER_REGISTRY_CONFIGURATIONS_DEFAULT_SECRET=${METADATA_DEFAULT_DOCKERHUB_PASSWORD}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    entrypoint: "/usr/src/app/wait-for-it.sh -t 180 spring-skipper-server:7577 -- /cnb/process/web"
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
#      - ${HOST_MOUNT_PATH:-.}:${DOCKER_MOUNT_PATH:-/root/scdf}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  spring-skipper-server:
    image: springcloud/spring-cloud-skipper-server:{{__SPRING_SKIPPER_version}}
    container_name: spring-skipper-server
    hostname: spring-skipper-server
    labels:
      com.platys.name: 'spring-dataflow'
      com.platys.description: "Spring DataFlow Data Integration Engine"
      com.platys.restapi.title: "Spring Cloud Skipper Server REST API"
      com.platys.restapi.url: "http://dataplatform:7577/api"
    ports:
      - "7577:7577"
#      - ${APPS_PORT_RANGE:-20000-20105:20000-20105}
    environment:
      - SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_LOCAL_ACCOUNTS_DEFAULT_PORTRANGE_LOW=20000
      - SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_LOCAL_ACCOUNTS_DEFAULT_PORTRANGE_HIGH=20100
      - SPRING_DATASOURCE_URL=jdbc:mysql://spring-dataflow-mysql:3306/dataflow
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=rootpw
      - SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.mariadb.jdbc.Driver
      - LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_CLOUD_SKIPPER_SERVER_DEPLOYER=ERROR
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    entrypoint: "/usr/src/app/wait-for-it.sh spring-dataflow-mysql:3306 -- /cnb/process/web"
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
#      - ${HOST_MOUNT_PATH:-.}:${DOCKER_MOUNT_PATH:-/root/scdf}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  spring-dataflow-mysql:
    image: mysql:5.7.25
    container_name: spring-dataflow-mysql
    hostname: spring-dataflow-mysql
    labels:
      com.platys.name: 'mysql'
      com.platys.description: "Spring DataFlow Data Integration Engine MySQL Instance"
    environment:
      MYSQL_DATABASE: dataflow
      MYSQL_USER: root
      MYSQL_ROOT_PASSWORD: rootpw
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    expose:
      - 3306
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SPRING_DATAFLOW_enable #}

{% if (ASPIRE_enable) | default(false) %}
  #  ================================== Aspire ========================================== #
  aspire-manager:
    image: docker.repository.sca.accenture.com/docker/aspire:{{__ASPIRE_version}}
    container_name: aspire-manager
    hostname: aspire-manager
    labels:
      com.platys.name: 'aspire-manager'
      com.platys.description: ""
      com.platys.webui.title: 'Streamsheets UI'
      com.platys.webui.url: "http://dataplatform:50443"
    ports:
      - 50443:50443
      - 58000:58000
    stdin_open: true 
    tty: true      
    mem_limit: 3g    
    environment:
      com_accenture_aspire_server_hostname: 'aspire-manager'
      ASPIRE_PORT: 50443
      ASPIRE_MANAGER_NODE: 'true'
      ASPIRE_WORKER_NODE: 'false'
      aspire_dashboards_enabled: 'true'
      aspire_dashboards_base: 'https://kibana:5601/app/dashboards#/view/'

      # ASPIRE_SSL_KEYSTORE_PASS: 123456
      # ASPIRE_SSL_KEYSTORE: /opt/aspire/ssl/certificates/self-signed.jks
      # ASPIRE_SSL_CA: /opt/aspire/ssl/certificates/ca/ca.crt
      ASPIRE_LICENSE_PATH: /config/AspireLicense.lic
      ASPIRE_SETTINGS_PATH: /config/settings.json
      ASPIRE_JWT_SECRET: 12345678901234567890123456789012
      ASPIRE_REMOTE_DEBUG: true    
      aspire_noSql_elastic_server: http://elasticsearch-1:9200
      aspire_noSql_elastic_authentication_basic: true
      aspire_noSql_elastic_user: elastic
      aspire_noSql_elastic_password: ${PLATYS_ELASTICSEARCH_PASSWORD:-abc123!}
      aspire_ldap_bind_dn_password: Adm1n!
      aspire_security_https_only: true         
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./licenses/aspire/AspireLicense.lic:/config/AspireLicense.lic
      - ./conf/aspire/settings.json:/config/settings.json    
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  aspire-worker:
    image: docker.repository.sca.accenture.com/docker/aspire:{{__ASPIRE_version}}
    container_name: aspire-worker
    hostname: aspire-worker
    labels:
      com.platys.name: 'aspire-worker'
      com.platys.description: ""
    ports:
      - 51443:50443
      - 58001:58000
    stdin_open: true 
    tty: true      
    mem_limit: 3g    
    environment:
      com_accenture_aspire_server_hostname: 'aspire-worker'
      ASPIRE_PORT: 50443
      ASPIRE_MANAGER_NODE: 'false'
      ASPIRE_WORKER_NODE: 'true'

      # ASPIRE_SSL_KEYSTORE_PASS: 123456
      # ASPIRE_SSL_KEYSTORE: /opt/aspire/ssl/certificates/self-signed.jks
      # ASPIRE_SSL_CA: /opt/aspire/ssl/certificates/ca/ca.crt
      ASPIRE_LICENSE_PATH: /config/AspireLicense.lic
      ASPIRE_SETTINGS_PATH: /config/settings.json
      ASPIRE_JWT_SECRET: 12345678901234567890123456789012
      ASPIRE_REMOTE_DEBUG: true    
      aspire_noSql_elastic_server: http://elasticsearch-1:9200
      aspire_noSql_elastic_authentication_basic: true
      aspire_noSql_elastic_user: elastic
      aspire_noSql_elastic_password: ${PLATYS_ELASTICSEARCH_PASSWORD:-abc123!}
      aspire_ldap_bind_dn_password: Adm1n!
      aspire_security_https_only: true         
        
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ASPIRE_enable #}

{%if AIRBYTE_enable | default(false) %}
  #  ================================== Airbyte ========================================== #

  airbyte-init:
    image: airbyte/init:{{__AIRBYTE_version}}
    container_name: airbyte-init
    hostname: airbyte-init
    environment:
      - LOCAL_ROOT=/tmp/airbyte_local
      - HACK_LOCAL_ROOT_PARENT=/tmp
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./tmp:/local_parent
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: /bin/sh -c "./scripts/create_mount_directories.sh /local_parent ${HACK_LOCAL_ROOT_PARENT} ${LOCAL_ROOT}"
    init: true

  airbyte-bootloader:
    image: airbyte/bootloader:{{__AIRBYTE_version}}
    container_name: airbyte-bootloader
    hostname: airbyte-bootloader
    environment:
      - AIRBYTE_VERSION={{__AIRBYTE_version}}
      - DATABASE_PASSWORD={{AIRBYTE_database_password}}
      - DATABASE_URL=jdbc:postgresql://{{AIRBYTE_database_host}}:{{AIRBYTE_database_port}}/{{AIRBYTE_database_db}}
      - DATABASE_USER={{AIRBYTE_database_user}}
      - LOG_LEVEL={{AIRBYTE_log_level}}
#      - RUN_DATABASE_MIGRATION_ON_STARTUP=
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    init: true

  airbyte-db:
    image: airbyte/db:{{__AIRBYTE_version}}
    container_name: airbyte-db
    hostname: airbyte-db
    labels:
      com.platys.name: 'airbyte'
      com.platys.description: "Data Integration Platform DB"
    environment:
      - CONFIG_DATABASE_PASSWORD=${CONFIG_DATABASE_PASSWORD:-}
      - CONFIG_DATABASE_URL=${CONFIG_DATABASE_URL:-}
      - CONFIG_DATABASE_USER=${CONFIG_DATABASE_USER:-}
      - DATABASE_PASSWORD={{AIRBYTE_database_password}}
      - DATABASE_URL=jdbc:postgresql://{{AIRBYTE_database_host}}:{{AIRBYTE_database_port}}/{{AIRBYTE_database_db}}
      - DATABASE_USER={{AIRBYTE_database_user}}
      - POSTGRES_PASSWORD={{AIRBYTE_database_password}}
      - POSTGRES_USER={{AIRBYTE_database_user}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if AIRBYTE_volume_map_data %}
      - airbyte-db-vol:/var/lib/postgresql/data
    {% endif -%}   {#  AIRBYTE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airbyte-worker:
    image: airbyte/worker:{{__AIRBYTE_version}}
    container_name: airbyte-worker
    hostname: airbyte-worker
    labels:
      com.platys.name: 'airbyte'
      com.platys.description: "Data Integration Platform Server"
    environment:
      - AIRBYTE_VERSION={{__AIRBYTE_version}}
      - AUTO_DISABLE_FAILING_CONNECTIONS=false
      - CONFIG_DATABASE_PASSWORD=${CONFIG_DATABASE_PASSWORD:-}
      - CONFIG_DATABASE_URL=${CONFIG_DATABASE_URL:-}
      - CONFIG_DATABASE_USER=${CONFIG_DATABASE_USER:-}
      - CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION={{AIRBYTE_configs_database_minimum_flyway_migration_version}}
      - CONFIG_ROOT=/data
      - DATABASE_PASSWORD={{AIRBYTE_database_password}}
      - DATABASE_URL=jdbc:postgresql://{{AIRBYTE_database_host}}:{{AIRBYTE_database_port}}/{{AIRBYTE_database_db}}
      - DATABASE_USER={{AIRBYTE_database_user}}
      - DEPLOYMENT_MODE=
      - INTERNAL_API_HOST=airbyte-server:8001
      - JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION={{AIRBYTE_jobs_database_minimum_flyway_migration_version}}
      - JOB_MAIN_CONTAINER_CPU_LIMIT=''
      - JOB_MAIN_CONTAINER_CPU_REQUEST=''
      - JOB_MAIN_CONTAINER_MEMORY_LIMIT=''
      - JOB_MAIN_CONTAINER_MEMORY_REQUEST=''
      - LOCAL_DOCKER_MOUNT=/tmp/airbyte_local
      - LOCAL_ROOT=/tmp/airbyte_local
      - LOG_LEVEL={{AIRBYTE_log_level}}
      - LOG_CONNECTOR_MESSAGES=
      - MAX_CHECK_WORKERS=5
      - MAX_DISCOVER_WORKERS=5
      - MAX_SPEC_WORKERS=5
      - MAX_SYNC_WORKERS=5
      - MAX_NOTIFY_WORKERS=5
      - SHOULD_RUN_NOTIFY_WORKFLOW=false
      - NORMALIZATION_JOB_MAIN_CONTAINER_MEMORY_LIMIT=
      - NORMALIZATION_JOB_MAIN_CONTAINER_MEMORY_REQUEST=
      - NORMALIZATION_JOB_MAIN_CONTAINER_CPU_LIMIT=
      - NORMALIZATION_JOB_MAIN_CONTAINER_CPU_REQUEST=
#      - SECRET_PERSISTENCE=${SECRET_PERSISTENCE}
      - SYNC_JOB_MAX_ATTEMPTS=3
      - SYNC_JOB_MAX_TIMEOUT_DAYS=3
      - TEMPORAL_HOST=airbyte-temporal:7233
      - TRACKING_STRATEGY=segment
      - WEBAPP_URL=http://${PUBLIC_IP}:28271/
      - WORKER_ENVIRONMENT=docker
      - WORKSPACE_DOCKER_MOUNT=airbyte_workspace
      - WORKSPACE_ROOT=/tmp/workspace
      - METRIC_CLIENT=
      - OTEL_COLLECTOR_ENDPOINT="http://host.docker.internal:4317"
      - JOB_ERROR_REPORTING_STRATEGY=logging
      - JOB_ERROR_REPORTING_SENTRY_DSN=
      - ACTIVITY_MAX_ATTEMPT=
      - ACTIVITY_INITIAL_DELAY_BETWEEN_ATTEMPTS_SECONDS=
      - ACTIVITY_MAX_DELAY_BETWEEN_ATTEMPTS_SECONDS=
      - WORKFLOW_FAILURE_RESTART_DELAY_SECONDS=
      - AUTO_DETECT_SCHEMA=true
      - USE_STREAM_CAPABLE_STATE=true
      - MICRONAUT_ENVIRONMENTS=control-plane
      - APPLY_FIELD_SELECTION=
      - FIELD_SELECTION_WORKSPACES=
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if AIRBYTE_volume_map_data %}
      - ./tmp/workspace:/tmp/workspace
    {% endif -%}   {#  use_timezone #}
      - ./tmp/airbyte_local:/tmp/airbyte_local
      - /var/run/docker.sock:/var/run/docker.sock
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airbyte-server:
    image: airbyte/server:{{__AIRBYTE_version}}
    container_name: airbyte-server
    hostname: airbyte-server
    labels:
      com.platys.name: 'airbyte'
      com.platys.description: "Data Integration Platform Server"
      com.platys.restapi.title: "Airbyte REST API"
      com.platys.restapi.url: "http://dataplatform:28270/api/v1"
    ports:
      - 28270:8001
    environment:
      - AIRBYTE_ROLE=${AIRBYTE_ROLE:-}
      - AIRBYTE_VERSION={{__AIRBYTE_version}}
      - CONFIG_DATABASE_PASSWORD=${CONFIG_DATABASE_PASSWORD:-}
      - CONFIG_DATABASE_URL=${CONFIG_DATABASE_URL:-}
      - CONFIG_DATABASE_USER=${CONFIG_DATABASE_USER:-}
      - CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION={{AIRBYTE_configs_database_minimum_flyway_migration_version}}
      - CONFIG_ROOT=/data
      - DATABASE_PASSWORD={{AIRBYTE_database_password}}
      - DATABASE_URL=jdbc:postgresql://{{AIRBYTE_database_host}}:{{AIRBYTE_database_port}}/{{AIRBYTE_database_db}}
      - DATABASE_USER={{AIRBYTE_database_user}}
      - JOB_MAIN_CONTAINER_CPU_LIMIT=''
      - JOB_MAIN_CONTAINER_CPU_REQUEST=''
      - JOB_MAIN_CONTAINER_MEMORY_LIMIT=''
      - JOB_MAIN_CONTAINER_MEMORY_REQUEST=''
      - JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION={{AIRBYTE_jobs_database_minimum_flyway_migration_version}}
      - LOG_LEVEL={{AIRBYTE_log_level}}
      - NEW_SCHEDULER=
#      - SECRET_PERSISTENCE=${SECRET_PERSISTENCE}
      - TEMPORAL_HOST=airbyte-temporal:7233
      - TRACKING_STRATEGY=segment
      - JOB_ERROR_REPORTING_STRATEGY=logging
      - JOB_ERROR_REPORTING_SENTRY_DSN=
      - WEBAPP_URL=http://${PUBLIC_IP}:28271/
      - WORKER_ENVIRONMENT=
      - WORKSPACE_ROOT=/tmp/workspace
      - GITHUB_STORE_BRANCH=
      - MICRONAUT_ENVIRONMENTS=control-plane
      - AUTO_DETECT_SCHEMA=true
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if AIRBYTE_volume_map_data %}
      - ./tmp/workspace:/tmp/workspace
    {% endif -%}   {#  AIRBYTE_volume_map_data #}
      - ./tmp/data:/data
      - ./tmp/airbyte_local:/tmp/airbyte_local
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airbyte-webapp:
    image: airbyte/webapp:{{__AIRBYTE_version}}
    container_name: airbyte-webapp
    hostname: airbyte-webapp
    labels:
      com.platys.name: 'airbyte'
      com.platys.description: "Data Integration Platform UI"
      com.platys.webui.title: "Airbyte UI"
      com.platys.webui.url: "http://dataplatform:28271"
    ports:
      - 28271:80
    environment:
      - AIRBYTE_ROLE=${AIRBYTE_ROLE:-}
      - AIRBYTE_VERSION={{__AIRBYTE_version}}
      - API_URL=/api/v1/
      - CONNECTOR_BUILDER_API_URL=/connector-builder-api
      - INTERNAL_API_HOST=airbyte-server:8001
      - CONNECTOR_BUILDER_API_HOST=airbyte-connector-builder-server:80
      - OPENREPLAY=${OPENREPLAY:-}
      - PAPERCUPS_STORYTIME=${PAPERCUPS_STORYTIME:-}
      - TRACKING_STRATEGY=segment
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airbyte-temporal:
    image: airbyte/temporal:{{__AIRBYTE_version}}
    container_name: airbyte-temporal
    hostname: airbyte-temporal
    labels:
      com.platys.name: 'airbyte'
      com.platys.description: "Data Integration Platform"
      com.platys.restapi.title: "Airbyte Temporal"
      com.platys.restapi.url: "http://dataplatform:28272"
    ports:
      - 28272:7233
    environment:
      - DB=postgresql
      - DB_PORT={{AIRBYTE_database_port}}
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development.yaml
      - LOG_LEVEL={{AIRBYTE_log_level}}
      - POSTGRES_PWD={{AIRBYTE_database_password}}
      - POSTGRES_SEEDS={{AIRBYTE_database_host}}
      - POSTGRES_USER={{AIRBYTE_database_user}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/airbyte/temporal/dynamicconfig:/etc/temporal/config/dynamicconfig
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airbyte-cron:
    image: airbyte/cron:{{__AIRBYTE_version}}
    container_name: airbyte-cron
    hostname: airbyte-cron
    labels:
      com.platys.name: 'airbyte'
      com.platys.description: "Data Integration Platform Scheduler"
    environment:
      - AIRBYTE_VERSION={{__AIRBYTE_version}}
      - CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION={{AIRBYTE_configs_database_minimum_flyway_migration_version}}
      - DATABASE_PASSWORD={{AIRBYTE_database_password}}
      - DATABASE_URL=jdbc:postgresql://{{AIRBYTE_database_host}}:{{AIRBYTE_database_port}}/{{AIRBYTE_database_db}}
      - DATABASE_USER={{AIRBYTE_database_user}}
      - DEPLOYMENT_MODE=${DEPLOYMENT_MODE}
      - LOG_LEVEL={{AIRBYTE_log_level}}
      - REMOTE_CONNECTOR_CATALOG_URL=
      - TEMPORAL_HISTORY_RETENTION_IN_DAYS=
      - UPDATE_DEFINITIONS_CRON_ENABLED=
      - WORKSPACE_ROOT=/tmp/workspace
      - MICRONAUT_ENVIRONMENTS=control-plane
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if AIRBYTE_volume_map_data %}
      - ./tmp/airbyte-workspace-vol:/tmp/workspace
    {% endif -%}   {#  AIRBYTE_volume_map_data #}
      - ./tmp/airbyte_local:/tmp/airbyte_local
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airbyte-connector-builder-server:
    image: airbyte/connector-builder-server:{{__AIRBYTE_version}}
    container_name: airbyte-connector-builder-server
    hostname: airbyte-connector-builder-server
    labels:
      com.platys.name: 'airbyte'
    ports:
      - 80
    environment:
      - AIRBYTE_VERSION={{__AIRBYTE_version}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  AIRBYTE_enable #}

{%if AIRFLOW_enable | default(false) %}
  {% if (AIRFLOW_executor | lower) == 'celery'  %}
     {% set airflow_executor = "CeleryExecutor" %}
  {% elif (AIRFLOW_executor | lower) == 'sequential' %}
     {% set airflow_executor = "SequentialExecutor" %}
  {% else %}
     {% set airflow_executor = "LocalExecutor" %}
  {% endif -%}
  #  ================================== Airflow ========================================== #
  airflow:
    image: {{AIRFLOW_custom_image_name if AIRFLOW_custom_image_name is defined and AIRFLOW_custom_image_name | length else ' apache/airflow'}}:{{__AIRFLOW_version}}
    container_name: airflow
    hostname: airflow
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Job Orchestration & Scheduler"
      com.platys.webui.title: "Airflow UI"
      com.platys.webui.url: "http://dataplatform:28139"
      com.platys.restapi.title: "Airflow REST API (dags as sample)"
      com.platys.restapi.url: "http://dataplatform:28139/api/v1/dags"
      com.platys.password.envvars: PLATYS_AIRFLOW_SECRET
    ports:
      - "28139:8080"
    command: webserver
  {% if (AIRFLOW_volume_map_docker_daemon) %}
    group_add:
      - "$DOCKER_GROUP"
  {% endif %}
    environment:
      - AIRFLOW__CORE__EXECUTOR={{airflow_executor}}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= '{{AIRFLOW_fernet_key}}'
      - AIRFLOW__WEBSERVER__SECRET_KEY=${PLATYS_AIRFLOW_SECRET:-{{AIRFLOW_secret_key}}}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION={{AIRFLOW_dags_paused_at_creation}}
      - AIRFLOW__CORE__LOAD_EXAMPLES={{AIRFLOW_provision_examples}}
      - AIRFLOW__API__AUTH_BACKENDS={{AIRFLOW_auth_backends | default(omit)}}
      - _PIP_ADDITIONAL_REQUIREMENTS={{AIRFLOW_additional_requirements}}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL={{AIRFLOW_dag_dir_list_interval}}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG={{AIRFLOW_expose_config}}
    {%if SPARK_enable | default(false) %}
      - SPARK_HOME=/opt/spark
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - AIRFLOW_VAR_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - AIRFLOW_VAR_S3_ACCESS_KEY={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AIRFLOW_VAR_S3_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
      - AIRFLOW_VAR_S3_PATH_STYLE_ACCESS='{{s3PathStyleAccess}}'
    {% endif %}
    {% if AIRFLOW_variables is defined and AIRFLOW_variables and AIRFLOW_variables | length | default(false) %}
      {% for variable in AIRFLOW_variables.split(',') %}
      - AIRFLOW_VAR_{{variable}}
      {% endfor %}
    {% endif %}
    {% if AIRFLOW_connections is defined and AIRFLOW_connections and AIRFLOW_connections | length | default(false) %}
      {% for connection in AIRFLOW_connections.split(',') %}
      - AIRFLOW_CONN_{{connection}}
      {% endfor %}
    {% endif %}
    {% if MARQUEZ_enable and AIRFLOW_openlineage_enabled | default(false) %}
      - AIRFLOW__OPENLINEAGE__DISABLED=false
      - AIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO=true
      - AIRFLOW__OPENLINEAGE__DEBUG_MODE=false
      - AIRFLOW__OPENLINEAGE__TRANSPORT='{"type": "http", "url": "http://marquez:5000", "endpoint": "api/v1/lineage"}'
    {% else %}      
      - AIRFLOW__OPENLINEAGE__DISABLED=true      
    {% endif %}      
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/airflow/dags:/opt/airflow/dags:Z
      - ./plugins/airflow/:/opt/airflow/plugins:Z
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    {% if AIRFLOW_volume_map_logs %}
      - ./container-volume/airflow/logs/webserver:/opt/airflow/logs:Z
    {% endif -%}   {#  AIRFLOW_volume_map_logs #}
    {%if SPARK_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/spark:RO
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - aws-credentials-vol:/home/airflow/.aws:RO
    {% endif %}  
    {% if AIRFLOW_volume_map_docker_daemon %}
      - /var/run/docker.sock:/var/run/docker.sock
    {% endif -%}   {#  AIRFLOW_volume_map_docker_daemon #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
    restart: {{container_restart_policy}}
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s    

  airflow-scheduler:
    image: {{AIRFLOW_custom_image_name if AIRFLOW_custom_image_name is defined and AIRFLOW_custom_image_name | length else ' apache/airflow'}}:{{__AIRFLOW_version}}
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Job Orchestration & Scheduler"
    command:
      - bash
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 airflow-db:5432
        airflow scheduler
  {% if (AIRFLOW_volume_map_docker_daemon) %}
    group_add:
      - "$DOCKER_GROUP"
  {% endif %}
    environment:
      - AIRFLOW__CORE__EXECUTOR={{airflow_executor}}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= '{{AIRFLOW_fernet_key}}'
      - AIRFLOW__WEBSERVER__SECRET_KEY={{AIRFLOW_secret_key}}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION={{AIRFLOW_dags_paused_at_creation}}
      - AIRFLOW__CORE__LOAD_EXAMPLES={{AIRFLOW_provision_examples}}
      - AIRFLOW__API__AUTH_BACKENDS={{AIRFLOW_auth_backends | default(omit)}}
      - _PIP_ADDITIONAL_REQUIREMENTS={{AIRFLOW_additional_requirements}}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL={{AIRFLOW_dag_dir_list_interval}}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG={{AIRFLOW_expose_config}}
    {%if SPARK_enable | default(false) %}
      - SPARK_HOME=/opt/spark
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - AIRFLOW_VAR_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - AIRFLOW_VAR_S3_ACCESS_KEY={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AIRFLOW_VAR_S3_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
      - AIRFLOW_VAR_S3_PATH_STYLE_ACCESS='{{s3PathStyleAccess}}'
    {% endif %}
    {% if AIRFLOW_variables is defined and AIRFLOW_variables | length | default(false) %}
      {% for variable in AIRFLOW_variables.split(',') %}
      - AIRFLOW_VAR_{{variable}}
      {% endfor %}
    {% endif %}
    {% if MARQUEZ_enable and AIRFLOW_openlineage_enabled | default(false) %}
      - AIRFLOW__OPENLINEAGE__DISABLED=false
      - AIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO=true
      - AIRFLOW__OPENLINEAGE__DEBUG_MODE=false
      - AIRFLOW__OPENLINEAGE__TRANSPORT='{"type": "http", "url": "http://marquez:5000", "endpoint": "api/v1/lineage"}'
    {% else %}      
      - AIRFLOW__OPENLINEAGE__DISABLED=true      
    {% endif %}         
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/airflow/dags:/opt/airflow/dags:Z
      - ./plugins/airflow/:/opt/airflow/plugins:Z
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    {% if AIRFLOW_volume_map_logs %}
      - ./container-volume/airflow/logs/scheduler:/opt/airflow/logs:Z
    {% endif -%}   {#  AIRFLOW_volume_map_logs #}
    {%if SPARK_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/spark:RO
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - aws-credentials-vol:/home/airflow/.aws:RO
    {% endif %}      
    {% if AIRFLOW_volume_map_docker_daemon %}
      - /var/run/docker.sock:/var/run/docker.sock
    {% endif -%}   {#  AIRFLOW_volume_map_docker_daemon #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
    restart: {{container_restart_policy}}
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8974/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  {% if (AIRFLOW_executor | lower) == 'celery'  %}
    {% for num in range(AIRFLOW_workers | default(1) ) %}

  airflow-worker-{{loop.index}}:
    image: {{AIRFLOW_custom_image_name if AIRFLOW_custom_image_name is defined and AIRFLOW_custom_image_name | length else ' apache/airflow'}}:{{__AIRFLOW_version}}
    container_name: airflow-worker-{{loop.index}}
    hostname: airflow-worker-{{loop.index}}
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Job Orchestration & Scheduler"
    command:
      - bash
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 airflow-db:5432
        airflow celery worker
  {% if (AIRFLOW_volume_map_docker_daemon) %}
    group_add:
      - "$DOCKER_GROUP"
  {% endif %}
    environment:
      - AIRFLOW__CORE__EXECUTOR={{airflow_executor}}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= '{{AIRFLOW_fernet_key}}'
      - AIRFLOW__WEBSERVER__SECRET_KEY={{AIRFLOW_secret_key}}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION={{AIRFLOW_dags_paused_at_creation}}
      - AIRFLOW__CORE__LOAD_EXAMPLES={{AIRFLOW_provision_examples}}
      - AIRFLOW__API__AUTH_BACKENDS={{AIRFLOW_auth_backends | default(omit)}}
      - _PIP_ADDITIONAL_REQUIREMENTS={{AIRFLOW_additional_requirements}}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL={{AIRFLOW_dag_dir_list_interval}}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG={{AIRFLOW_expose_config}}
      - DUMB_INIT_SETSID="0"
    {%if SPARK_enable | default(false) %}
      - SPARK_HOME=/opt/spark
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - AIRFLOW_VAR_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - AIRFLOW_VAR_S3_ACCESS_KEY={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AIRFLOW_VAR_S3_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
      - AIRFLOW_VAR_S3_PATH_STYLE_ACCESS='{{s3PathStyleAccess}}'
    {% endif %}
    {% if AIRFLOW_variables is defined and AIRFLOW_variables | length | default(false) %}
      {% for variable in AIRFLOW_variables.split(',') %}
      - AIRFLOW_VAR_{{variable}}
      {% endfor %}
    {% endif %}
    {% if MARQUEZ_enable and AIRFLOW_openlineage_enabled | default(false) %}
      - AIRFLOW__OPENLINEAGE__DISABLED=false
      - AIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO=true
      - AIRFLOW__OPENLINEAGE__DEBUG_MODE=false
      - AIRFLOW__OPENLINEAGE__TRANSPORT='{"type": "http", "url": "http://marquez:5000", "endpoint": "api/v1/lineage"}'
    {% else %}      
      - AIRFLOW__OPENLINEAGE__DISABLED=true      
    {% endif %}         
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/airflow/dags:/opt/airflow/dags:Z
      - ./plugins/airflow/:/opt/airflow/plugins:Z
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    {% if AIRFLOW_volume_map_logs %}
      - ./container-volume/airflow/logs/scheduler:/opt/airflow/logs:Z
    {% endif -%}   {#  AIRFLOW_volume_map_logs #}
    {%if SPARK_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/spark:RO
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - aws-credentials-vol:/home/airflow/.aws:RO
    {% endif %}      
    {% if AIRFLOW_volume_map_docker_daemon %}
      - /var/run/docker.sock:/var/run/docker.sock
    {% endif -%}   {#  AIRFLOW_volume_map_docker_daemon #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
    restart: {{container_restart_policy}}
    {% endfor %}

  airflow-triggerer:
    image: {{AIRFLOW_custom_image_name if AIRFLOW_custom_image_name is defined and AIRFLOW_custom_image_name | length else ' apache/airflow'}}:{{__AIRFLOW_version}}
    container_name: airflow-triggerer
    hostname: airflow-triggerer
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Job Orchestration & Scheduler"
    command: triggerer
  {% if (AIRFLOW_volume_map_docker_daemon) %}
    group_add:
      - "$DOCKER_GROUP"
  {% endif %}
    environment:
      - AIRFLOW__CORE__EXECUTOR={{airflow_executor}}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= '{{AIRFLOW_fernet_key}}'
      - AIRFLOW__WEBSERVER__SECRET_KEY={{AIRFLOW_secret_key}}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION={{AIRFLOW_dags_paused_at_creation}}
      - AIRFLOW__CORE__LOAD_EXAMPLES={{AIRFLOW_provision_examples}}
      - AIRFLOW__API__AUTH_BACKENDS={{AIRFLOW_auth_backends | default(omit)}}
      - _PIP_ADDITIONAL_REQUIREMENTS={{AIRFLOW_additional_requirements}}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL={{AIRFLOW_dag_dir_list_interval}}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG={{AIRFLOW_expose_config}}
    {%if SPARK_enable | default(false) %}
      - SPARK_HOME=/opt/spark
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - AIRFLOW_VAR_S3_ENDPOINT={{s3Endpoint | default(omit) }}
      - AIRFLOW_VAR_S3_ACCESS_KEY={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - AIRFLOW_VAR_S3_SECRET_ACCESS_KEY={{s3SecretAccessKey}}
      - AIRFLOW_VAR_S3_PATH_STYLE_ACCESS='{{s3PathStyleAccess}}'
    {% endif %}
    {% if AIRFLOW_variables is defined and AIRFLOW_variables | length | default(false) %}
      {% for variable in AIRFLOW_variables.split(',') %}
      - AIRFLOW_VAR_{{variable}}
      {% endfor %}
    {% endif %}
    {% if MARQUEZ_enable and AIRFLOW_openlineage_enabled | default(false) %}
      - AIRFLOW__OPENLINEAGE__DISABLED=false
      - AIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO=true
      - AIRFLOW__OPENLINEAGE__DEBUG_MODE=false
      - AIRFLOW__OPENLINEAGE__TRANSPORT='{"type": "http", "url": "http://marquez:5000", "endpoint": "api/v1/lineage"}'
    {% else %}      
      - AIRFLOW__OPENLINEAGE__DISABLED=true      
    {% endif %}         
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/airflow/dags:/opt/airflow/dags:Z
      - ./plugins/airflow/:/opt/airflow/plugins:Z
    {% if AIRFLOW_volume_map_logs %}
      - ./container-volume/airflow/logs/triggerer:/opt/airflow/logs:Z
    {% endif -%}   {#  AIRFLOW_volume_map_logs #}
    {%if SPARK_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/spark:RO
    {% endif -%}   {#  SPARK_enable #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - aws-credentials-vol:/home/airflow/.aws:RO
    {% endif %}      
    {% if AIRFLOW_volume_map_docker_daemon %}
      - /var/run/docker.sock:/var/run/docker.sock
    {% endif -%}   {#  AIRFLOW_volume_map_docker_daemon #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
    restart: {{container_restart_policy}}

  airflow-redis:
    image: redis:latest
    container_name: airflow-redis
    hostname: airflow-redis
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Key-Value Store"
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airflow-flower:
    image: {{AIRFLOW_custom_image_name if AIRFLOW_custom_image_name is defined and AIRFLOW_custom_image_name | length else ' apache/airflow'}}:{{__AIRFLOW_version}}
    container_name: airflow-flower
    hostname: airflow-flower
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Job Orchestration & Scheduler"
      com.platys.webui.title: "Airflow Flower UI"
      com.platys.webui.url: "http://dataplatform:5555"
    ports:
      - 5555:5555
    command: celery flower
    environment:
      - AIRFLOW__CORE__EXECUTOR={{airflow_executor}}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= ''
      - AIRFLOW__WEBSERVER__SECRET_KEY={{AIRFLOW_secret_key}}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION={{AIRFLOW_dags_paused_at_creation}}
      - AIRFLOW__CORE__LOAD_EXAMPLES={{AIRFLOW_provision_examples}}
      - AIRFLOW__API__AUTH_BACKENDS={{AIRFLOW_auth_backends | default(omit)}}
      - _PIP_ADDITIONAL_REQUIREMENTS={{AIRFLOW_additional_requirements}}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL={{AIRFLOW_dag_dir_list_interval}}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG={{AIRFLOW_expose_config}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "${AIRFLOW_UID:-50000}:0"
    restart: {{container_restart_policy}}

  {% endif %}  {#  AIRFLOW_executor  #}

  airflow-db:
    image: 'postgres:13'
    container_name: airflow-db
    hostname: airflow-db
    labels:
      com.platys.name: 'airflow'
      com.platys.description: "Job Orchestration & Scheduler"
      com.platys.password.envvars: "PLATYS_AIRFLOW_POSTGRESQL_PASSWORD"      
    environment:
      - POSTGRES_DB=airflowdb
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  airflow-init:
    image: {{AIRFLOW_custom_image_name if AIRFLOW_custom_image_name is defined and AIRFLOW_custom_image_name | length else ' apache/airflow'}}:{{__AIRFLOW_version}}
    container_name: airflow-init
    hostname: airflow-init
    labels:
      com.platys.password.envvars: "PLATYS_AIRFLOW_ADMIN_PASSWORD,PLATYS_AIRFLOW_SECRET_KEY"    
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        function ver() {
          printf "%04d%04d%04d%04d" $${1//./ }
        }
        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && airflow version)
        airflow_version_comparable=$$(ver $${airflow_version})
        min_airflow_version=2.2.0
        min_airflow_version_comparable=$$(ver $${min_airflow_version})
        if (( airflow_version_comparable < min_airflow_version_comparable )); then
          echo
          echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
          echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
          echo
          exit 1
        fi
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      - AIRFLOW__CORE__EXECUTOR={{airflow_executor}}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      # For backward compatibility, with Airflow <2.3
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:${PLATYS_AIRFLOW_POSTGRESQL_PASSWORD:-abc123!}@airflow-db/airflowdb
      - AIRFLOW__CELERY__BROKER_URL=redis://:@airflow-redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY= ''
      - AIRFLOW__WEBSERVER__SECRET_KEY=${PLATYS_AIRFLOW_SECRET_KEY:-{{AIRFLOW_secret_key}}}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION={{AIRFLOW_dags_paused_at_creation}}
      - AIRFLOW__CORE__LOAD_EXAMPLES={{AIRFLOW_provision_examples}}
      - AIRFLOW__API__AUTH_BACKENDS={{AIRFLOW_auth_backends | default(omit)}}
      - _PIP_ADDITIONAL_REQUIREMENTS={{AIRFLOW_additional_requirements}}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL={{AIRFLOW_dag_dir_list_interval}}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG={{AIRFLOW_expose_config}}
      - _AIRFLOW_DB_MIGRATE=True
      - _AIRFLOW_WWW_USER_CREATE=True
      - _AIRFLOW_WWW_USER_USERNAME={{AIRFLOW_admin_username}}
      - _AIRFLOW_WWW_USER_PASSWORD=${PLATYS_AIRFLOW_ADMIN_PASSWORD:-{{AIRFLOW_admin_password}}}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "0:0"
    init: true
{% endif %}   {#  AIRFLOW_enable  #}

{%if MAGE_AI_enable | default(false) %}
  #  ================================== Mage AI ========================================== #
  mage-ai:
    image: mageai/mageai:{{__MAGE_AI_version}}
    container_name: mage-ai
    hostname: mage-ai
    labels:
      com.platys.name: 'mage-ai'
      com.platys.description: "Orchestration Engine"
      com.platys.webui.title: "Mage AI UI"
      com.platys.webui.url: "http://dataplatform:6789"
    ports:
      - "6789:6789"      
    environment:
      USER_CODE_PATH: /home/src/{{MAGE_AI_project_name}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/mage-ai/:/home/src/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: mage start {{MAGE_AI_project_name}}
    restart: {{container_restart_policy}}
{% endif %}   {#  MAGE_AI_enable  #}

{%if KESTRA_enable | default(false) %}
  #  ================================== Kestra ========================================== #
  kestra:
    image: kestra/kestra:{{__KESTRA_version}}
    container_name: kestra
    hostname: kestra
    labels:
      com.platys.name: 'kestra'
      com.platys.description: "Orchestration Engine"
      com.platys.webui.title: "Kestra UI"
      com.platys.webui.url: "http://dataplatform:28245"
      com.platys.restapi.title: Kestra UI
      com.platys.restapi.url: http://dataplatform:28245/api/v1/usages/all      
      com.platys.password.envvars: "PLATYS_KESTRA_PASSWORD"
  {% if KESTRA_flavour | default('standalone') == 'standalone' %}
    depends_on:
      postgresql:
        condition: service_started    
  {% endif -%}   {#  KESTRA_flavour  #}
    ports:
      - "28245:8080"      
      - "28246:8081"      
    environment:
      KESTRA_CONFIGURATION: |
        datasources:
  {% if KESTRA_flavour | default('standalone') == 'standalone' %}
          postgres:
            url: jdbc:postgresql://postgresql:5432/{{KESTRA_database_name}}
            driverClassName: org.postgresql.Driver
            username: {{KESTRA_database_user}}
            password: {{KESTRA_database_password}}
  {% endif -%}   {#  KESTRA_flavour  #}
        kestra:
          server:
            basicAuth:
              enabled: {{KESTRA_auth_enabled | default(false)}}
              username: {{KESTRA_username if KESTRA_username is defined and KESTRA_username and KESTRA_username | length else 'kestra'}} # it must be a valid email address
              password: ${PLATYS_KESTRA_PASSWORD:-{{KESTRA_password}}}
  {% if KESTRA_flavour | default('standalone') == 'standalone' %}
          repository:
            type: {{KESTRA_database_type}}
  {% endif -%}   {#  KESTRA_flavour  #}
          storage:
            type: {{KESTRA_storage_type}}            
  {%if KESTRA_storage_type == 'local' %}
            local:
              basePath: "/app/storage"
  {% endif -%}   {#  KESTRA_storage_type = local #}            
  {%if KESTRA_storage_type == 'minio' and MINIO_enable | default(false) %}
            minio:
              endpoint: {{s3Endpoint | default(omit) }}
              port: 9000
              accessKey: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
              secretKey: {{s3SecretAccessKey}}
              region: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
              bucket: {{KESTRA_bucket_name}}
  {% endif -%}   {#  KESTRA_storage_type = 'minio' #}      
          queue:
            type: {{KESTRA_database_type}}
          tasks:
            tmpDir:
              path: /tmp/kestra-wd/tmp
          tutorialFlows:
            enabled: {{KESTRA_provision_tutorial_flows | default(false)}}    
          variables:
            envVarsPrefix: KESTRA_            
          url: http://${PUBLIC_IP}:28245/
    {% if KESTRA_env_variables is defined and KESTRA_env_variables and KESTRA_env_variables | length | default(false) %}
      {% for envvar in KESTRA_env_variables.split(",") %}
      {{"KESTRA_"}}{{envvar.split('=')[0]}}: "{{envvar.split('=')[1]}}"
      {% endfor %}
    {% endif -%}   {#  KESTRA_env_variables #}        
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if KESTRA_volume_map_data | default(false) %}  
      - ./container-volume/kestra/data:/app/storage
    {% endif %}  
    {% if KESTRA_volume_map_temp | default(false) %}  
      - ./container-volume/kestra/tmp/kestra-wd:/tmp/kestra-wd
    {% endif %}  
      - /var/run/docker.sock:/var/run/docker.sock
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: server {{KESTRA_flavour | default('standalone')}} --worker-thread=40 --log-level=INFO
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s    
{% endif %}   {#  KESTRA_enable  #}

{%if OFELIA_enable | default(false) %}
  #  ================================== Ofelia ========================================== #
  ofelia:
    image: mcuadros/ofelia:{{__OFELIA_version}}
    container_name: ofelia
    hostname: ofelia
    labels:
      com.platys.name: 'ofelia'
      com.platys.description: "Docker Job Scheduler"
    {%if OFELIA_job_name is defined and OFELIA_job_name and OFELIA_job_name | length | default(false) %}
      ofelia.{{OFELIA_job_type}}.{{OFELIA_job_name | replace(' ','-') }}.schedule: "{{OFELIA_job_schedule if OFELIA_job_name is defined and OFELIA_job_name and OFELIA_job_name | length else omit}}"
      ofelia.{{OFELIA_job_type}}.{{OFELIA_job_name | replace(' ','-') }}.container: "{{OFELIA_job_container if OFELIA_job_container is defined and OFELIA_job_container and OFELIA_job_container | length else omit}}"
      ofelia.{{OFELIA_job_type}}.{{OFELIA_job_name | replace(' ','-') }}.image: "{{OFELIA_job_image if OFELIA_job_image is defined and OFELIA_job_image and OFELIA_job_image | length else omit}}"
      ofelia.{{OFELIA_job_type}}.{{OFELIA_job_name | replace(' ','-') }}.command: "{{OFELIA_job_command if OFELIA_job_command is defined and OFELIA_job_command and OFELIA_job_command | length else omit}}"
      ofelia.{{OFELIA_job_type}}.{{OFELIA_job_name | replace(' ','-') }}.network: "{{OFELIA_job_network if OFELIA_job_network is defined and OFELIA_job_network and OFELIA_job_network | length else omit}}"
    {% endif -%}   {#  OFELIA_job_name #}
    {%if OFELIA_log_to_folder | default(false) %}
      ofelia.save-folder: /var/log/
    {% endif -%}   {#  OFELIA_log_to_folder #}
    environment:
      DUMMY: make-it-valid
    {% if OFELIA_env_variables is defined and OFELIA_env_variables and OFELIA_env_variables | length | default(false) %}
      {% for envvar in OFELIA_env_variables.split(",") %}
      {{envvar.split('=')[0]}}: "{{envvar.split('=')[1]}}"
      {% endfor %}
    {% endif -%}   {#  OFELIA_env_variables #}       
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: daemon --docker
    restart: {{container_restart_policy}}
{% endif %}   {#  OFELIA_enable  #}

{%if ZEPPELIN_enable | default(false) %}
  #  ================================== Zeppelin ========================================== #
  zeppelin:
    image: apache/zeppelin:{{__ZEPPELIN_version}}
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.platys.name: 'zeppelin'
      com.platys.description: "Data Science Notebook"
      com.platys.webui.title: "Apache Zeppelin UI"
      com.platys.webui.url: "http://dataplatform:28080"
      com.platys.password.envvars: "PLATYS_ZEPPELIN_ADMIN_PASSWORD,PLATYS_ZEPPELIN_USER_PASSWORD,PLATYS_AWS_SECRET_ACCESS_KEY"
    ports:
      - "28080:8080"
      - "6060:6060"
      - "5050:5050"
      - "4050-4054:4050-4054"
    environment:
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      AWS_ENDPOINT: {{s3Endpoint if s3Endpoint is defined and s3Endpoint else omit}}
      AWS_DEFAULT_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
  {% endif -%}   {#  MINIO_enable #}

  {%if SPARK_enable | default(false) %}
      SPARK_HOME: /opt/bitnami/spark
    {%if ZEPPELIN_use_local_spark | default(false) %}
      ZEPPELIN_SPARK_MASTER: "local[*]"
      SPARK_MASTER: "local[*]"
    {% else %}
      ZEPPELIN_SPARK_MASTER: "spark://spark-master:7077"
      SPARK_MASTER: "spark://spark-master:7077"
    {% endif -%}   {#  ZEPPELIN_use_local_spark #}
      SPARK_SUBMIT_OPTIONS: "{{ZEPPELIN_spark_submit_options}} --conf spark.ui.port=4050 --conf spark.driver.host=zeppelin --conf spark.driver.port=5050 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.blockManager.port=6060 --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4"
      PYSPARK_PYTHON: "python3"
      PYSPARK_DRIVER_PYTHON: "python3"
  {% endif -%}   {#  SPAR_enable #}
  {%if FLINK_enable | default(false) %}
      FLINK_HOME: /opt/flink
  {% endif -%}   {#  FLINK_enable #}
      # Zeppelin Properties
      ZEPPELIN_ADDR: "0.0.0.0"
      ZEPPELIN_PORT: "8080"
      ZEPPELIN_MEM: '-Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m'
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: 'https://repo.maven.apache.org/maven2'
      ZEPPELIN_NOTEBOOK_DIR: '{{ZEPPELIN_notebook_dir}}'
      ZEPPELIN_NOTEBOOK_CRON_ENABLE: '{{ZEPPELIN_notebook_cron_enable}}'

      # Will replace values in shiro.ini
      ZEPPELIN_ADMIN_USERNAME: {{ZEPPELIN_admin_username}}
      ZEPPELIN_ADMIN_PASSWORD: ${PLATYS_ZEPPELIN_ADMIN_PASSWORD:-{{ZEPPELIN_admin_password}}}
      ZEPPELIN_USER_USERNAME: {{ZEPPELIN_user_username}}
      ZEPPELIN_USER_PASSWORD: ${PLATYS_ZEPPELIN_USER_PASSWORD:-{{ZEPPELIN_user_password}}}
  {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
  {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/zeppelin:/opt/zeppelin/conf.templates
      - ./init/zeppelin/zeppelin-init.sh:/opt/zeppelin/bin/zeppelin-init.sh
      - ./container-volume/spark/logs/:/var/log/spark/logs
  {% if ZEPPELIN_volume_map_data %}
      - ./container-volume/spark/notebooks:/opt/zeppelin/{{ZEPPELIN_notebook_dir}}
  {% endif -%}   {#  ZEPPELIN_volume_map_data #}
      - './conf/awscli/s3cfg.template:/root/.s3cfg.template'
  {%if SPARK_enable | default(false) %}
      - spark-{{__SPARK_version | replace('.','-')}}-vol:/opt/bitnami/spark:RO
  {% endif -%}   {#  SPARK_enable #}
  {%if FLINK_enable | default(false) %}
      - flink-vol:/opt/flink:RO
  {% endif -%}   {#  FLINK_enable #}
  {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
  {% endif -%}   {#  use_timezone #}
  {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
  {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - bash
      - -c
      - |
        ./bin/zeppelin-init.sh
        ./bin/zeppelin.sh
    restart: {{container_restart_policy}}
{% endif %}   {#  ZEPPELIN_enable  #}

{%if JUPYTER_enable | default(false) %}
  #  ================================== Jupyter ========================================== #
  jupyter:
    image: {{'quay.io/' if (SPARK_base_version | lower) == '3.5' }}jupyter/{{ (JUPYTER_edition | lower) }}-notebook:{{__JUPYTER_version}}
    container_name: jupyter
    hostname: jupyter
    labels:
      com.platys.name: 'jupyter'
      com.platys.description: 'Web-based interactive development environment for notebooks, code, and data'
      com.platys.webui.title: "Jupyter UI"
      com.platys.webui.url: "http://dataplatform:28888"
      com.platys.password.envvars: "PLATYS_JUPYTER_TOKEN,PLATYS_AWS_SECRET_ACCESS_KEY"
    ports:
      - "28888:8888"
  {%if JUPYTER_edition == 'all-spark' or JUPYTER_edition == 'pyspark' | default(false) %}
      - "28376-28380:4040-4044"
  {% endif -%}   {#  JUPYTER_edition #}
    user: "root"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      JUPYTER_ENABLE_LAB: "'yes'"
      GRANT_SUDO: "'yes'"
      JUPYTER_TOKEN: ${PLATYS_JUPYTER_TOKEN:-{{JUPYTER_token if JUPYTER_token is defined and JUPYTER_token | length else '<token undefined>'}}}
      NB_USER: {{JUPYTER_nb_user if JUPYTER_nb_user is defined and JUPYTER_nb_user | length else omit}}
      NOTEBOOK_ARGS: {{JUPYTER_notebook_args if JUPYTER_notebook_args is defined and JUPYTER_notebook_args | length else omit}}
      DOCKER_STACKS_JUPYTER_CMD: {{JUPYTER_startup_cmd if JUPYTER_startup_cmd is defined and JUPYTER_startup_cmd | length  else omit}}
      MAVEN_DOWNLOAD_JARS: {{JUPYTER_download_jars}}
      # remove some JARS if they are conflicting with the ones installed above
      REMOVE_JARS: "guava-14.0.1.jar"
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      AWS_ENDPOINT: {{s3Endpoint if s3Endpoint is defined and s3Endpoint else omit}}
      AWS_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      AWS_DEFAULT_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
    {%if MLFLOW_SERVER_enable | default(false) %}
      MLFLOW_S3_ENDPOINT_URL: {{s3Endpoint | default(omit) }}
      MLFLOW_TRACKING_URI: http://mlflow-tracking-server:5000
    {% endif %}
  {% endif -%}     {#  MINIO_enable  #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if JUPYTER_upload_data_transfer_to_workspace_enabled %}
      - ./data-transfer:/home/jovyan/work/data-transfer
    {% endif -%}   {#  JUPYTER_upload_data_transfer_to_workspace_enabled #}
      - ./init/jupyter/on-startup-jupyter/:/usr/local/bin/start-notebook.d/
      - ./init/jupyter/on-startup-jupyter-finished/:/usr/local/bin/before-notebook.d/
      - ./init/jupyter/on-startup-notebook-kernel:/home/jovyan/.ipython/profile_default/startup/
      - ./scripts/docker/maven-download.sh:/maven-download.sh
    {% if JUPYTER_volume_map_docker_daemon %}
      - /var/run/docker.sock:/var/run/docker.sock
    {% endif -%}   {#  JUPYTER_volume_map_docker_daemon #}
    {% if JUPYTER_volume_map_data %}
      - "{{JUPYTER_data_folder_on_docker_host}}:/home/jovyan/work"
    {% endif %}
    {% if JUPYTER_requirements_file is defined and JUPYTER_requirements_file and JUPYTER_requirements_file|length  %}
      - {{JUPYTER_requirements_file}}:/tmp/requirements.txt
    {% endif %}
    {% if JUPYTER_edition == 'all-spark' %}
    #  - "./conf/jupyter/spark-defaults.conf:/usr/local/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf"
    {% endif -%}   {#  JUPYTER_edition == 'all-spark' #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
    {% if JUPYTER_edition == 'all-spark' and JUPYTER_spark_jars_packages is defined and JUPYTER_spark_jars_packages|length  %}
        printf '\nspark.jars.packages  {{JUPYTER_spark_jars_packages}}' >> /usr/local/spark/conf/spark-defaults.conf
    {% endif -%}   {#  JUPYTER_spark_jars_packages #}
    {% if JUPYTER_python_packages is defined and JUPYTER_python_packages and JUPYTER_python_packages|length  %}
        pip install {{JUPYTER_python_packages}}
    {% endif -%}   {#  JUPYTER_python_packages #}
    {% if JUPYTER_requirements_file is defined and JUPYTER_requirements_file and JUPYTER_requirements_file|length  %}
        pip install -r /tmp/requirements.txt
    {% endif -%}   {#  JUPYTER_python_packages #}
    {% if JUPYTER_tokenless %}
        start-notebook.sh --LabApp.token=''
    {% else %}
        start-notebook.sh
    {% endif -%}   {#  JUPYTER_tokenless #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  JUPYTER_enable  #}

{%if JUPYTERHUB_enable | default(false) %}
  #  ================================== JupyterHub ========================================== #
  jupyterhub:
    image: jupyterhub/jupyterhub:{{__JUPYTERHUB_version}}
    container_name: jupyterhub
    hostname: jupyterhub
    labels:
      com.platys.name: 'jupyterhub'
      com.platys.description: "Serve Jupyter notebook for multiple users"
      com.platys.webui.title: "JupyterHub UI"
      com.platys.webui.url: "http://dataplatform:28284"
    ports:
      - "28284:8000"
    environment:
      JUPYTERHUB_AUTHENTICATOR_CLASS: {{JUPYTERHUB_authenticator_class}}
      JUPYTERHUB_PASSWORD: {{JUPYTERHUB_global_password}}
      DOCKER_NETWORK_NAME: {{platys['platform-name']}}
      DOCKER_NOTEBOOK_IMAGE: "{{JUPYTERHUB_notebook_image}}"
      DOCKER_NOTEBOOK_DIR: "/home/jovyan/work"
      DATA_VOLUME_CONTAINER: "/data"
      DOCKER_SPAWN_CMD: "start-singleuser.sh"
    {% if JUPYTERHUB_use_postgres is defined and JUPYTERHUB_use_postgres %}
      JUPYTERHUB_USE_POSTGRES: "'yes'"
      POSTGRES_HOST: {{JUPYTERHUB_postgres_host}}
      POSTGRES_DB: {{JUPYTERHUB_postgres_db}}
      POSTGRES_USERNAME: {{JUPYTERHUB_postgres_username}}
      POSTGRES_PASSWORD: {{JUPYTERHUB_postgres_password}}
    {% else -%}   {#  JUPYTERHUB_use_postgres #}
      JUPYTERHUB_USE_POSTGRES: "'no'"
    {% endif -%}   {#  JUPYTERHUB_use_postgres #}
      DATA_VOLUME_CONTAINER: /data
      #GITHUB_CLIENT_ID:
      #GITHUB_CLIENT_SECRET:
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - "/var/run/docker.sock:/var/run/docker.sock:rw"
      - ./conf/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py
      - ./{{ 'custom-' if (JUPYTERHUB_custom_userlist | default(false) )}}conf/jupyterhub/userlist:/srv/jupyterhub/userlist
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        pip install dockerspawner
    {% if JUPYTERHUB_use_postgres is defined and JUPYTERHUB_use_postgres  %}
        pip install psycopg2-binary
    {% endif -%}   {#  JUPYTERHUB_use_postgres #}
    {% if JUPYTERHUB_python_packages is defined and JUPYTERHUB_python_packages|length  %}
        pip install {{JUPYTERHUB_python_packages}}
    {% endif -%}   {#  JUPYTERHUB_python_packages #}
        jupyterhub -f /srv/jupyterhub/jupyterhub_config.py
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  JUPYTERHUB_enable  #}

{%if ANACONDA_enable | default(false) %}
  #  ================================== Anaconda ========================================== #
  anaconda:
    image: continuumio/anaconda3:{{__ANACONDA_version}}
    container_name: anaconda
    hostname: anaconda
    labels:
      com.platys.name: 'anaconda'
      com.platys.description: "Anacona Jupyter"
      com.platys.webui.title: "Anaconda Jupyter UI"
      com.platys.webui.url: "http://dataplatform:28889"
    ports:
      - "28889:8888"
    environment:
      JUPYTER_TOKEN: {{ANACONDA_jupyter_token if ANACONDA_jupyter_token is defined and ANACONDA_jupyter_token | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ANACONDA_volume_map_notebooks | default(false) %}
      - ./container-volume/anaconda/notebooks:/opt/notebooks
    {% endif -%}   {#  ANACONDA_volume_map_notebooks #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      /bin/bash -c "/opt/conda/bin/conda install jupyter -y --quiet && mkdir -p /opt/notebooks && /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='0.0.0.0' --port=8888 --no-browser --allow-root"
    tty: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  JUPYTERHUB_enable  #}

{%if RSTUDIO_enable | default(false) %}
  #  ================================== RStudio ========================================== #
  rstudio:
    image: rocker/rstudio:{{__RSTUDIO_version}}
    container_name: rstudio
    hostname: rstudio
    labels:
      com.platys.name: 'rstudio'
      com.platys.description: "Integrated development environment (IDE) for R "
      com.platys.webui.title: "RStudio UI"
      com.platys.webui.url: "http://dataplatform:8787"
      com.platys.password.envvars: "PLATYS_RSTUDIO_PASSWORD"
    ports:
      - "8787:8787"
    environment:
      PASSWORD: ${PLATYS_RSTUDIO_PASSWORD:-{{RSTUDIO_password | default ('rstudio')}}}
      ROOT: '{{RSTUDIO_run_as_root | default(false)}}'
      DISABLE_AUTH: '{{RSTUDIO_disable_auth | default(false)}}'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  RSTUDIO_enable  #}

{%if SHINY_SERVER_enable | default(false) %}
  #  ================================== Shiny Server ========================================== #
  shiny-server:
    image: rocker/{{ 'shiny' if (SHINY_SERVER_edition | lower) == 'base' else 'shiny-verse' }}:{{__SHINY_SERVER_version}}
    container_name: shiny-server
    hostname: shiny-server
    labels:
      com.platys.name: 'shiny'
      com.platys.description: "Web framework for building web applications using R"
      com.platys.webui.title: "Shiny Server UI"
      com.platys.webui.url: "http://dataplatform:3838"
    ports:
      - "3838:3838"
    environment:
      APPLICATION_LOGS_TO_STDOUT: 'false'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if SHINY_SERVER_volume_map_apps %}
      - ./plugins/shiny-server/apps:/srv/shiny-server
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SHINY_SERVER_enable  #}

{%if DATAIKU_DSS_enable | default(false) %}
  #  ================================== Dataiku Data Science Studio  ========================================== #
  dataiku-dss:
    image: dataiku/dss:{{__DATAIKU_DSS_version}}
    container_name: dataiku-dss
    hostname: dataiku-dss
    labels:
      com.platys.name: 'dataiku'
      com.platys.description: "Platform for Everyday AI"
      com.platys.webui.title: "Dataiku DSS UI"
      com.platys.webui.url: "http://dataplatform:28315"
    ports:
      - "28315:10000"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATAIKU_DSS_volume_map_data %}
      - "./container-volume/dataiku-dss/work:/home/dataiku/dss"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DATAIKU_DSS_enable  #}

{%if MLFLOW_SERVER_enable | default(false) %}
  #  ================================== MLflow Server ========================================== #
  mlflow-tracking-server:
    image: ghcr.io/mlflow/mlflow:{{__MLFLOW_SERVER_version}}
    container_name: mlflow-tracking-server
    hostname: mlflow-tracking-server
    labels:
      com.platys.name: 'mlflow'
      com.platys.description: "MLOps platform"
      com.platys.webui.title: "MLflow Tracking Server UI"
      com.platys.webui.url: "http://dataplatform:28229"
    ports:
      - "28229:5000"
    command: mlflow server
    environment:
      MLFLOW_HOST: 0.0.0.0
      MLFLOW_PORT: 5000
    {%if MLFLOW_SERVER_backend | default('file') == 'postgresql' %}
      MLFLOW_BACKEND_STORE_URI: postgresql+psycopg2://{{MLFLOW_SERVER_db_user | default('mlflow')}}:{{MLFLOW_SERVER_db_password | default('mlflow')}}@postgresql:5432/mlflowdb
    {%elif MLFLOW_SERVER_backend | default('file') == 'mylsql' %}
      MLFLOW_BACKEND_STORE_URI: mysql+mysqldb://${MLFLOW_SERVER_db_user | default('mlflow')}}:${MLFLOW_SERVER_db_password | default('mlflow')}}@mlflowdb:3306/mlflowdb
    {%elif MLFLOW_SERVER_backend | default('file') == 'file' %}
      MLFLOW_BACKEND_STORE_URI: '/mlruns'
    {% endif -%}   {#  MLFLOW_SERVER_backend  #}
      MLFLOW_DEFAULT_ARTIFACT_ROOT: http://mlflow-artifacts-server:5500/api/2.0/mlflow-artifacts/artifacts/experiments
      MLFLOW_GUNICORN_OPTS: "--log-level debug"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MLFLOW_SERVER_volume_map_data %}
      - ./container-volume/mlflow/data:/mlruns
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  mlflow-artifacts-server:
    image: ghcr.io/mlflow/mlflow:{{__MLFLOW_SERVER_version}}
    container_name: mlflow-artifacts-server
    hostname: mlflow-artifacts-server
    labels:
      com.platys.name: 'mlflow'
      com.platys.description: "MLOps platform"
      com.platys.webui.title: "MLflow Artifacts Server UI"
      com.platys.webui.url: "http://dataplatform:28230"
    ports:
      - "28230:5500"
    command: mlflow server --artifacts-only
    environment:
      MLFLOW_HOST: 0.0.0.0
      MLFLOW_PORT: 5500
      MLFLOW_ARTIFACTS_DESTINATION: '{{MLFOW_SERVER_artifact_root | default ('/mlruns')}}'
      MLFLOW_GUNICORN_OPTS: "--log-level debug"
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      MLFLOW_S3_ENDPOINT_URL: {{s3Endpoint if s3Endpoint is defined and s3Endpoint else omit}}
      MLFLOW_S3_UPLOAD_EXTRA_ARGS: "{}"
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      AWS_DEFAULT_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
    {% endif %}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MLFLOW_SERVER_volume_map_data %}
      - ./container-volume/mlflow/data:/mlruns
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MLFLOW_SERVER_enable  #}

{% if (OPTUNA_enable) | default(false) %}
  #  ================================== Optuna  ========================================== #
  optuna:
    image: optuna/optuna:{{__OPTUNA_version}}
    container_name: optuna
    hostname: optuna
    labels:
      com.platys.name: 'optuna'
      com.platys.description: "Hyperparameter optimization framework"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if (OPTUNA_DASHBOARD_enable) | default(false) %}
  #  ================================== Optuna Dashboard  ========================================== #
  optuna-dashboard:
    image: trivadis/optuna-dashboard:{{__OPTUNA_DASHBOARD_version}}
    container_name: optuna-dashboard
    hostname: optuna-dashboard
    labels:
      com.platys.name: 'optuna'
      com.platys.description: "Hyperparameter optimization framework"
      com.platys.webui.title: "Optuna Dashboard"
      com.platys.webui.url: "http://dataplatform:28231"
    ports:
      - "28231:8080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {# OPTUNA_DASHBOARD_enable  #}
{% endif %}   {# OPTUNA_enable  #}

{%if MINDSDB_enable | default(false) %}
  #  ================================== MindsDB  ========================================== #
  mindsdb:
    image: mindsdb/mindsdb:{{__MINDSDB_version}}
    container_name: mindsdb
    hostname: mindsdb
    labels:
      com.platys.name: 'mindsdb'
      com.platys.description: "A platform for building Custom AI"
      com.platys.webui.title: "MindsDB UI"
      com.platys.webui.url: "http://dataplatform:47334"
      com.platys.restapi.title: "MindsDB API"
      com.platys.restapi.url: "http://dataplatform:47334/v1/api/api/databases"
    ports:
      - 47334:47334
      - 47335:47335
      - 47336:47336
    environment:
      MINDSDB_DOCKER_ENV: "True"
      #MINDSDB_STORAGE_DIR: /mindsdb/var
      FLASK_DEBUG: 1
      MINDSDB_LOG_LEVEL: "DEBUG"
      #OPENAI_API_KEY:
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: bash -c "watchfiles --filter python 'python -m mindsdb' ."
    restart: {{container_restart_policy}}
    healthcheck:
      test:  ["CMD", "curl", "-f", "http://localhost:47334/api/util/ping"]
      interval: 30s
      timeout: 4s
      retries: 100
{% endif %}   {# MINDSDB_enable  #}

{%if OLLAMA_enable | default(false) %}
  #  ================================== Ollama  ========================================== #
  ollama:
    image: ollama/ollama:{{__OLLAMA_version}}
  {% if OLLAMA_gpu_enabled | default(false) %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: {{OLLAMA_gpu_driver}}
              count: {{OLLAMA_gpu_count}}
              capabilities: [gpu]
  {% endif -%}    {# OLLAMA_gpu_enabled  #}
    container_name: ollama
    hostname: ollama
    labels:
      com.platys.name: 'ollama'
      com.platys.description: "Locally runs large language models"
      com.platys.restapi.title: "Ollama API"
      com.platys.restapi.url: "http://dataplatform:11434/api/tags"
    ports:
      - 11434:11434
    tty: true
    environment:
      OLLAMA_DEBUG: {{1 if OLLAMA_debug else 0}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if OLLAMA_volume_map_data %}
      ./container-volume/ollama/data:/root/.ollama
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {%if OLLAMA_llm is defined and OLLAMA_llm and OLLAMA_llm | length %}
  ollama-pull-model:
    image: genai-stack/pull-model:latest
    container_name: ollama-pull-model
    hostname: ollama-pull-model
    build:
      context: ./dockerfiles/genai-stack
      dockerfile: pull_model.Dockerfile
    environment:
      - OLLAMA_BASE_URL={{ns.ollamaUrl}}
    entrypoint:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        echo "Installing LLMs"
        for llm in $$(echo "{{OLLAMA_llms}}" | sed "s/,/ /g")
        do
          export LLM="$$llm"
          bb -f pull_model.clj "$$llm"
        done
    tty: true
    init: true
  {% endif -%}   {#  OLLAMA_llm is defined ... #}
{% endif %}   {# OLLAMA_enable  #}

{%if NVIDIA_NIM_enable and (NVIDIA_NIM_images is defined and NVIDIA_NIM_images and NVIDIA_NIM_images | length) | default(false) %}
  #  ================================== Nvidia NIMs  ========================================== #
  {% for image in NVIDIA_NIM_images.split(",") %}
    {% set external_port = 28250 + loop.index - 1 %}
  nvidia-nim-{{loop.index}}:
    image: {{image}}
  {% if NVIDIA_NIM_gpu_enabled | default(false) %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: {{NVIDIA_NIM_gpu_counts[loop.index-1] | default(1)}}
              capabilities: [gpu]
  {% endif -%}    {# OLLAMA_gpu_enabled  #}
    container_name: nvidia-nim-{{loop.index}}
    hostname: nvidia-nim-{{loop.index}}
    labels:
      com.platys.name: 'nvidia-nim'
      com.platys.description: "Locally runs Nvidia NIM model(s)"
      com.platys.restapi.title: "Nvidia NIM API"
      com.platys.restapi.url: "http://dataplatform:{{external_port}}/v1"
    ports:
      - {{external_port}}:8000
    environment:
      NGC_API_KEY: {{NVIDIA_NIM_api_key}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if NVIDIA_NIM_volume_map_cache %}
      ./container-volume/nvidia-nim/cache-{{loop.index}}:/opt/nim/.cache
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {# NVIDIA_NIM_enable  #}

{%if LOCAL_AI_enable | default(false) %}
  #  ================================== LocalAI  ========================================== #
  local-ai:
    image: localai/localai:{{__LOCAL_AI_version}}
  {% if LOCAL_AI_gpu_enabled | default(false) %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  {% endif -%}    {# OLLAMA_gpu_enabled  #}
    container_name: local-ai
    hostname: local-ai
    labels:
      com.platys.name: 'local-ai'
      com.platys.description: "Locally runs large language models"
      com.platys.restapi.title: "LocalAI API"
      com.platys.restapi.url: "http://dataplatform:28356"
    ports:
      - 28356:8080
    environment:
      CONTEXT_SIZE: 512
      MODELS_PATH: /models
      IMAGE_PATH: /tmp
      DEBUG: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if LOCAL_AI_volume_map_data %}
      ./container-volume/local-ai/models:/models:cached
      ./container-volume/local-ai/images:/tmp/generated/images/
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {# LOCAL_AI_enable  #}

{%if OPEN_WEBUI_enable | default(false) %}
  #  ================================== Open WebUI  ========================================== #
  open-webui:
    image: ghcr.io/open-webui/open-webui:{{__OPEN_WEBUI_version}}
    container_name: open-webui
    hostname: open-webui
    labels:
      com.platys.name: 'open-webui'
      com.platys.description: "User-friendly WebUI for LLMs"
      com.platys.webui.title: "Open Web UI"
      com.platys.webui.url: "http://dataplatform:28338"
      com.centurylinklabs.watchtower.enable: {{true if MARKDOWN_MADNESS_watchtower_enable else omit}}      
    ports:
      - 28338:8080
    environment:
      WEBUI_AUTH: false
      WEBUI_SECRET_KEY: {{OPEN_WEBUI_secret_key if OPEN_WEBUI_secret_key is defined and OPEN_WEBUI_secret_key and OPEN_WEBUI_secret_key | length else omit}}
      ENABLE_SIGNUP: false
      ENABLE_OLLAMA_API: true
      OLLAMA_BASE_URL: {{ns.ollamaUrl}}
      USE_OLLAMA_DOCKER: false
      ENABLE_OPENAI_API: true
      OPENAI_API_BASE_URL: https://api.openai.com/v1
      OPENAI_API_KEY:       
      DEFAULT_MODELS: llama3
      RAG_EMBEDDING_ENGINE: ollama
      VECTOR_DB: {{OPEN_WEBUI_vector_db if OPEN_WEBUI_vector_db is defined and OPEN_WEBUI_vector_db and OPEN_WEBUI_vector_db | length else omit}}
      AUDIO_STT_ENGINE: {{OPEN_WEBUI_audio_stt_engine if OPEN_WEBUI_audio_stt_engine is defined and OPEN_WEBUI_audio_stt_engine and OPEN_WEBUI_audio_stt_engine | length else omit}}  
      AUDIO_STT_MODEL: whisper-1
      AUDIO_STT_OPENAI_API_BASE_URL: 'http://${PUBLIC_IP}:'
      AUDIO_STT_OPENAI_API_KEY: ''
      USE_CUDA_DOCKER: {{OPEN_WEBUI_gpu_enabled | default(false)}}      
    {% if (OPEN_WEBUI_database_type | lower) == 'postgres' %}
      DATABASE_URL: postgresql://{{OPEN_WEBUI_database_user}}:{{OPEN_WEBUI_database_password}}@postgresql:5432/{{OPEN_WEBUI_database_name}}
#    {% elif (OPEN_WEBUI_database_type | lower) == 'mysql' %}
#      DATABASE_URL: mysql://root:mysql@localhost:3306/mysql
    {% endif -%}   {#  OPEN_WEBUI_database_type #}
    {% if TIKA_enable | default(false) %}
       TIKA_SERVER_URL: http://tika-server:9998
    {% endif -%}   {#  TIKA_enable #}
    {% if CHROMA_enable | default(false) %}
       CHROMA_HTTP_HOST: chroma
       CHROMA_HTTP_PORT: 8000
    {% endif -%}   {#  CHROMA_enable #}
    {% if MILVUS_enable | default(false) %}
       MILVUS_URI: http://milvus:19530
    {% endif -%}   {#  MILVUS_enable #}
    {% if QDRANT_enable | default(false) %}
       QDRANT_URI: http://qdrant:6333
    {% endif -%}   {#  MILVUS_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # - ./container-volume/open-webui:/app/backend/data
    {% if OPEN_WEBUI_volume_map_data %}
      - ./container-volume/open-webui/data:/app/backend/data
    {% endif -%}   {#  OPEN_WEBUI_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {%if OPEN_WEBUI_audio_transcriber_enabled and WHISPER_enable | default(false) %}
  audio-transcriber:
    image: didevlab/audio-transcriber-whisper-api:1.0.0
    container_name: audio-transcriber
    hostname: audio-transcriber
    labels:
      com.platys.name: 'audio-transcriber'
      com.platys.description: "User-friendly WebUI for LLMs (audio transcriber)"
      com.platys.restui.title: "Transcribrer API"
      com.platys.restui.url: "http://dataplatform:28223/audio/transcriptions"
      com.centurylinklabs.watchtower.enable: {{true if MARKDOWN_MADNESS_watchtower_enable else omit}}      
    ports:
      - "28223:80"
    environment:
      - PORT=80
      - API_WHISPER_URL=whisper  # Whisper service address (e.g., 'whisper_api' or IP)
      - API_WHISPER_PORT=9000
      - API_WHISPER_TIMEOUT=360000  # 6 minutes timeout for Whisper response
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif -%}   {#  OPEN_WEBUI_audio_transcriber_enabled and WHISPER_enable ... #}

{% endif %}   {# OPEN_WEBUI_enable  #}

{%if ALPACA_WEBUI_enable | default(false) %}
  #  ================================== Alpaca WebUI  ========================================== #
  alpaca-webui:
    image: forloopse/alpaca-webui:{{__ALPACA_WEBUI_version}}
    container_name: alpaca-webui
    hostname: alpaca-webui
    labels:
      com.platys.name: 'alpaca-webui'
      com.platys.description: "User-friendly WebUI for LLMs"
      com.platys.webui.title: "Alpaca Web UI"
      com.platys.webui.url: "http://dataplatform:28365"
    ports:
      - 28365:3000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # - ./container-volume/open-webui:/app/backend/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
{% endif %}   {# ALPACA_WEBUI_enable  #}

{%if ANYTHING_LLM_enable | default(false) %}
  #  ================================== Anything LLM  ========================================== #
  anything-llm:
    image: mintplexlabs/anythingllm:{{__ANYTHING_LLM_version}}
    container_name: anything-llm
    hostname: anything-llm
    labels:
      com.platys.name: 'anything-llm'
      com.platys.description: "A multi-user ChatGPT for any LLMs and vector database"
      com.platys.webui.title: "Anything LLM UI"
      com.platys.webui.url: "http://dataplatform:28344"
    ports:
      - 28344:3001
    extra_hosts:
      - "host.docker.internal:host-gateway"
    cap_add:
      - SYS_ADMIN
    user: "${UID:-1000}:${GID:-1000}"
    environment:
      STORAGE_DIR: /app/server/storage
      LLM_PROVIDER: ollama
      OLLAMA_BASE_PATH: {{ns.ollamaUrl}}
      OLLAMA_MODEL_PREF: llama3.1
      OLLAMA_MODEL_TOKEN_LIMIT: 4096
      EMBEDDING_ENGINE: ollama
      EMBEDDING_BASE_PATH: {{ns.ollamaUrl}}
      EMBEDDING_MODEL_PREF: nomic-embed-text:latest
      EMBEDDING_MODEL_MAX_CHUNK_LENGTH: 8192
      VECTOR_DB: lancedb
      WHISPER_PROVIDER: local
      TTS_PROVIDER: native
      PASSWORDMINCHAR: 8
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ANYTHING_LLM_volume_map_dotenv %}
      - ./custom-conf/anything-llm/.env:/app/server/.env"
    {% endif %}
    {% if ANYTHING_LLM_volume_map_data %}
      - ./container-volume/anything-llm/storage:/app/server/storage
      - ./container-volume/anything-llm/hotdir:/app/collector/hotdir
      - ./container-volume/anything-llm/outputs:/app/collector/outputs
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# ANYTHING_LLM_enable  #}

{%if PERPLEXICA_enable | default(false) %}
  #  ================================== Perplexica  ========================================== #
  perplexica-backend:
    image: itzcrazykns1337/perplexica-backend:{{__PERPLEXICA_version}}
    container_name: perplexica-backend
    hostname: perplexica-backend
    labels:
      com.platys.name: 'perplexica'
      com.platys.description: "AI-powered search engine"
      com.platys.restapi.title: "searxng API"
      com.platys.restapi.url: "http://dataplatform:28405/api/search"
    ports:
      - 28405:3001
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - searxng
    environment: 
      SEARXNG_API_URL: http://searxng:8080
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/perplexica/config.toml:/home/perplexica/config.toml    
    {% if PERPLEXICA_volume_map_data %}
      - ./container-volume/perplexica/data:/home/perplexica/data
#      - ./container-volume/perplexica/uploads:/home/perplexica/uploads  
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  perplexica-frontend:
    image: itzcrazykns1337/perplexica-frontend:{{__PERPLEXICA_version}}
    container_name: perplexica-frontend
    hostname: perplexica-frontend
    labels:
      com.platys.name: 'perplexica'
      com.platys.description: "AI-powered search engine"
      com.platys.webui.title: "Perplexica UI"
      com.platys.webui.url: "http://dataplatform:28406"      
    ports:
      - 28406:3000
    depends_on:
      - perplexica-backend
    environment: 
      NEXT_PUBLIC_API_URL: http://${PUBLIC_IP}:3001/api
      NEXT_PUBLIC_WS_URL: ws://${PUBLIC_IP}:3001
    {%if use_timezone | default(false) %}    
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}    
{% endif %}   {# PERPLEXICA_enable  #}

{%if BIG_AGI_enable | default(false) %}
  #  ================================== Big-AGI  ========================================== #
  big-agi:
    image: ghcr.io/enricoros/big-agi:{{__BIG_AGI_version}}
    container_name: big-agi
    hostname: big-agi
    labels:
      com.platys.name: 'big-agi'
      com.platys.description: "Personal AI application powered by LLMs"
      com.platys.webui.title: "Big-AGI Rest API"
      com.platys.webui.url: "http://dataplatform:28355"
    ports:
      - 28355:3000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      OLLAMA_API_HOST: {{ns.ollamaUrl}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: [ "next", "start", "-p", "3000" ]
    restart: {{container_restart_policy}}
{% endif %}   {# BIG_AGI_enable  #}

{%if AUTOGEN_STUDIO_enable | default(false) %}
  #  ================================== Autogen Studio  ========================================== #
  autogen-studio:
    image: daxiongmao87/autogen-studio-ui:{{__AUTOGEN_STUDIO_version}}
    container_name: autogen-studio
    hostname: autogen-studio
    labels:
      com.platys.name: 'autogen-studio'
      com.platys.description: "Tool for AI development"
      com.platys.webui.title: "AutoGen Studio UI"
      com.platys.webui.url: "http://dataplatform:28364"
    ports:
      - 28364:8080
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      AUTOGEN_PORT: 8080
      AUTOGEN_HOST: 127.0.0.1
      AUTOGEN_WORKERS: {{AUTOGEN_STUDIO_workers | default(1)}}
      OPENAI_API_KEY: {{AUTOGEN_STUDIO_openai_api_key if AUTOGEN_STUDIO_openai_api_key is defined and AUTOGEN_STUDIO_openai_api_key and AUTOGEN_STUDIO_openai_api_key | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# AUTOGEN_STUDIO_enable  #}

{%if CREWAI_STUDIO_enable | default(false) %}
  #  ================================== CrewAI Studio  ========================================== #
  crewai-studio:
    image: tham0nk/crewai-studio:{{__CREWAI_STUDIO_version}}
    container_name: crewai-studio
    hostname: crewai-studio
    labels:
      com.platys.name: 'crewai-studio'
      com.platys.description: "Tool for AI development"
      com.platys.webui.title: "CrewAI Studio UI"
      com.platys.webui.url: "http://dataplatform:8501"
      com.platys.password.envvars: "PLATYS_CREWAI_STUDIO_AGENTOPS_API_KEY"
    ports:
      - "8501:8501"
    environment:
      AGENTOPS_ENABLED: {{CREWAI_STUDIO_agentops_enabled}}
      AGENTOPS_API_KEY: ${PLATYS_CREWAI_STUDIO_AGENTOPS_API_KEY:-undefined}
    {% if (OLLAMA_enable or external['OLLAMA_enable']) | default(false) %}
      OLLAMA_HOST: {{ns.ollamaUrl}}
      #OLLAMA_MODELS: {{CREWAI_STUDIO_ollama_models}}
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./app/crewai-studio/app:/app
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# CREWAI_STUDIO_enable  #}

{%if AGENT_ZERO_enable | default(false) %}
  #  ================================== Agent Zero  ========================================== #
  agent-zero:
    image: frdel/agent-zero-run:{{__AGENT_ZERO_version}}
    container_name: agent-zero
    hostname: agent-zero
    labels:
      com.platys.name: 'agent-zero'
      com.platys.description: "Agent Zero AI framework "
      com.platys.webui.title: "Agent Zero UI"
      com.platys.webui.url: "http://dataplatform:28418"
    ports:
      - "28418:80"
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    environment:
      WEB_UI_PORT: 80
      WEB_UI_HOST: 0.0.0.0
    {% if (OLLAMA_enable or external['OLLAMA_enable']) | default(false) %}
      OLLAMA_BASE_URL: {{ns.ollamaUrl}}
    {% endif -%}   {#  OLLAMA_enable #}
    {% if ( external['OPENAI_enable']) | default(false) %}
      API_KEY_OPENAI: external['OPENAI_api_key'])
    {% endif -%}   {#  OPENAI_enable #}
    {% if ( external['GROQ_enable']) | default(false) %}
      API_KEY_GROQ: external['GROQ_api_key'])
    {% endif -%}   {#  GROQ_enable #}
    {% if ( external['OPENROUTER_enable']) | default(false) %}
      API_KEY_OPENROUTER: external['OPENROUTER_api_key'])
    {% endif -%}   {#  GROQ_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if AGENT_ZERO_volume_map_data %}
      - ./containter-volume/agent-zero:/a0
    {% endif -%}   {#  AGENT_ZERO_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# AGENT_ZERO_enable  #}

{%if LITELLM_enable | default(false) %}
  #  ================================== LiteLLM  ========================================== #
  litellm:
    image: ghcr.io/berriai/litellm:{{__LITELLM_version}}
    container_name: litellm
    hostname: litellm
    labels:
      com.platys.name: 'litellm'
      com.platys.description: "Proxy for calling all LLM APIs using OpenAI format"
      com.platys.webui.title: "LiteLLM UI"
      com.platys.webui.url: "http://dataplatform:4002/ui"
      com.platys.restapi.title: "LiteLLM API"
      com.platys.restapi.url: "http://dataplatform:4002"
    ports:
      - 4002:4000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      LITELLM_MASTER_KEY: sk-1234
      LITELLM_LOG: info
      DISABLE_ADMIN_UI: false
      UI_USERNAME: litellm
      UI_PASSWORD: abc123!
      DATABASE_URL: "postgresql://{{LITELLM_db_user}}:{{LITELLM_db_password}}@postgresql:5432/{{LITELLM_db_name}}"
      STORE_MODEL_IN_DB: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if LITELLM_use_custom_config | default(false)%}  
      - ./custom-conf/litellm/config.yaml:/app/config.yaml
    {% endif -%}   {#  LITELLM_use_custom_config #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: [{{'"--config", "/app/config.yaml",' if LITELLM_use_custom_config}}"--port", "4000", "--num_workers", "8"]
    restart: {{container_restart_policy}}
{% endif %}   {# LITELLM_enable  #}

{%if FLOWISE_enable | default(false) %}
  #  ================================== Flowise AI  ========================================== #
  flowise:
    image: flowiseai/flowise:{{__FLOWISE_version}}
    container_name: flowise
    hostname: flowise
    labels:
      com.platys.name: 'flowise'
      com.platys.description: "Low-Code Tool to build cusotmized LLM agents"
      com.platys.webui.title: "Flowise UI"
      com.platys.webui.url: "http://dataplatform:28340"
    ports:
      - 28340:3000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      PORT: 3000
      CORS_ORIGINS: "*"
      IFRAME_ORIGINS: "*"
      FLOWISE_USERNAME: {{FLOWISE_username if FLOWISE_username is defined and FLOWISE_username and FLOWISE_username | length else omit}}
      FLOWISE_PASSWORD: {{FLOWISE_password if FLOWISE_password is defined and FLOWISE_password and FLOWISE_password | length else omit}}
      FLOWISE_FILE_SIZE_LIMIT: 50mb
      DEBUG: {{FLOWISE_debug | default(false) }}
      DATABASE_TYPE: {{FLOWISE_database_type}}
  {% if (FLOWISE_database_type | lower) == 'mysql' %}
      DATABASE_PORT: 3306
      DATABASE_HOST: mysql
      DATABASE_NAME: {{FLOWISE_database_name}}
      DATABASE_USER: {{FLOWISE_database_user}}
      DATABASE_PASSWORD: {{FLOWISE_database_password}}
  {% endif -%}   {#  FLOWISE_database_type == mysql #}
  {% if (FLOWISE_database_type | lower) == 'postgres' %}
      DATABASE_PORT: 5432
      DATABASE_HOST: postgresql
      DATABASE_NAME: {{FLOWISE_database_name}}
      DATABASE_USER: {{FLOWISE_database_user}}
      DATABASE_PASSWORD: {{FLOWISE_database_password}}
  {% endif -%}   {#  FLOWISE_database_type == postgres #}
  {% if FLOWISE_langchain_tracing_v2 %}
      LANGCHAIN_TRACING_V2: {{FLOWISE_langchain_tracing_v2}}
      LANGCHAIN_ENDPOINT: {{FLOWISE_langchain_endpoint}}
      LANGCHAIN_API_KEY: {{FLOWISE_langchain_api_key}}
      LANGCHAIN_PROJECT: {{FLOWISE_langchain_project}}
  {% endif -%}   {#  FLOWISE_langchain_tracing_v2 #}
      # DATABASE_SSL:
      # DATABASE_SSL_KEY_BASE64:
      # FLOWISE_SECRETKEY_OVERWRITE=${FLOWISE_SECRETKEY_OVERWRITE}
      LOG_LEVEL: {{FLOWISE_log_level | default(info) }}
      DISABLE_FLOWISE_TELEMETRY: {{not FLOWISE_enable_telemetry}}
    {% if FLOWISE_volume_map_data %}
      DATABASE_PATH: /root/.flowise
      APIKEY_PATH: /root/.flowise
      SECRETKEY_PATH: /root/.flowise
      LOG_PATH: /root/.flowise/logs
      BLOB_STORAGE_PATH: /root/.flowise/storage
    {% endif -%}   {#  FLOWISE_volume_map_data #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if FLOWISE_volume_map_data %}
      - ./container-volume/flowise/data:/root/.flowise
    {% endif -%}   {#  FLOWISE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: /bin/sh -c "sleep 3; flowise start"
    restart: {{container_restart_policy}}
{% endif %}   {# FLOWISE_enable  #}

{%if LANGFLOW_enable | default(false) %}
  #  ================================== Langflow  ========================================== #
  langflow:
    image: langflowai/langflow:{{__LANGFLOW_version}}
    platform: linux/amd64    
    container_name: langflow
    hostname: langflow
    labels:
      com.platys.name: 'langflow'
      com.platys.description: 'visual way to build, iterate and deploy AI apps'
      com.platys.webui.title: 'Langflow UI'
      com.platys.webui.url: "http://dataplatform:7860"
      com.platys.password.envvars: "PLATYS_LANGFLOW_SUPERUSER_PASSWORD"      
    ports:
      - "7860:7860"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
    {% if (LANGFLOW_database_type | lower) == 'sqlite' %}
      LANGFLOW_DATABASE_URL: sqlite:///./langflow.db
    {% endif -%}   {#  LANGFLOW_database_type #}
    {% if (LANGFLOW_database_type | lower) == 'postgres' %}
      LANGFLOW_DATABASE_URL: postgresql://{{LANGFLOW_database_user}}:{{LANGFLOW_database_password}}@postgresql:5432/{{LANGFLOW_database_name}}
    {% endif -%}   {#  LANGFLOW_database_type #}
      LANGFLOW_CONFIG_DIR: /app/langflow
      LANGFLOW_COMPONENTS_PATH: /tmp/langflow/components
    {% if LANGFLOW_auth_enable | default(false) %}  
      LANGFLOW_AUTO_LOGIN: false
      LANGFLOW_SUPERUSER: {{LANGFLOW_superuser_username}}
      LANGFLOW_SUPERUSER_PASSWORD: ${PLATYS_LANGFLOW_SUPERUSER_PASSWORD:-{{LANGFLOW_superuser_password}}}
    {% else %}  
      LANGFLOW_LOAD_FLOWS_PATH: /tmp/langflow/flows
    {% endif -%}  {#  LANGFLOW_auth_enable #}
      LANGFLOW_SECRET_KEY: {{LANGFLOW_secret_key | upper if LANGFLOW_secret_key is defined and LANGFLOW_secret_key and LANGFLOW_secret_key | length else omit}}
      LANGFLOW_NEW_USER_IS_ACTIVE: true
      LANGFLOW_BACKEND_ONLY: {{LANGFLOW_backend_only | default(false)}}
      LANGFLOW_LOG_LEVEL: {{LANGFLOW_log_level | upper if LANGFLOW_log_level is defined and LANGFLOW_log_level and LANGFLOW_log_level | length else omit}}
      LANGFLOW_PROMETHEUS_ENABLED: {{jmx_monitoring_with_prometheus_enable | default(false)}}
      LANGFLOW_PROMETHEUS_PORT: 9090
      LANGFLOW_STORE: {{LANGFLOW_store_enabled | default(false)}}
      LANGFLOW_STORE_ENVIRONMENT_VARIABLES: true
    {% if LANGWATCH_enable | default(false) %}
      LANGWATCH_ENDPOINT: http://langwatch-app:5560
    {% endif -%}   {#  LANGWATCH_enable #}
    {% if LANGFUSE_enable or external['LANGFUSE_enable'] | default(false) %}
      LANGFUSE_SECRET_KEY={{external['LANGFUSE_secret_key'] if external['LANGFUSE_enable'] else omit}}
      LANGFUSE_PUBLIC_KEY={{external['LANGFUSE_public_key'] if external['LANGFUSE_enable'] else omit}}
      LANGFUSE_HOST={{external['LANGFUSE_url'] if external['LANGFUSE_enable'] else "http://langfuse:3000"}}
    {% endif -%}   {#  LANGWATCH_enable #}
    {%if external['LANGSMITH_enable'] | default(false) %}
      LANGCHAIN_API_KEY: {{LANGSMITH_api_key}}
      LANGCHAIN_PROJECT: {{LANGSMITH_project}} 
    {% endif -%}   {#  LANGSMITH_enable #}   
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/langflow:/tmp/langflow/flows
    {% if LANGFLOW_volume_map_components %}  
      - ./plugins/langflow:/tmp/langflow/components      
    {% endif -%}   {#  LANGFLOW_volume_map_components #}
    {% if LANGFLOW_volume_map_data %}
      - ./container-volume/langflow/data:/app/langflow
    {% endif -%}   {#  FLOWISE_volLANGFLOW_volume_map_dataume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# LANGFLOW_enable  #}

{%if LANGFUSE_enable | default(false) %}
  #  ================================== Langfuse  ========================================== #
  langfuse:
    image: ghcr.io/langfuse/langfuse:{{__LANGFUSE_version}}
    container_name: langfuse
    hostname: langfuse
    labels:
      com.platys.name: 'langfuse'
      com.platys.description: 'LLM engineering platform'
      com.platys.webui.title: 'Langfuse UI'
      com.platys.webui.url: "http://dataplatform:28366"
      com.platys.restapi.title: 'Langfuse REST API'
      com.platys.restapi.url: "http://dataplatform:28366/api/public/health"
    ports:
      - "28366:3000"
    environment:
    {% if (LANGFUSE_database_type | lower) == 'postgres' %}
      DATABASE_URL: postgresql://{{LANGFUSE_database_user}}:{{LANGFUSE_database_password}}@postgresql:5432/{{LANGFUSE_database_name}}
    {% endif -%}   {#  LANGFUSE_database_type #}
      NEXTAUTH_SECRET: abc123!
      SALT: abc123!
      NEXTAUTH_URL: http://${PUBLIC_IP}:28366
      TELEMETRY_ENABLED: {{LANGFUSE_enable_telemetry}}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: {{LANGFUSE_enable_experimental_features}}
      AUTH_DISABLE_USERNAME_PASSWORD: false
      AUTH_DISABLE_SIGNUP: false
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# LANGFUSE_enable  #}

{%if OPENLIT_enable | default(false) %}
  #  ================================== OpenLIT  ========================================== #
  openlit:
    image: ghcr.io/openlit/openlit:{{__OPENLIT_version}}
    container_name: openlit
    hostname: openlit
    labels:
      com.platys.name: 'openlit'
      com.platys.description: 'LLM engineering platform'
      com.platys.webui.title: 'OpenLIT UI'
      com.platys.webui.url: "http://dataplatform:28242"
      com.platys.restapi.title: 'OpenLIT REST API'
      com.platys.restapi.url: "http://dataplatform:28242/api/"
    depends_on:
      - clickhouse
    ports:
      - "28242:3000"
    environment:
      TELEMETRY_ENABLED: true
      INIT_DB_HOST: clickhouse
      INIT_DB_PORT: 8123
      INIT_DB_DATABASE: openlit
      INIT_DB_USERNAME: openlit
      INIT_DB_PASSWORD: abc123!
      SQLITE_DATABASE_URL: file:/app/client/data/data.db
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if OPENLIT_volume_map_data | default(false) %}  
      - ./container-volume/openlit/data:/app/client/data
    {% endif -%}   {#  OPENLIT_volume_map_data #}      
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# OPENLIT_enable  #}

{%if LANGWATCH_enable | default(false) %}
  #  ================================== LangWatch  ========================================== #
  langwatch-app:
    image: langwatch/langwatch:{{__LANGWATCH_version}}
    container_name: langwatch-app
    hostname: langwatch-app
    labels:
      com.platys.name: 'langwatch'
      com.platys.description: 'LLMOps platform'
      com.platys.webui.title: 'LangWatch UI'
      com.platys.webui.url: "http://dataplatform:5560"
    depends_on:
      langwatch-postgresql:
        condition: service_healthy
      langwatch-redis:
        condition: service_healthy
      langwatch-opensearch:
        condition: service_healthy      
    ports:
      - "5560:5560"
    environment:
      SKIP_ENV_VALIDATION: true
      DISABLE_PII_REDACTION: true
      DATABASE_URL: postgresql://prisma:prisma@langwatch-postgresql:5432/mydb?schema=mydb
      ELASTICSEARCH_NODE_URL: http://langwatch-opensearch:9200
      IS_OPENSEARCH: true
      REDIS_URL: redis://langwatch-redis:6379
      LANGWATCH_NLP_SERVICE: http://langwatch-nlp:8080
      LANGEVALS_ENDPOINT: http://langevals:8000
      # taken from .env file
      NODE_ENV: "development"
      BASE_HOST: "http://langwatch-app:5560"
      NEXTAUTH_URL: "http://langwatch-app:5560"
      DEBUG: langwatch:*
      NEXTAUTH_PROVIDER: "email"
      NEXTAUTH_SECRET: "N+u6ngvcoDceOWhMFBmZ6lYFTJSW9QPISxIuw6t4tNg="
      API_TOKEN_JWT_SECRET: "2w+3wX8t//xbcSrNVXLfjM2Ot/Jbrtk1AZa8OYpTk6U="
      GOOGLE_APPLICATION_CREDENTIALS: 
      SENDGRID_API_KEY: 
      OPENAI_API_KEY: 
      GROQ_API_KEY:
   {%if use_timezone | default(false) %} 
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  langwatch-nlp:
    image: langwatch/langwatch_nlp:{{__LANGWATCH_version}}
    container_name: langwatch-nlp
    hostname: langwatch-nlp
    labels:
      com.platys.name: 'langwatch'
      com.platys.description: 'LLMOps platform'
      com.platys.restapi.title: 'LangWatch NLP REST API'
      com.platys.restapi.url: "http://dataplatform:28417/docs"      
    ports:
      - "28416:8080"
    environment:
      LANGWATCH_ENDPOINT: http://langwatch-app:5560
      # taken from .env file
      NODE_ENV: "development"
      BASE_HOST: "http://langwatch-app:5560"
      NEXTAUTH_URL: "http://langwatch-app:5560"
      DEBUG: langwatch:*
      NEXTAUTH_PROVIDER: "email"
      NEXTAUTH_SECRET: "N+u6ngvcoDceOWhMFBmZ6lYFTJSW9QPISxIuw6t4tNg="
      API_TOKEN_JWT_SECRET: "2w+3wX8t//xbcSrNVXLfjM2Ot/Jbrtk1AZa8OYpTk6U="
      GOOGLE_APPLICATION_CREDENTIALS: 
      SENDGRID_API_KEY: 
      OPENAI_API_KEY: 
      GROQ_API_KEY:      
    {%if use_timezone | default(false) %} 
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  langwatch-postgresql:
    image: postgres:16
    container_name: langwatch-postgresql
    hostname: langwatch-postgresql
    labels:
      com.platys.name: 'postgresql'
      com.platys.description: 'LangWatch Postgresql'
    ports:
      - "5438:5432"    
    environment:
      POSTGRES_DB: mydb
      POSTGRES_USER: prisma
      POSTGRES_PASSWORD: prisma
    {%if use_timezone | default(false) %} 
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
      - ./data-transfer:/data-transfer    
    {%if LANGWATCH_volume_map_data | default(false) %} 
      - ./container-volume/langwatch/postgresql-data:/var/lib/postgresql/data
    {% endif -%}   {#  LANGWATCH_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}      
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  langwatch-redis:
    image: redis:alpine
    container_name: langwatch-redis
    hostname: langwatch-redis
    labels:
      com.platys.name: 'redis'
      com.platys.description: 'LangWatch Redis'    
    ports:
      - "6388:6379"
    volumes:
      - ./data-transfer:/data-transfer    
    {%if LANGWATCH_volume_map_data | default(false) %} 
      - ./container-volume/langwatch/redis-data:/data
    {% endif -%}   {#  LANGWATCH_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}    
    restart: {{container_restart_policy}}         
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  langwatch-opensearch:
    image: opensearchproject/opensearch:2.17.1
    container_name: langwatch-opensearch
    hostname: langwatch-opensearch
    labels:
      com.platys.name: 'opensearch'
      com.platys.description: 'LangWatch Opensearch'        
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536    
    ports:
      - "9201:9200"
      - "9601:9600"
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      # Disable specific plugins
      - "plugins.anomaly_detection.enabled=false"
      - "plugins.flow_framework.enabled=false"
      - "plugins.security_analytics.ioc_finding_enabled=false"
      - "plugins.sql.enabled=false"
      - "plugins.rollup.enabled=false"
      # Performance settings
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m -XX:+UseG1GC -XX:-UseSerialGC -XX:G1ReservePercent=25 -XX:+AlwaysPreTouch -XX:InitiatingHeapOccupancyPercent=30"
      - "cluster.routing.allocation.disk.threshold_enabled=false"
      - "bootstrap.memory_lock=false" # Changed from true to avoid memory locking warnings
      # Disk settings
      - "cluster.routing.allocation.disk.watermark.low=95%"
      - "cluster.routing.allocation.disk.watermark.high=96%"
      - "cluster.routing.allocation.disk.watermark.flood_stage=97%"
      - "cluster.info.update.interval=1m"
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=Some-random-password-which-is-greater-than-16-chars-long~
      # System limits
      - "node.store.allow_mmap=false"
    {%if use_timezone | default(false) %} 
      TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
      - ./data-transfer:/data-transfer        
    {%if LANGWATCH_volume_map_data | default(false) %} 
      - ./container-volume/langwatch/opensearch-data:/usr/share/opensearch/data
    {% endif -%}   {#  LANGWATCH_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}    
    restart: {{container_restart_policy}}       
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200 || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 40s

{% endif %}   {# LANGWATCH_enable  #}

{%if LANGEVALS_enable | default(false) %}
  #  ================================== LangEvals  ========================================== #
  langevals:
    image: langwatch/langevals:{{__LANGEVALS_version}}
    container_name: langevals
    hostname: langevals
    labels:
      com.platys.name: 'langevals'
      com.platys.description: 'LLM engineering platform'
      com.platys.restapi.title: 'Langfuse REST API'
      com.platys.restapi.url: "http://dataplatform:28417/docs"
    ports:
      - "28417:8000"
    environment:
      DISABLE_EVALUATORS_PRELOAD: true
   {%if use_timezone | default(false) %} 
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# LANGEVALS_enable  #}

{%if ARIZE_PHOENIX_enable | default(false) %}
  #  ================================== Arize Phoenix  ========================================== #
  arize-phoenix:
    image: arizephoenix/phoenix:{{__ARIZE_PHOENIX_version}}
    container_name: arize-phoenix
    hostname: arize-phoenix
    labels:
      com.platys.name: 'arize-phoenix'
      com.platys.description: 'LLM engineering platform'
      com.platys.webui.title: 'Airze Phoneix UI'
      com.platys.webui.url: "http://dataplatform:6006"
    ports:
      - "6006:6006"
      - "4318:4317"
    environment:
      PHOENIX_SQL_DATABASE_URL: postgresql://postgres:abc123!@postgresql:5432/postgres
   {%if use_timezone | default(false) %} 
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# ARIZE_PHOENIX_enable  #}

{%if GPT_RESEARCHER_enable | default(false) %}
  #  ================================== GPT Researcher  ========================================== #
  gpt-researcher:
    image: gpt-researcher/gpt-researcher:{{__GPT_RESEARCHER_version}}
    container_name: gpt-researcher
    hostname: gpt-researcher
    labels:
      com.platys.name: 'gpt-researcher'
      com.platys.description: "Low-Code Tool to build cusotmized LLM agents"
      com.platys.webui.title: "GPT Resarcher UI"
      com.platys.webui.url: "http://dataplatform:28353"
    ports:
      - 28353:8000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      LLM_PROVIDER: {{GPT_RESEARCHER_llm_provider if GPT_RESEARCHER_llm_provider is defined and GPT_RESEARCHER_llm_provider and GPT_RESEARCHER_llm_provider | length else omit}}
      EMBEDDING_PROVIDER: {{GPT_RESEARCHER_embedding_provider if GPT_RESEARCHER_embedding_provider is defined and GPT_RESEARCHER_embedding_provider and GPT_RESEARCHER_embedding_provider | length else omit}}
      FAST_LLM_MODEL: {{GPT_RESEARCHER_fast_llm_model if GPT_RESEARCHER_fast_llm_model is defined and GPT_RESEARCHER_fast_llm_model and GPT_RESEARCHER_fast_llm_model | length else omit}}
      SMART_LLM_MODEL: {{GPT_RESEARCHER_smart_llm_model if GPT_RESEARCHER_smart_llm_model is defined and GPT_RESEARCHER_smart_llm_model and GPT_RESEARCHER_smart_llm_model | length else omit}}
      EMBEDDING_MODEL: {{GPT_RESEARCHER_embedding_model if GPT_RESEARCHER_embedding_model is defined and GPT_RESEARCHER_embedding_model and GPT_RESEARCHER_embedding_model | length else omit}}
      TEMPERATURE: {{GPT_RESEARCHER_temperature if GPT_RESEARCHER_temperature is defined and GPT_RESEARCHER_temperature and GPT_RESEARCHER_temperature | length else omit}}
    {% if GPT_RESEARCHER_llm_provider | lower == 'ollama' | default(false) %}
      OLLAMA_BASE_URL: {{ns.ollamaUrl}}
    {% elif GPT_RESEARCHER_llm_provider | lower == 'openai' | default(false) %}
      OPENAI_API_KEY: none
    {% elif GPT_RESEARCHER_llm_provider | lower == 'groq' | default(false) %}
      GROQ_API_KEY: none
    {% endif %}

      RETRIEVER: {{GPT_RESEARCHER_retriever if GPT_RESEARCHER_retriever is defined and GPT_RESEARCHER_retriever and GPT_RESEARCHER_retriever | length else omit}}
    {% if GPT_RESEARCHER_retriever == 'tavily' or '' | default(false) %}
      TAVILY_API_KEY: {{GPT_RESEARCHER_retriever_api_key}}
    {% elif GPT_RESEARCHER_retriever == 'google' | default(false) %}
      GOOGLE_API_KEY: {{GPT_RESEARCHER_retriever_api_key}}
      GOOGLE_CX_KEY: {{GPT_RESEARCHER_retriever_api_key2}}
    {% elif GPT_RESEARCHER_retriever == 'searx' | default(false) %}
      SEARX_URL: {{GPT_RESEARCHER_retriever_url}}
    {% elif GPT_RESEARCHER_retriever == 'serpapi' | default(false) %}
      SERPAPI_API_KEY: {{GPT_RESEARCHER_retriever_api_key}}
    {% elif GPT_RESEARCHER_retriever == 'googleSerp' | default(false) %}
      SERPER_API_KEY: {{GPT_RESEARCHER_retriever_api_key}}
    {% elif GPT_RESEARCHER_retriever == 'bing' | default(false) %}
      BING_API_KEY: {{GPT_RESEARCHER_retriever_api_key}}
    {% endif %}

      LANGCHAIN_API_KEY: {{GPT_RESEARCHER_langchain_api_key if GPT_RESEARCHER_langchain_api_key is defined and GPT_RESEARCHER_langchain_api_key and GPT_RESEARCHER_langchain_api_key | length else omit}}

      DOC_PATH: /data
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  gpt-researcher-nextjs:
    image: gpt-researcher/gptr-nextjs:{{__GPT_RESEARCHER_version}}
    container_name: gpt-researcher-nextjs
    hostname: gpt-researcher-nextjs
    labels:
      com.platys.name: 'gpt-researcher'
      com.platys.description: "Low-Code Tool to build cusotmized LLM agents"
      com.platys.webui.title: "GPT Resarcher UI"
      com.platys.webui.url: "http://dataplatform:3007"
    ports:
      - 3007:3000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}    
{% endif %}   {# GPT_RESEARCHER_enable  #}

{%if LOCAL_DEEP_RESEARCHER_enable | default(false) %}
  #  ================================== Local Deep Researcher  ========================================== #
  local-deep-researcher:
    image: ghcr.io/gschmutz/local-deep-researcher:{{__LOCAL_DEEP_RESEARCHER_version}}
    container_name: local-deep-researcher
    hostname: local-deep-researcher
    labels:
      com.platys.name: 'local-deep-researcher'
      com.platys.description: "Local web research and report writing assistant"
      com.platys.webui.title: "Local Deep Researcher UI"
      com.platys.webui.url: "https://smith.langchain.com/studio/thread?baseUrl=http://localhost:2024"
    ports:
      - 2024:2024
    extra_hosts:
      - "host.docker.internal:host-gateway"      
    environment:
      - SEARCH_API={{LOCAL_DEEP_RESEARCHER_search_api if LOCAL_DEEP_RESEARCHER_search_api is defined and LOCAL_DEEP_RESEARCHER_search_api and LOCAL_DEEP_RESEARCHER_search_api | length else omit}}
      - TAVILY_API_KEY={{LOCAL_DEEP_RESEARCHER_tavily_api_key if LOCAL_DEEP_RESEARCHER_tavily_api_key is defined and LOCAL_DEEP_RESEARCHER_tavily_api_key and LOCAL_DEEP_RESEARCHER_tavily_api_key | length else omit}} 
      - PERPLEXITY_API_KEY={{LOCAL_DEEP_RESEARCHER_perplexity_api_key if LOCAL_DEEP_RESEARCHER_perplexity_api_key is defined and LOCAL_DEEP_RESEARCHER_perplexity_api_key and LOCAL_DEEP_RESEARCHER_perplexity_api_key | length else omit}}
      - SEARXNG_URL={{ns.searxngUrl if LOCAL_DEEP_RESEARCHER_search_api == 'searxng' else omit}}
      - LLM_PROVIDER={{LOCAL_DEEP_RESEARCHER_llm_provider if LOCAL_DEEP_RESEARCHER_llm_provider is defined and LOCAL_DEEP_RESEARCHER_llm_provider and LOCAL_DEEP_RESEARCHER_llm_provider | length else omit}}
      - LOCAL_LLM={{LOCAL_DEEP_RESEARCHER_model if LOCAL_DEEP_RESEARCHER_model is defined and LOCAL_DEEP_RESEARCHER_model and LOCAL_DEEP_RESEARCHER_model | length else omit}}
      - OLLAMA_BASE_URL={{ns.ollamaUrl}}
      - LMSTUDIO_BASE_URL={{ns.lmstudioUrl}}
      - MAX_WEB_RESEARCH_LOOPS=3
      - FETCH_FULL_PAGE=false
      - LANGSMITH_API_KEY={{external['LANGSMITH_api_key'] if external['LANGSMITH_enable'] is defined and external['LANGSMITH_enable'] else omit}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:2024 || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
{% endif %}   {# LOCAL_DEEP_RESEARCHER_enable  #}

{%if VERBA_enable | default(false) %}
  #  ================================== Verba  ========================================== #
  verba:
    image: trivadis/verba:{{__VERBA_version}}
    container_name: verba
    hostname: verba
    labels:
      com.platys.name: 'verba'
      com.platys.description: "Retrieval Augmented Generation (RAG) chatbot"
      com.platys.webui.title: "Verba UI"
      com.platys.webui.url: "http://dataplatform:28373"
    ports:
      - 28373:8000
    environment:
    {% if WEAVIATE_enable | default(false) %}
      - WEAVIATE_URL_VERBA=http://weaviate:8080
      ## IF YOU WANT AUTHENTICATION, UNCOMENT BELLOW
      ## AND ADD WEAVIATE SERVICE
      #- WEAVIATE_API_KEY_VERBA=adminkey
    {% endif %}
#      - OPENAI_API_KEY=$OPENAI_API_KEY
#      - COHERE_API_KEY=
      - OLLAMA_URL={{ns.ollamaUrl}}
      - OLLAMA_MODEL={{VERBA_ollama_model}}
      - UNSTRUCTURED_API_KEY=${PLATYS_UNSTRUCTURED_API_KEY:-{{VERBA_unstructured_api_key}}}
#      - UNSTRUCTURED_API_URL=ns.unstructuredApiUrl
      - GITHUB_TOKEN={{VERBA_github_token}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if VERBA_volume_map_data %}
      - "./container-volume/verba/data:/data/"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8000 || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
{% endif %}   {# VERBA_enable  #}

{%if RAGFLOW_enable | default(false) %}
  #  ================================== RAGFlow  ========================================== #
  ragflow:
    image: infiniflow/ragflow:{{__RAGFLOW_version}}{{'-slim' if RAGFLOW_edition == 'dev-slim'}}
    platform: linux/amd64                  
    container_name: ragflow
    hostname: ragflow
    labels:
      com.platys.name: 'ragflow'
      com.platys.description: "Retrieval Augmented Generation (RAG) engine"
      com.platys.webui.title: "Ragflow UI"
      com.platys.webui.url: "http://dataplatform:28396"      
    depends_on:
      mysql:
        condition: service_healthy
      elasticsearch-1:
        condition: service_healthy      
      minio-1:
        condition: service_healthy      
    ports:
      - 9380:9380
      - 28396:80
      - 28397:443
      - 28398:5678
    extra_hosts:
      - "host.docker.internal:host-gateway"    
    environment:
      - HF_ENDPOINT={{'https://hf-mirror.com' if RAGFLOW_use_huggingface_mirror else omit}}
      - MACOS={{'1' if RAGFLOW_use_macos_optimization else omit}}
      - DB_TYPE={{RAGFLOW_db_type | default("mysql")}}
      - STORAGE_IMPL=MINIO
      - MYSQL_HOST=mysql
      - MYSQL_USER=ragflow
      - MYSQL_PASSWORD=abc123!
      - MYSQL_DBNAME=ragflow_db
      - DOC_ENGINE={{RAGFLOW_doc_engine_type | default("elasticsearch")}}
    {%if RAGFLOW_doc_engine_type == 'elasticsearch' | default(false) %}
      - ES_HOST=elasticsearch-1
      - ES_USER=elastic
      - ES_PASSWORD=abc123!
    {% elif RAGFLOW_doc_engine_type == 'infinity' | default(false) %}   
      - INFINITY_HOST=infinity
    {% endif -%}   {#  use_timezone #}
      - REDIS_HOST=ragflow-redis
      - REDIS_PASSWORD=${PLATYS_RAGFLOW_REDIS_PASSWORD:-{{RAGFLOW_redis_password}}}
      - MINIO_HOST=minio-1
      - MINIO_USER=admin
      - MINIO_PASSWORD=abc123abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - ./conf/ragflow/service_conf.yaml:/ragflow/conf/service_conf.yaml
      - ./conf/ragflow/nginx/ragflow.conf:/etc/nginx/conf.d/ragflow.conf
      - ./conf/ragflow/nginx/proxy.conf:/etc/nginx/proxy.conf
      - ./conf/ragflow/nginx/nginx.conf:/etc/nginx/nginx.conf
    {% if RAGFLOW_volume_map_logs %}
      - ./container-volume/ragflow/logs:/ragflow/logs
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {% if RAGFLOW_with_gpu | default(false) %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]    
    {% endif -%}   {#  RAGFLOW_with_gpu #}    
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  ragflow-redis:
    image: valkey/valkey:8
    container_name: ragflow-redis
    hostname: ragflow-redis
    labels:
      com.platys.name: 'redis'
      com.platys.description: "Redis key/value store for RAGFlow"  
      com.platys.password.envvars: "PLATYS_RAGFLOW_REDIS_PASSWORD"
    ports:
      - 6387:6379
    {% if RAGFLOW_volume_map_redis_data %}
    volumes:
      - ./container-volume/reagflow/redis-data:/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}    
    command: redis-server --requirepass ${PLATYS_RAGFLOW_REDIS_PASSWORD:-{{RAGFLOW_redis_password}}} --maxmemory 128mb --maxmemory-policy allkeys-lru
    restart: {{container_restart_policy}} 
{% endif %}   {# RAGFLOW_enable  #}

{%if NEO4j_KG_BUILDER_enable | default(false) %}
  #  ================================== Neo4j Knowledge Graph Builder  ========================================== #
  neo4j-kg-builder-backend:
    image: trivadis/llm-graph-builder-backend:{{__NEO4j_KG_BUILDER_version}}                
    container_name: neo4j-kg-builder-backend
    hostname: neo4j-kg-builder-backend
    labels:
      com.platys.name: 'neo4j-kg-builder'
      com.platys.description: "Knowledge Graph Builder"
      com.platys.restapi.title: "Knowledge Graph Builder REST API"
      com.platys.restapi.url: "http://dataplatform:28414"        
    ports:
      - 28414:8000
    extra_hosts:
      - "host.docker.internal:host-gateway"    
    environment:
  {% if NEO4J_enable | default(false) %}    
      - NEO4J_URI=neo4j://${PUBLIC_IP}:7687
      - NEO4J_PASSWORD=${PLATYS_NEO4J_PASSWORD:-{{NEO4J_password}}}
      - NEO4J_USERNAME=neo4j
      - NEO4J_DATABASE=neo4j
  {% endif %}
      - OPENAI_API_KEY={{NEO4J_KG_BUILDER_openai_api_key}}
      - DIFFBOT_API_KEY={{NEO4J_KG_BUILDER_diffbot_api_key}}
      - EMBEDDING_MODEL={{NEO4J_KG_BUILDER_embedding_model}}
      - LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
      - LANGCHAIN_TRACING_V2=false
      - LANGCHAIN_PROJECT=""
      - LANGCHAIN_API_KEY=""
      - KNN_MIN_SCORE={{NEO4J_KG_BUILDER_knn_min_score}}
      - IS_EMBEDDING={{NEO4J_KG_BUILDER_is_embedding}}
      - GEMINI_ENABLED=false
      - GCP_LOG_METRICS_ENABLED=false
      - UPDATE_GRAPH_CHUNKS_PROCESSED=20
      - NUMBER_OF_CHUNKS_TO_COMBINE=6
      - ENTITY_EMBEDDING=false
      - GCS_FILE_CACHE=false
#      - LLM_MODEL_CONFIG_anthropic_claude_35_sonnet=${LLM_MODEL_CONFIG_anthropic_claude_35_sonnet-}
#      - LLM_MODEL_CONFIG_fireworks_llama_v3_70b=${LLM_MODEL_CONFIG_fireworks_llama_v3_70b-}
#      - LLM_MODEL_CONFIG_azure_ai_gpt_4o=${LLM_MODEL_CONFIG_azure_ai_gpt_4o-}
#      - LLM_MODEL_CONFIG_azure_ai_gpt_35=${LLM_MODEL_CONFIG_azure_ai_gpt_35-}
#      - LLM_MODEL_CONFIG_groq_llama3_70b=${LLM_MODEL_CONFIG_groq_llama3_70b-}
#      - LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet=${LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet-}
#     - LLM_MODEL_CONFIG_fireworks_qwen_72b=${LLM_MODEL_CONFIG_fireworks_qwen_72b-}
      - LLM_MODEL_CONFIG_ollama_llama3={{NEO4J_KG_BUILDER_ollama_model}},http://host.docker.internal:11434
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - ./backend:/code
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  neo4j-kg-builder-frontend:
    image: trivadis/llm-graph-builder-frontend:{{__NEO4j_KG_BUILDER_version}}                
    container_name: neo4j-kg-builder-frontend
    hostname: neo4j-kg-builder-frontend
    labels:
      com.platys.name: 'neo4j-kg-builder'
      com.platys.description: "Knowledge Graph Builder"
      com.platys.webui.title: "Knowledge Graph Builder UI"
      com.platys.webui.url: "http://dataplatform:28415"        
    ports:
      - 28415:8080
    extra_hosts:
      - "host.docker.internal:host-gateway"    
    environment:
      - VITE_BACKEND_API_URL="http://${PUBLIC_IP}:8000"
      - VITE_BLOOM_URL="https://workspace-preview.neo4j.io/workspace/explore?connectURL={CONNECT_URL}&search=Show+me+a+graph&featureGenAISuggestions=true&featureGenAISuggestionsInternal=true"
      - VITE_REACT_APP_SOURCES="local,youtube,wiki,s3,web"
      - VITE_LLM_MODELS="diffbot,openai_gpt_3.5,openai_gpt_4o"
      - VITE_ENV="DEV"
      - VITE_TIME_PER_PAGE=50
      - VITE_CHUNK_SIZE=5242880
      - VITE_LARGE_FILE_SIZE=5242880
      - VITE_GOOGLE_CLIENT_ID=""
      - VITE_CHAT_MODES=""
      - VITE_BATCH_SIZE=2
      - VITE_LLM_MODELS_PROD="openai_gpt_4o,openai_gpt_4o_mini,diffbot,gemini_1.5_flash"
      - VITE_FRONTEND_HOSTNAME="${PUBLIC_IP}:8080"
      - VITE_SEGMENT_API_URL=""
   {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./frontend:/app
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# NEO4j_KG_BUILDER_enable  #}

{%if KIE_SERVER_enable | default(false) %}
  #  ================================== Drools KIE Server  ========================================== #
  business-central:
    image: jboss/business-central-workbench:{{__KIE_SERVER_version}}
    container_name: business-central
    hostname: business-central
    labels:
      com.platys.name: 'kie-server'
      com.platys.description: "standalone execution server for rules"
      com.platys.webui.title: "Business Central UI"
      com.platys.webui.url: "http://dataplatform:28236/business-central"
    ports:
      - 28235:8001
      - 28236:8080
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  kie-server:
    image: jboss/kie-server:{{__KIE_SERVER_version}}
    container_name: kie-server
    hostname: kie-server
    labels:
      com.platys.name: 'kie-server'
      com.platys.description: "standalone execution server for rules"
      com.platys.webui.title: "Drools KIE Server Web UI"
      com.platys.webuis.url: "http://dataplatform:28234"
      com.platys.restapi.title: "Drools KIE Server REST API"
      com.platys.restapi.url: "http://dataplatform:28234/kie-server/services/rest/server/"
    ports:
      - "28234:8080"
    environment:
      KIE_SERVER_LOCATION: http://kie-server:8080/kie-server/services/rest/server
      KIE_SERVER_CONTROLLER: http://business-central:8080/business-central/rest/controller
      KIE_MAVEN_REPO: http://business-central:8080/business-central/maven2
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  KIE_SERVER_enable  #}

{% if METABASE_enable | default(false) %}
  #  ================================== Metabase ========================================== #
  metabase:
    image: metabase/metabase:{{__METABASE_version}}
    hostname: metabase
    container_name: metabase
    labels:
      com.platys.name: "metabase"
      com.platys.description: "Simple and powerful analytics tool"
      com.platys.webui.title: "Metabase UI"
      com.platys.webui.url: "http://dataplatform:28257"
    ports:
      - "28257:3000"
    environment:
    {% if (METABASE_db_type | lower) == 'h2' %}
      MB_DB_TYPE: h2
      {% if METABASE_volume_map_data %}
      MB_DB_FILE: /metabase-data/metabase.db
      {% endif -%}   {#  METABASE_volume_map_data #}
    {% endif -%}   {#  METABASE_db_type = h2 #}
    {% if (METABASE_db_type | lower) == 'postgres' %}
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: {{METABASE_postgres_dbname}}
      MB_DB_PORT: 5432
      MB_DB_USER: {{METABASE_postgres_user}}
      MB_DB_PASS: {{METABASE_postgres_password}}
      MB_DB_HOST: postgresql
    {% endif -%}   {#  METABASE_db_type = postgres #}
    {% if (METABASE_db_type | lower) == 'mysql' %}
      MB_DB_TYPE: mysql
      MB_DB_DBNAME: {{METABASE_mysql_dbname}}
      MB_DB_PORT: 3306
      MB_DB_USER: {{METABASE_mysql_user}}
      MB_DB_PASS: {{METABASE_mysql_password}}
      MB_DB_HOST: mysql
    {% endif -%}   {#  METABASE_db_type = postgres #}
      MB_ENABLE_QUERY_CACHING: {{METABASE_query_caching_enabled}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
      JAVA_TIMEZONE: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if METABASE_volume_map_data %}
      - ./container-volume/metabase/data:/metabase-data
    {% endif -%}   {#  METABASE_volume_map_data #}
      - ./plugins/metabase/plugin:/plugins
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  METABASE_enable #}

{% if SUPERSET_enable | default(false) %}
  #  ================================== Superset ========================================== #
  superset:
    image: apache/superset:{{__SUPERSET_version}}
    hostname: superset
    container_name: superset
    labels:
      com.platys.name: "superset"
      com.platys.description: "modern data exploration and visualization platform"
      com.platys.webui.title: "Superset UI"
      com.platys.webui.url: "http://dataplatform:28151"
    ports:
      - "28151:8088"
    user: "root"
    environment:
      - DATABASE_DB=superset
      - DATABASE_HOST=superset-db
      - DATABASE_PASSWORD=superset
      - DATABASE_USER=superset
      - DATABASE_PORT=5432
      - DATABASE_DIALECT=postgresql
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
      - PYTHONPATH=/app/pythonpath:/app/docker/pythonpath_dev
      - REDIS_HOST=superset-redis
      - REDIS_PORT=6379
      - FLASK_ENV=development
      - SUPERSET_ENV=development
      - SUPERSET_LOAD_EXAMPLES={{ 'yes' if (SUPERSET_provision_examples ) else 'no' }}
      - CYPRESS_CONFIG=false
      - SUPERSET_PORT=8088
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/superset:/app/docker
#      - ./container-volume/superset:/app/superset_home
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ["/app/docker/docker-bootstrap.sh", "app"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  superset-init:
    image: apache/superset:{{__SUPERSET_version}}
    hostname: superset-init
    container_name: superset-init
    user: "root"
    environment:
      - DATABASE_DB=superset
      - DATABASE_HOST=superset-db
      - DATABASE_PASSWORD=superset
      - DATABASE_USER=superset
      - DATABASE_PORT=5432
      - DATABASE_DIALECT=postgresql
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
      - PYTHONPATH=/app/pythonpath:/app/docker/pythonpath_dev
      - REDIS_HOST=superset-redis
      - REDIS_PORT=6379
      - FLASK_ENV=development
      - SUPERSET_ENV=development
      - SUPERSET_LOAD_EXAMPLES={{ 'yes' if (SUPERSET_provision_examples ) else 'no' }}
      - CYPRESS_CONFIG=false
      - SUPERSET_PORT=8088
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/superset:/app/docker
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ["/app/docker/docker-init.sh"]
    init: true

  superset-worker:
    image: apache/superset:{{__SUPERSET_version}}
    hostname: superset-worker
    container_name: superset-worker
    labels:
      com.platys.name: "superset"
      com.platys.description: "modern data exploration and visualization platform"
    user: "root"
    environment:
      - DATABASE_DB=superset
      - DATABASE_HOST=superset-db
      - DATABASE_PASSWORD=superset
      - DATABASE_USER=superset
      - DATABASE_PORT=5432
      - DATABASE_DIALECT=postgresql
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
      - PYTHONPATH=/app/pythonpath:/app/docker/pythonpath_dev
      - REDIS_HOST=superset-redis
      - REDIS_PORT=6379
      - FLASK_ENV=development
      - SUPERSET_ENV=development
      - SUPERSET_LOAD_EXAMPLES={{ 'yes' if (SUPERSET_provision_examples ) else 'no' }}
      - CYPRESS_CONFIG=false
      - SUPERSET_PORT=8088
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/superset:/app/docker
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ["/app/docker/docker-bootstrap.sh", "worker"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  superset-worker-beat:
    image: apache/superset:{{__SUPERSET_version}}
    hostname: superset-worker-beat
    container_name: superset-worker-beat
    labels:
      com.platys.name: "superset"
      com.platys.description: "modern data exploration and visualization platform"
    user: "root"
    environment:
      - DATABASE_DB=superset
      - DATABASE_HOST=superset-db
      - DATABASE_PASSWORD=superset
      - DATABASE_USER=superset
      - DATABASE_PORT=5432
      - DATABASE_DIALECT=postgresql
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
      - PYTHONPATH=/app/pythonpath:/app/docker/pythonpath_dev
      - REDIS_HOST=superset-redis
      - REDIS_PORT=6379
      - FLASK_ENV=development
      - SUPERSET_ENV=development
      - SUPERSET_LOAD_EXAMPLES={{ 'yes' if (SUPERSET_provision_examples ) else 'no' }}
      - CYPRESS_CONFIG=false
      - SUPERSET_PORT=8088
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/superset:/app/docker
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ["/app/docker/docker-bootstrap.sh", "beat"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  superset-db:
    image: postgres:10
    hostname: superset-db
    container_name: superset-db
    labels:
      com.platys.name: "superset"
      com.platys.description: "modern data exploration and visualization platform"
    environment:
      - DATABASE_DB=superset
      - DATABASE_HOST=superset-db
      - DATABASE_PASSWORD=superset
      - DATABASE_USER=superset
      - DATABASE_PORT=5432
      - DATABASE_DIALECT=postgresql
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=superset
      - PYTHONPATH=/app/pythonpath:/app/docker/pythonpath_dev
      - REDIS_HOST=superset-redis
      - REDIS_PORT=6379
      - FLASK_ENV=development
      - SUPERSET_ENV=development
      - SUPERSET_LOAD_EXAMPLES={{ 'yes' if (SUPERSET_provision_examples ) else 'no' }}
      - CYPRESS_CONFIG=false
      - SUPERSET_PORT=8088
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  superset-redis:
    image: redis:3.2
    hostname: superset-redis
    container_name: superset-redis
    labels:
      com.platys.name: "superset"
      com.platys.description: "Key-value store"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  SUPERSET_enable #}

{% if REDASH_enable | default(false) %}
  #  ================================== Redash ========================================== #
  redash-server:
    image: redash/redash:{{__REDASH_version}}
    hostname: redash-server
    container_name: redash-server
    labels:
      com.platys.name: "redash"
      com.platys.description: "Build dashboards to visualize data"
      com.platys.webui.title: "Redash UI"
      com.platys.webui.url: "http://dataplatform:28161"
    ports:
      - "28161:5000"
    command: server
    environment:
      REDASH_REDIS_URL: "redis://redash-redis:6379/0"
      REDASH_DATABASE_URL: "postgresql://postgres@redash-postgresql/postgres"
      REDASH_PASSWORD_LOGIN_ENABLED: "true"
      REDASH_RATELIMIT_ENABLED: "false"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  redash-scheduler:
    image: redash/redash:{{__REDASH_version}}
    hostname: redash-scheduler
    container_name: redash-scheduler
    labels:
      com.platys.name: "redash"
      com.platys.description: "Build dashboards to visualize data"
    command: scheduler
    environment:
      REDASH_LOG_LEVEL: "INFO"
      REDASH_REDIS_URL: "redis://redash-redis:6379/0"
      REDASH_DATABASE_URL: "postgresql://postgres@redash-postgresql/postgres"
      REDASH_PASSWORD_LOGIN_ENABLED: "true"
      REDASH_RATELIMIT_ENABLED: "false"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  redash-worker:
    image: redash/redash:{{__REDASH_version}}
    hostname: redash-worker
    container_name: redash-worker
    labels:
      com.platys.name: "redash"
      com.platys.description: "Build dashboards to visualize data"
    command: worker
    environment:
      REDASH_LOG_LEVEL: "INFO"
      REDASH_REDIS_URL: "redis://redash-redis:6379/0"
      REDASH_DATABASE_URL: "postgresql://postgres@redash-postgresql/postgres"
      REDASH_PASSWORD_LOGIN_ENABLED: "true"
      REDASH_RATELIMIT_ENABLED: "false"
      PYTHONUNBUFFERED: 0
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  redash-postgresql:
    image: postgres:9.5-alpine
    hostname: redash-postgresql
    container_name: redash-postgresql
    labels:
      com.platys.name: "redash"
    # The following turns the DB into less durable, but gains significant performance improvements for the tests run (x3
    # improvement on my personal machine). We should consider moving this into a dedicated Docker Compose configuration for
    # tests.
    ports:
      - "15432:5432"
    command: "postgres -c fsync=off -c full_page_writes=off -c synchronous_commit=OFF"
    environment:
      POSTGRES_HOST_AUTH_METHOD: "trust"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  redash-db-setup:
    image: redash/redash:{{__REDASH_version}}
    hostname: redash-db-setup
    container_name: redash-db-setup
    labels:
      com.platys.name: "redash"
    command: create_db
    depends_on:
      - redash-postgresql
    environment:
      REDASH_DATABASE_URL: "postgresql://postgres@redash-postgresql/postgres"
    init: true

  redash-redis:
    image: redis:{{__REDIS_version}}
    hostname: redash-redis
    container_name: redash-redis
    labels:
      com.platys.name: "redash"
    ports:
      - 6385:6379
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  REDASH_enable #}

{% if SMASHING_enable | default(false) %}
  #  ================================== Smashing ========================================== #
  smashing:
    image: trivadis/smashing:{{__SMASHING_version}}
    hostname: smashing
    container_name: smashing
    labels:
      com.platys.name: "smashing"
      com.platys.description: "Simple dashboard framework"
      com.platys.webui.title: "Smashing UI"
      com.platys.webui.url: "http://dataplatform:28171"
    ports:
      - 28171:3030
    environment:
      WIDGETS: {{SMASHING_install_widgets}}
      GEMS: {{SMASHING_install_gems}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if SMASHING_volume_map_dashboards %}
      - ./scripts/smashing/dashboards:/dashboards
    {% endif -%}   {#  SMASHING_volume_map_dashboards #}
    {% if SMASHING_volume_map_jobs %}
      - ./scripts/smashing/jobs:/jobs
    {% endif -%}   {#  SMASHING_volume_map_jobs #}
    {% if SMASHING_volume_map_widgets %}
      - ./plugins/smashing/widgets:/widgets
    {% endif -%}   {#  SMASHING_volume_map_widgets #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SMASHING_enable #}

{% if TIPBOARD_enable | default(false) %}
  #  ================================== Tipboard Dashboard ========================================== #
  tipboard:
    image: trivadis/tipboard:{{__TIPBOARD_version}}
    hostname: tipboard
    container_name: tipboard
    labels:
      com.platys.name: "tipboard"
      com.platys.description: "Simple dashboard framework"
      com.platys.webui.title: "Tipboard UI"
      com.platys.webui.url: "http://dataplatform:28172"
    ports:
      - 28172:7272
    environment:
      TIPBOARD_redis_host: "'{{TIPBOARD_redis_host}}'"
      TIPBOARD_redis_port: {{TIPBOARD_redis_port}}
      {%if TIPBOARD_redis_password != '' %}
      TIPBOARD_redis_password: "'{{TIPBOARD_redis_password}}'"
      {% endif -%}   {#  TIPBOARD_redis_password #}
      TIPBOARD_redis_db: {{TIPBOARD_redis_db}}
      TIPBOARD_debug: "'False'"
      TIPBOARD_api_key: "'{{TIPBOARD_api_key}}'"
      TIPBOARD_host: "'localhost'"
      TIPBOARD_port: {{TIPBOARD_port}}
      TIPBOARD_project_name: "'{{TIPBOARD_project_name}}'"
      TIPBOARD_flipboard_interval: {{TIPBOARD_flipboard_interval}}
      {%if TIPBOARD_flipboard_sequence != '' %}
      TIPBOARD_flipboard_sequence: "'[{{TIPBOARD_flipboard_sequence}}]'"
      {% endif -%}   {#  TIPBOARD_flipboard_sqeuence #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if TIPBOARD_volume_map_dashboards %}
      - ./scripts/tipboard/dashboards:/dashboards
    {% endif -%}   {#  TIPBOARD_volume_map_dashboards #}
      - ./plugins/tipboard/custom_tiles:/root/.tipboard/custom_tiles
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  TIPBOARD_enable #}

{% if CHARTBOARD_enable | default(false) %}
  #  ================================== Chartboard Dashboard ========================================== #
  chartboard:
    image: trivadis/chartboard:{{__CHARTBOARD_version}}
    hostname: chartboard
    container_name: chartboard
    labels:
      com.platys.name: "chartboard"
      com.platys.description: "Simple dashboard framework"
      com.platys.webui.title: "Chartboard UI"
      com.platys.webui.url: "http://dataplatform:28173"
    ports:
      - 28173:8080
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if CHARTBOARD_volume_map_dashboards %}
      - ./scripts/chartboard:/dashboards
    {% endif -%}   {#  TIPBOARD_volume_map_dashboards #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CHARTBOARD_enable #}

{% if RETOOL_enable | default(false) %}
  #  ================================== Retool ========================================== #
  retool-api:
    image: tryretool/backend:{{__RETOOL_version}}
    hostname: retool-api
    container_name: retool-api
    labels:
      com.platys.name: "retool"
      com.platys.description: "Low-Code Application development"
      com.platys.webui.title: "ReTool UI"
      com.platys.webui.url: "http://dataplatform:3006"
    ports:
      - 3006:3000
    environment:
      SERVICE_TYPE: MAIN_BACKEND
      DB_CONNECTOR_HOST: http://retool-db-connector
      DB_CONNECTOR_PORT: 3002
      DB_SSH_CONNECTOR_HOST: http://retool-db-ssh-connector
      DB_SSH_CONNECTOR_PORT: 3002
      NODE_ENV: production
      JWT_SECRET: abc123!
      POSTGRES_DB: retooldb
      POSTGRES_USER: retool
      POSTGRES_HOST: postgresql
      POSTGRES_PORT: 5432
      POSTGRES_PASSWORD: abc123!
      ENCRYPTION_KEY: abc123!
      LICENSE_KEY: LOCAL-ONLY-TRIAL
      COOKIE_INSECURE: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: bash -c "/usr/src/app/wait-for-it.sh postgresql:5432; ./docker_scripts/start_api.sh"
    restart: {{container_restart_policy}}

  retool-jobs-runner:
    image: tryretool/backend:{{__RETOOL_version}}
    hostname: retool-jobs-runner
    container_name: retool-jobs-runner
    labels:
      com.platys.name: "retool"
    environment:
      SERVICE_TYPE: JOBS_RUNNER
      NODE_ENV: production
      JWT_SECRET: abc123!
      POSTGRES_DB: retooldb
      POSTGRES_USER: retool
      POSTGRES_HOST: postgresql
      POSTGRES_PORT: 5432
      POSTGRES_PASSWORD: abc123!
      ENCRYPTION_KEY: abc123!
      LICENSE_KEY: LOCAL-ONLY-TRIAL
      COOKIE_INSECURE: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: bash -c "chmod -R +x ./docker_scripts; sync; /usr/src/app/wait-for-it.sh postgresql:5432; ./docker_scripts/start_api.sh"
    restart: {{container_restart_policy}}

  retool-db-connector:
    image: tryretool/backend:{{__RETOOL_version}}
    hostname: retool-db-connector
    container_name: retool-db-connector
    labels:
      com.platys.name: "retool"
    environment:
      SERVICE_TYPE: DB_CONNECTOR_SERVICE
      NODE_ENV: production
      JWT_SECRET: abc123!
      POSTGRES_DB: retooldb
      POSTGRES_USER: retool
      POSTGRES_HOST: postgresql
      POSTGRES_PORT: 5432
      POSTGRES_PASSWORD: abc123!
      ENCRYPTION_KEY: abc123!
      LICENSE_KEY: LOCAL-ONLY-TRIAL
      COOKIE_INSECURE: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: bash -c "./retool_backend"
    restart: {{container_restart_policy}}

  retool-db-ssh-connector:
    image: tryretool/backend:{{__RETOOL_version}}
    hostname: retool-db-ssh-connector
    container_name: retool-db-ssh-connector
    labels:
      com.platys.name: "retool"
    environment:
      SERVICE_TYPE: DB_SSH_CONNECTOR_SERVICE
      NODE_ENV: production
      JWT_SECRET: abc123!
      POSTGRES_DB: retooldb
      POSTGRES_USER: retool
      POSTGRES_HOST: postgresql
      POSTGRES_PORT: 5432
      POSTGRES_PASSWORD: abc123!
      ENCRYPTION_KEY: abc123!
      LICENSE_KEY: LOCAL-ONLY-TRIAL
      COOKIE_INSECURE: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: bash -c "./docker_scripts/generate_key_pair.sh; ./retool_backend"
    restart: {{container_restart_policy}}
{% endif %}   {#  RETOOL_enable #}

{% if TOOLJET_enable | default(false) %}
  #  ================================== ToolJet ========================================== #
  tooljet:
    image: tooljet/tooljet:{{__BASEROW_version}}
    hostname: tooljet
    container_name: tooljet
    labels:
      com.platys.name: "tooljet"
      com.platys.description: "Low-Code Application development"
      com.platys.webui.title: "ToolJet UI"
      com.platys.webui.url: "http://dataplatform:28334"
    ports:
      - "28334:80"
    tty: true
    stdin_open: true
    environment:
      SERVE_CLIENT: "true"
      PORT: "80"
      TOOLJET_HOST: http://localhost:80
      #LOCKBOX_MASTER_KEY: # replace_with_lockbox_master_key
      #SECRET_KEY_BASE: # replace_with_secret_key_base
      ORM_LOGGING: all
      PG_DB: demodb
      PG_USER: demo
      PG_HOST: postgresql
      PG_PASS: abc123!
      # TOOLJET DATABASE
      ENABLE_TOOLJET_DB: false
      #TOOLJET_DB: tooljet_db
      #TOOLJET_DB_USER: postgres
      #TOOLJET_DB_HOST: postgresql
      #TOOLJET_DB_PASS:

      #PGRST_DB_URI: # postgres://<postgres_username>:<postgres_password><@postgres_hostname>/<database_name>
      #PGRST_HOST: postgrest
      #PGRST_JWT_SECRET: # If you have openssl installed, you can run the following command openssl rand -hex 32 to generate the value for PGRST_JWT_SECRET.
      REDIS_HOST: redis-1
      REDIS_PORT: 6379
      REDIS_USER: default
      REDIS_PASSWORD: abc123!
      DISABLE_TOOLJET_TELEMETRY: true
      DEFAULT_FROM_EMAIL: hello@tooljet.io
      #SMTP_USERNAME:
      #SMTP_PASSWORD:
      #SMTP_DOMAIN:
      #SMTP_PORT:
      COMMENT_FEATURE_ENABLE: false
      ENABLE_MULTIPLAYER_EDITING: true
      ENABLE_MARKETPLACE_FEATURE: true
      USER_SESSION_EXPIRY: 2880
      DEPLOYMENT_PLATFORM: docker
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: npm run start:prod
    restart: {{container_restart_policy}}
{% endif %}   {#  TOOLJET_enable #}

{% if STREAMLIT_enable | default(false) %}
  #  ================================== Streamlit ========================================== #
  {% for num in range(STREAMLIT_apps.split(",") | count ) %}
    {% set external_port = 28345 + loop.index - 1 %}
  streamlit-{{loop.index}}:
    image: {{STREAMLIT_image}}:{{__STREAMLIT_python_version}}
    container_name: streamlit-{{loop.index}}
    hostname: streamlit-{{loop.index}}
    labels:
      com.platys.name: "streamlit"
      com.platys.description: "{{STREAMLIT_apps_description.split(',')[loop.index-1]}}"
      com.platys.webui.title: "Streamlit Application UI"
      com.platys.webui.url: "http{{"s" if STREAMLIT_enable_https}}://dataplatform:{{external_port}}"
    ports:
      - {{external_port}}:80
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      STREAMLIT_SERVER_PORT: 80
      STREAMLIT_SERVER_ADDRESS: 0.0.0.0
      STREAMLIT_SERVER_ENABLE_CORS: false
      STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION: false
      STREAMLIT_SERVER_ENABLE_STATIC_SERVING: true
    {% if STREAMLIT_enable_https %}
      STREAMLIT_SERVER_SSL_KEY_FILE: /tmp/key.pem
      STREAMLIT_SERVER_SSL_CERT_FILE: /tmp/cert.pem
    {% endif -%}   {#  STREAMLIT_enable_https #}
    {% if STREAMLIT_env_variables is defined and STREAMLIT_env_variables and STREAMLIT_env_variables | length | default(false) %}
      {% for envvar in STREAMLIT_env_variables.split(",") %}
      {{envvar.split('=')[0]}}: "{{envvar.split('=')[1]}}"
      {% endfor %}
    {% endif -%}   {#  STREAMLIT_env_variables #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/streamlit/run_pip_and_streamlit.sh:/app/run_pip_and_streamlit.sh:ro
    {% if STREAMLIT_artefacts_folder is defined and STREAMLIT_artefacts_folder and STREAMLIT_artefacts_folder|length %}
      - {{STREAMLIT_artefacts_folder.split(",")[loop.index-1]}}:/tmp:ro
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - sh
      - -c
      - |
    {% if STREAMLIT_python_packages is defined and STREAMLIT_python_packages and STREAMLIT_python_packages|length  %}
        pip install {{STREAMLIT_python_packages}}
    {% endif -%}   {#  PYTHON_python_packages #}
    {% if STREAMLIT_requirements_files is defined and STREAMLIT_requirements_files and STREAMLIT_requirements_files|length %}
        /app/run_pip_and_streamlit.sh /tmp/{{STREAMLIT_apps.split(",")[loop.index-1]}} /tmp/{{STREAMLIT_requirements_files.split(",")[loop.index-1]}}
    {% else %}
        /app/run_pip_and_streamlit.sh /tmp/{{STREAMLIT_apps.split(",")[loop.index-1]}}
    {% endif -%}   {#  PYTHON_requirements_files #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor -%}   {#  STREAMLIT_apps #}
{% endif %}   {# STREAMLIT_enable #}

{% if BASEROW_enable | default(false) %}
  #  ================================== Baserow ========================================== #
  baserow:
    image: baserow/baserow:{{__BASEROW_version}}
    hostname: baserow
    container_name: baserow
    labels:
      com.platys.name: "baserow"
      com.platys.description: "no-code database tool and Airtable alternative"
      com.platys.webui.title: "Baserow UI"
      com.platys.webui.url: "http://dataplatform:28299"
    ports:
      - "28299:80"
      - "28300:443"
    environment:
      BASEROW_PUBLIC_URL: 'http://${PUBLIC_IP}:28299'
    {% if not BASEROW_volume_map_data %}
      DISABLE_VOLUME_CHECK: true
    {% endif -%}   {#  BASEROW_volume_map_data #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if BASEROW_volume_map_data %}
      - ./container-volume/baserow/data:/baserow/data
    {% endif -%}   {#  BASEROW_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  BASEROW_enable #}

{% if MEMCACHED_enable | default(false) %}
  #  ================================== Memcached ========================================== #
  memchached:
    image: memcached:{{__MEMCACHED_version}}
    hostname: memcached
    container_name: memcached
    labels:
      com.platys.name: "memcached"
      com.platys.description: "distributed memory object caching system"
    ports:
      - 11211:11211/tcp
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MEMCACHED_enable #}

{% if REDIS_enable | default(false) %}
  #  ================================== Redis ========================================== #
  redis-1:
    image: bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}:{{__REDIS_version if REDIS_edition | lower == 'redis' else __VALKEY_version}}
    hostname: redis-1
    container_name: redis-1
    labels:
      com.platys.name: {{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}
      com.platys.description: "Key-value store"
    ports:
      - 6379:6379
    environment:
    {% if REDIS_replicasets is defined and REDIS_replicasets and REDIS_replicasets > 0 %}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_REPLICATION_MODE={{'master' if REDIS_edition | lower == 'redis' else 'primary'}}
    {% endif -%}   {#  REDIS_replicasets > 0 #}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_PORT_NUMBER=6379
      - ALLOW_EMPTY_PASSWORD={{'yes' if REDIS_allow_empty_password else 'no' }}
    {% if REDIS_disable_commands is defined and REDIS_disable_commands | default(false) %}
      - {{'' if REDIS_edition | lower == 'redis' else 'VALKEY_'}}DISABLE_COMMANDS={{REDIS_disable_commands | default(omit) }}
    {% endif -%}   {# REDIS_disable_commands #}
    {% if not REDIS_allow_empty_password and REDIS_password is defined and REDIS_password and REDIS_password | length | default(false) %}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_PASSWORD={{REDIS_password if REDIS_password is defined and REDIS_password and REDIS_password | length else omit}}
    {% elif not REDIS_allow_empty_password and REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length | default(false) %}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_PASSWORD_FILE=/opt/bitnami/redis/mounted-etc/{{REDIS_password_file if REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length else omit }}
    {% endif -%}   {# not REDIS_allow_empty_password and REDIS_password or REDIS_PASSWORD_FILE #}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_ACLFILE=/{{REDIS_aclfile if REDIS_aclfile is defined and REDIS_aclfile and REDIS_aclfile | length else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_AOF_ENABLED={{'yes' if REDIS_aof_enable else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_RDB_POLICY={{REDIS_rdb_save_policy if REDIS_rdb_enable and REDIS_rdb_save_policy is defined and REDIS_rdb_save_policy and REDIS_rdb_save_policy | length else omit}}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_RDB_POLICY_DISABLED={{'yes' if not REDIS_rdb_enable else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_IO_THREADS={{REDIS_io_threads if REDIS_io_threads is defined and REDIS_io_threads and REDIS_io_threads | length and REDIS_io_threads > 0 else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_IO_THREADS_DO_READS={{'yes' if REDIS_io_threads_do_reads_enable else omit }}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if REDIS_overrides_config_file is defined and REDIS_overrides_config_file and REDIS_overrides_config_file | length %}
      - ./custom-conf/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/{{REDIS_overrides_config_file}}:/opt/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/mounted-etc/{{REDIS_overrides_config_file}}
    {% endif %}
    {% if REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length %}
      - ./security/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/{{REDIS_password_file}}:/opt/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/mounted-etc/{{REDIS_password_file}}
    {% endif %}
    {% if REDIS_aclfile is defined and REDIS_aclfile and REDIS_aclfile | length %}
      - ./security/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/{{REDIS_aclfile}}:/opt/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/mounted-etc/{{REDIS_aclfile}}
    {% endif %}
    {% if REDIS_volume_map_data %}
      - "./container-volume/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/data:/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/data"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: /opt/bitnami/scripts/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/run.sh --loglevel {{REDIS_log_level}}
    restart: {{container_restart_policy}}
    healthcheck:
    {% if not REDIS_allow_empty_password and REDIS_password is defined and REDIS_password and REDIS_password | length | default(false) %}
      test: ["CMD-SHELL", "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}-cli --no-auth-warning -a \"$$(echo $$REDIS_PASSWORD)\" ping | grep PONG"]
    {% elif not REDIS_allow_empty_password and REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length | default(false) %}
      test: ["CMD-SHELL", "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}-cli --no-auth-warning -a \"$$(cat $$REDIS_PASSWORD_FILE)\" ping | grep PONG"]
    {% else %}
      test: ["CMD-SHELL", "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}-cli --no-auth-warning ping | grep PONG"]
    {% endif -%}   {# not REDIS_allow_empty_password and REDIS_password or REDIS_PASSWORD_FILE #}
      interval: 5s
      timeout: 3s
      retries: 5

  {% for num in range(REDIS_replicasets| default(0) ) %}
    {% set external_port = 6380 + loop.index - 1 %}
  redis-replica-{{loop.index}}:
    image: bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}:{{__REDIS_version if REDIS_edition | lower == 'redis' else __VALKEY_version}}
    hostname: redis-replica-{{loop.index}}
    container_name: redis-replica-{{loop.index}}
    labels:
      com.platys.name: "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}"
      com.platys.description: "Key-Value NoSQL datastore"
    ports:
      - {{external_port}}:6379
    environment:
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_REPLICATION_MODE={{'slave' if REDIS_edition | lower == 'redis' else 'replica'}}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_MASTER_HOST=redis-1
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_MASTER_PORT_NUMBER=6379
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_MASTER_PASSWORD={{REDIS_password | default(omit) }}
      - ALLOW_EMPTY_PASSWORD={{'yes' if REDIS_allow_empty_password else 'no' }}
    {% if REDIS_disable_commands is defined and REDIS_disable_commands | default(false) %}
      - {{'' if REDIS_edition | lower == 'redis' else 'VALKEY_'}}DISABLE_COMMANDS={{REDIS_disable_commands | default(omit) }}
    {% endif -%}   {# REDIS_disable_commands #}
    {% if not REDIS_allow_empty_password and REDIS_password is defined and REDIS_password and REDIS_password | length | default(false) %}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_PASSWORD={{REDIS_password if REDIS_password is defined and REDIS_password and REDIS_password | length else omit}}
    {% elif not REDIS_allow_empty_password and REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length | default(false) %}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_PASSWORD_FILE=/opt/bitnami/redis/mounted-etc/{{REDIS_password_file if REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length else omit }}
    {% endif -%}   {# not REDIS_allow_empty_password and REDIS_password or REDIS_PASSWORD_FILE #}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_ACLFILE=/{{REDIS_aclfile if REDIS_aclfile is defined and REDIS_aclfile and REDIS_aclfile | length else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_AOF_ENABLED={{'yes' if REDIS_aof_enable else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_RDB_POLICY={{REDIS_rdb_save_policy if REDIS_rdb_enable and REDIS_rdb_save_policy is defined and REDIS_rdb_save_policy and REDIS_rdb_save_policy | length else omit}}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_RDB_POLICY_DISABLED={{'yes' if not REDIS_rdb_enable else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_IO_THREADS={{REDIS_io_threads if REDIS_io_threads is defined and REDIS_io_threads and REDIS_io_threads | length and REDIS_io_threads > 0 else omit }}
      - {{'REDIS' if REDIS_edition | lower == 'redis' else 'VALKEY'}}_IO_THREADS_DO_READS={{'yes' if REDIS_io_threads_do_reads_enable else omit }}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if REDIS_overrides_config_file is defined and REDIS_overrides_config_file and REDIS_overrides_config_file | length %}
      - ./custom-conf/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/{{REDIS_overrides_config_file}}:/opt/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/mounted-etc/{{REDIS_overrides_config_file}}
    {% endif %}    {% if REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length %}
      - ./security/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/{{REDIS_password_file}}:/opt/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/mounted-etc/{{REDIS_password_file}}
    {% endif %}
    {% if REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length %}
      - ./security/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/{{REDIS_password_file}}:/opt/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/mounted-etc/{{REDIS_password_file}}
    {% endif %}
    {% if REDIS_volume_map_data %}
      - "./container-volume/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/data/redis-replica-{{loop.index}}:/bitnami/{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}/data"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
    {% if not REDIS_allow_empty_password and REDIS_password is defined and REDIS_password and REDIS_password | length | default(false) %}
      test: ["CMD-SHELL", "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}-cli --no-auth-warning -a \"$$(echo $$REDIS_PASSWORD)\" ping | grep PONG"]
    {% elif not REDIS_allow_empty_password and REDIS_password_file is defined and REDIS_password_file and REDIS_password_file | length | default(false) %}
      test: ["CMD-SHELL", "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}-cli --no-auth-warning -a \"$$(cat $$REDIS_PASSWORD_FILE)\" ping | grep PONG"]
    {% else %}
      test: ["CMD-SHELL", "{{'redis' if REDIS_edition | lower == 'redis' else 'valkey'}}-cli --no-auth-warning ping | grep PONG"]
    {% endif -%}   {# not REDIS_allow_empty_password and REDIS_password or REDIS_PASSWORD_FILE #}
      interval: 5s
      timeout: 3s
      retries: 5
  {% endfor %}
{% endif %}   {#  REDIS_enable #}

{% if REDIS_STACK_enable | default(false) %}
  #  ================================== Redis Server ========================================== #
  redis-stack-1:
    image: redis/redis-stack-server:{{__REDIS_STACK_version}}
    hostname: redis-stack-1
    container_name: redis-stack-1
    labels:
      com.platys.name: "redis-stack"
      com.platys.description: "Key-Value NoSQL datastore with additional Redis modules"
    ports:
      - 6385:6379
    environment:
      - REDIS_ARGS={{'--requirepass' ~ REDIS_STACK_password if REDIS_STACK_password is defined and REDIS_STACK_password and REDIS_STACK_password | length }}
      - REDISEARCH_ARGS=
      - REDISJSON_ARGS=
      - REDISGRAPH_ARGS=
      - REDISTIMESERIES_ARGS=
      - REDISBLOOM_ARGS=
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if REDIS_STACK_volume_map_data %}
      - "./container-volume/redis-stack/data:/data"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  REDIS_STACK_enable #}

{% if REDIS_EXPORTER_enable | default(false) %}
  #  ================================== Redis Exporter ========================================== #
  redis-exporter:
    image: bitnami/redis-exporter:{{__REDIS_EXPORTER_version}}
    container_name: redis-exporter
    hostname: redis-exporter
    labels:
      com.platys.name: "redis-exporter"
      com.platys.restapi.title: "Redis Promethes Exporter API"
      com.platys.restapi.url: "http://dataplatform:9121/metrics"
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis-1:6379
    {% if not REDIS_allow_empty_password and REDIS_password is defined and REDIS_password and REDIS_password | length | default(false) %}
      - REDIS_PASSWORD={{REDIS_password if REDIS_password is defined and REDIS_password and REDIS_password | length else omit}}
    {% elif not REDIS_allow_empty_password and REDIS_EXPORTER_password_file is defined and REDIS_EXPORTER_password_file and REDIS_EXPORTER_password_file | length | default(false) %}
      - REDIS_PASSWORD_FILE=/opt/bitnami/redis/mounted-etc/{{REDIS_EXPORTER_password_file if REDIS_EXPORTER_password_file is defined and REDIS_EXPORTER_password_file and REDIS_EXPORTER_password_file | length else omit }}
    {% endif -%}   {# not REDIS_allow_empty_password and REDIS_password or REDIS_EXPORTER_password_file #}
      - REDIS_EXPORTER_CHECK_KEYS='{{REDIS_EXPORTER_check_keys if REDIS_EXPORTER_check_keys is defined else omit}}'
      - REDIS_EXPORTER_CHECK_SINGLE_KEYS='{{REDIS_EXPORTER_check_single_keys if REDIS_EXPORTER_check_single_keys is defined else omit}}'
      - REDIS_EXPORTER_COUNT_KEYS='{{REDIS_EXPORTER_count_keys if REDIS_EXPORTER_count_keys is defined else omit}}'
      - REDIS_EXPORTER_REDIS_ONLY_METRICS={{REDIS_EXPORTER_redis_only_metrics}}
      - REDIS_EXPORTER_INCL_CONFIG_METRICS={{REDIS_EXPORTER_incl_config_metrics}}
      - REDIS_EXPORTER_INCL_SYSTEM_METRICS={{REDIS_EXPORTER_incl_system_metrics}}
      - REDIS_EXPORTER_REDACT_CONFIG_METRICS={{REDIS_EXPORTER_redact_config_metrics}}
      - REDIS_EXPORTER_PING_ON_CONNECT={{REDIS_EXPORTER_ping_on_connect}}
      - REDIS_EXPORTER_IS_TILE38=false
      - REDIS_EXPORTER_IS_CLUSTER=false
      - REDIS_EXPORTER_EXPORT_CLIENT_LIST={{REDIS_EXPORTER_export_client_list if REDIS_EXPORTER_export_client_list is defined else omit}}
      - REDIS_EXPORTER_CHECK_KEY_GROUPS={{REDIS_EXPORTER_check_key_groups if REDIS_EXPORTER_check_key_groups is defined else omit}}
      - REDIS_EXPORTER_MAX_DISTINCT_KEY_GROUPS={{REDIS_EXPORTER_max_distinct_key_groups}}
      - REDIS_EXPORTER_SKIP_TLS_VERIFICATION=true
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if REDIS_EXPORTER_password_file is defined and REDIS_EXPORTER_password_file and REDIS_EXPORTER_password_file | length %}
      - ./security/redis-exporter/{{REDIS_EXPORTER_password_file}}:/opt/bitnami/redis/mounted-etc/{{REDIS_EXPORTER_password_file}}
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  REDIS_EXPORTER_enable #}

{% if REDIS_INSIGHT_enable | default(false) %}
  #  ================================== Redis Insight ========================================== #
  redis-insight:
    image: redis/redisinsight:{{__REDIS_INSIGHT_version}}
    container_name: redis-insight
    hostname: redis-insight
    labels:
      com.platys.name: "redis-insight"
      com.platys.description: "Graphical interface for Redis"
      com.platys.webui.title: "Redis Insight UI"
      com.platys.webui.url: "http://dataplatform:28174"
    ports:
      - "28174:5540"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  REDIS_INSIGHT_enable #}

{% if REDIS_COMMANDER_enable | default(false) %}
  #  ================================== Redis Commander ========================================== #
  redis-commander:
    image: rediscommander/redis-commander:{{__REDIS_COMMANDER_version}}
    container_name: redis-commander
    hostname: redis-commander
    labels:
      com.platys.name: "redis-commander"
      com.platys.description: "Graphical interface for Redis"
      com.platys.webui.title: "Redis Commander UI"
      com.platys.webui.url: "http://dataplatform:28119"
    ports:
      - "28119:8081"
    environment:
      - REDIS_HOST=redis-1
      - REDIS_PORT=6379
      - REDIS_PASSWORD={{REDIS_password if REDIS_password is defined and REDIS_password and REDIS_password | length else omit}}
      - REDIS_DB=0
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  REDIS_COMMANDER_enable #}

{% if VALKEY_enable | default(false) %}
  #  ================================== Valkey ========================================== #
  valkey-1:
    image: bitnami/valkey:{{__VALKEY_version}}
    hostname: valkey-1
    container_name: valkey-1
    labels:
      com.platys.name: "valkey"
      com.platys.description: "Key-value store"
    ports:
      - 6389:6379
    environment:
    {% if VALKEY_replicasets is defined and VALKEY_replicasets and VALKEY_replicasets > 0 %}
      - VALKEY_REPLICATION_MODE=primary
    {% endif -%}   {#  VALKEY_replicasets > 0 #}
      - VALKEY_PORT_NUMBER=6379
      - ALLOW_EMPTY_PASSWORD={{'yes' if VALKEY_allow_empty_password else 'no' }}
    {% if VALKEY_disable_commands is defined and VALKEY_disable_commands | default(false) %}
      - VALKEY_DISABLE_COMMANDS={{VALKEY_disable_commands | default(omit) }}
    {% endif -%}   {# VALKEY_disable_commands #}
    {% if not VALKEY_allow_empty_password and VALKEY_password is defined and VALKEY_password and VALKEY_password | length | default(false) %}
      - VALKEY_PASSWORD={{VALKEY_password if VALKEY_password is defined and VALKEY_password and VALKEY_password | length else omit}}
    {% elif not VALKEY_allow_empty_password and VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length | default(false) %}
      - VALKEY_PASSWORD_FILE=/opt/bitnami/valkey/mounted-etc/{{VALKEY_password_file if VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length else omit }}
    {% endif -%}   {# not VALKEY_allow_empty_password and VALKEY_password or VALKEY_PASSWORD_FILE #}
      - VALKEY_ACLFILE=/{{VALKEY_aclfile if VALKEY_aclfile is defined and VALKEY_aclfile and VALKEY_aclfile | length else omit }}
      - VALKEY_AOF_ENABLED={{'yes' if VALKEY_aof_enable else omit }}
      - VALKEY_RDB_POLICY={{VALKEY_rdb_save_policy if VALKEY_rdb_enable and VALKEY_rdb_save_policy is defined and VALKEY_rdb_save_policy and VALKEY_rdb_save_policy | length else omit}}
      - VALKEY_RDB_POLICY_DISABLED={{'yes' if not VALKEY_rdb_enable else omit }}
      - VALKEY_IO_THREADS={{VALKEY_io_threads if VALKEY_io_threads is defined and VALKEY_io_threads and VALKEY_io_threads | length and VALKEY_io_threads > 0 else omit }}
      - VALKEY_IO_THREADS_DO_READS={{'yes' if VALKEY_io_threads_do_reads_enable else omit }}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if VALKEY_overrides_config_file is defined and VALKEY_overrides_config_file and VALKEY_overrides_config_file | length %}
      - ./custom-conf/valkey/{{VALKEY_overrides_config_file}}:/opt/bitnami/valkey/mounted-etc/{{VALKEY_overrides_config_file}}
    {% endif %}
    {% if VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length %}
      - ./security/valkey/{{VALKEY_password_file}}:/opt/bitnami/valkey/mounted-etc/{{VALKEY_password_file}}
    {% endif %}
    {% if VALKEY_aclfile is defined and VALKEY_aclfile and VALKEY_aclfile | length %}
      - ./security/valkey/{{VALKEY_aclfile}}:/opt/bitnami/valkey/mounted-etc/{{VALKEY_aclfile}}
    {% endif %}
    {% if VALKEY_volume_map_data %}
      - "./container-volume/valkey/data:/bitnami/valkey/data"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: /opt/bitnami/scripts/valkey/run.sh --loglevel {{VALKEY_log_level}}
    restart: {{container_restart_policy}}
    healthcheck:
    {% if not VALKEY_allow_empty_password and VALKEY_password is defined and VALKEY_password and VALKEY_password | length | default(false) %}
      test: ["CMD-SHELL", "valkey-cli --no-auth-warning -a \"$$(echo $$VALKEY_PASSWORD)\" ping | grep PONG"]
    {% elif not VALKEY_allow_empty_password and VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length | default(false) %}
      test: ["CMD-SHELL", "valkey-cli --no-auth-warning -a \"$$(cat $$VALKEY_PASSWORD_FILE)\" ping | grep PONG"]
    {% else %}
      test: ["CMD-SHELL", "valkey-cli --no-auth-warning ping | grep PONG"]
    {% endif -%}   {# not VALKEY_allow_empty_password and VALKEY_password or VALKEY_PASSWORD_FILE #}
      interval: 5s
      timeout: 3s
      retries: 5
    
  {% for num in range(VALKEY_replicasets| default(0) ) %}
    {% set external_port = 6390 + loop.index - 1 %}
  valkey-replica-{{loop.index}}:
    image: bitnami/valkey:{{__VALKEY_version}}
    hostname: valkey-replica-{{loop.index}}
    container_name: valkey-replica-{{loop.index}}
    labels:
      com.platys.name: "valkey"
      com.platys.description: "Key-Value NoSQL datastore"
    ports:
      - {{external_port}}:6379
    environment:
      - VALKEY_REPLICATION_MODE=replica
      - VALKEY_PRIMARY_HOST=valkey-1
      - VALKEY_PRIMARY_PORT_NUMBER=6379
      - VALKEY_PRIMARY_PASSWORD={{VALKEY_password | default(omit) }}
      - ALLOW_EMPTY_PASSWORD={{'yes' if VALKEY_allow_empty_password else 'no' }}
    {% if VALKEY_disable_commands is defined and VALKEY_disable_commands | default(false) %}
      - VALKEY_DISABLE_COMMANDS={{VALKEY_disable_commands | default(omit) }}
    {% endif -%}   {# VALKEY_disable_commands #}
    {% if not VALKEY_allow_empty_password and VALKEY_password is defined and VALKEY_password and VALKEY_password | length | default(false) %}
      - VALKEY_PASSWORD={{VALKEY_password if VALKEY_password is defined and VALKEY_password and VALKEY_password | length else omit}}
    {% elif not VALKEY_allow_empty_password and VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length | default(false) %}
      - VALKEY_PASSWORD_FILE=/opt/bitnami/valkey/mounted-etc/{{VALKEY_password_file if VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length else omit }}
    {% endif -%}   {# not VALKEY_allow_empty_password and VALKEY_password or VALKEY_PASSWORD_FILE #}
      - VALKEY_ACLFILE=/{{VALKEY_aclfile if VALKEY_aclfile is defined and VALKEY_aclfile and VALKEY_aclfile | length else omit }}
      - VALKEY_AOF_ENABLED={{'yes' if VALKEY_aof_enable else omit }}
      - VALKEY_RDB_POLICY={{VALKEY_rdb_save_policy if VALKEY_rdb_enable and VALKEY_rdb_save_policy is defined and VALKEY_rdb_save_policy and VALKEY_rdb_save_policy | length else omit}}
      - VALKEY_RDB_POLICY_DISABLED={{'yes' if not VALKEY_rdb_enable else omit }}
      - VALKEY_IO_THREADS={{VALKEY_io_threads if VALKEY_io_threads is defined and VALKEY_io_threads and VALKEY_io_threads | length and VALKEY_io_threads > 0 else omit }}
      - VALKEY_IO_THREADS_DO_READS={{'yes' if VALKEY_io_threads_do_reads_enable else omit }}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if VALKEY_overrides_config_file is defined and VALKEY_overrides_config_file and VALKEY_overrides_config_file | length %}
      - ./custom-conf/valkey/{{VALKEY_overrides_config_file}}:/opt/bitnami/valkey/mounted-etc/{{VALKEY_overrides_config_file}}
    {% endif %}    {% if VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length %}
      - ./security/valkey/{{VALKEY_password_file}}:/opt/bitnami/valkey/mounted-etc/{{VALKEY_password_file}}
    {% endif %}
    {% if VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length %}
      - ./security/valkey/{{VALKEY_password_file}}:/opt/valkey/valkey/mounted-etc/{{VALKEY_password_file}}
    {% endif %}
    {% if VALKEY_volume_map_data %}
      - "./container-volume/valkey/data/valkey-replica-{{loop.index}}:/bitnami/valkey/data"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
    {% if not VALKEY_allow_empty_password and VALKEY_password is defined and VALKEY_password and VALKEY_password | length | default(false) %}
      test: ["CMD-SHELL", "valkey-cli --no-auth-warning -a \"$$(echo $$VALKEY_PASSWORD)\" ping | grep PONG"]
    {% elif not VALKEY_allow_empty_password and VALKEY_password_file is defined and VALKEY_password_file and VALKEY_password_file | length | default(false) %}
      test: ["CMD-SHELL", "valkey-cli --no-auth-warning -a \"$$(cat $$VALKEY_PASSWORD_FILE)\" ping | grep PONG"]
    {% else %}
      test: ["CMD-SHELL", "valkey-cli --no-auth-warning ping | grep PONG"]
    {% endif -%}   {# not VALKEY_allow_empty_password and VALKEY_password or VALKEY_PASSWORD_FILE #}
      interval: 5s
      timeout: 3s
      retries: 5
  {% endfor %}
{% endif %}   {#  VALKEY_enable #}

{% if CASSANDRA_enable | default(false) %}
  #  ================================== Cassandra ========================================== #
  {% for num in range(CASSANDRA_nodes| default(1) ) %}
    {% set external_port = 29042 + loop.index - 1 %}
    {% set external_jmx_port = 7199 + loop.index - 1 %}
    {% set external_thrift_port = 9160 + loop.index - 1 %}
  cassandra-{{loop.index}}:
    image: bitnami/cassandra:{{__CASSANDRA_version}}
    container_name: cassandra-{{loop.index}}
    hostname: cassandra-{{loop.index}}
    labels:
      com.platys.name: "cassandra"
      com.platys.description: "wide-column NoSQL database"
    ports:
      - {{external_port}}:9042
      - {{external_jmx_port}}:7199
      - {{external_thrift_port}}:9160
    environment:
      - CASSANDRA_CLUSTER_NAME="Test Cluster"
      - CASSANDRA_DATACENTER=se1
    {% if loop.index > 1 %}
      - CASSANDRA_SEEDS=cassandra-1
    {% endif %}
    {% if loop.index == 1 %}
      - CASSANDRA_PASSWORD_SEEDER=yes
    {% endif %}
      - CASSANDRA_USER={{CASSANDRA_username | default(cassandra) }}
      - CASSANDRA_PASSWORD={{CASSANDRA_password | default(cassandra) }}
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_NUM_TOKENS=128
      - LOCAL_JMX={{CASSANDRA_local_jmx}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/cassandra/jmxremote.access:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/management/jmxremote.access
      - ./conf/cassandra/jmxremote.password:/etc/cassandra/jmxremote.password
    {% if CASSANDRA_volume_map_data %}
      - "./container-volume/cassandra/data/cassandra-{{loop.index}}:/var/lib/cassandra"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  {% if REAPER_enable | default(false) %}
  reaper:
    image: thelastpickle/cassandra-reaper:{{__REAPER_version}}
    container_name: reaper
    hostname: reaper
    labels:
      com.platys.name: "cassandra-reaper"
      com.platys.description: "Repair Management for Cassandra"
      com.platys.webui.title: "Reaper Application UI"
      com.platys.webui.url: "http://dataplatform:28232/webui"
    ports:
      - "28232:8080"
      - "28233:8081"
    environment:
      - REAPER_STORAGE_TYPE=memory
      - REAPER_AUTO_SCHEDULING_ENABLED=true
      - REAPER_AUTO_SCHEDULING_INITIAL_DELAY_PERIOD="PT15S"
      - REAPER_AUTO_SCHEDULING_TIME_BEFORE_FIRST_SCHEDULE="PT1M"
      - REAPER_AUTO_SCHEDULING_PERIOD_BETWEEN_POLLS="PT1M"
      - REAPER_JMX_AUTH_USERNAME=reaper
      - REAPER_JMX_AUTH_PASSWORD=abc123!
      - CRYPTO_SYSTEM_PROPERTY_SECRET="CRYPTO_SECRET"
      - CRYPTO_SECRET="secret"
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {% endif %}   {#  REAPER_enable #}
{% endif %}   {#  CASSANDRA_enable #}

{% if DATASTAX_enable | default(false) %}
  #  ================================== DataStax Enterprise (DSE) ========================================== #
  {% for num in range(DATASTAX_nodes| default(1) ) %}
    {% set external_port = 9042 + loop.index - 1 %}

  dse-{{loop.index}}: # DataStax Enterprise Database
    image: datastax/dse-server:{{__DATASTAX_version}}
    hostname: dse-{{loop.index}}
    labels:
      com.platys.name: "datastax"
      com.platys.description: "Datastax Wide Column NoSQL store"
    container_name: dse-{{loop.index}}
    command: [ -s -k ] # Run with Search and Spark capabilities enabled
  {% if loop.index == 1 %}
    ports:
      - "{{external_port}}:9042" # Exposes DSE port
  {% endif %}
    environment:
      DS_LICENSE: accept # Accept licence on start
      SPARK_SCALA_VERSION: "2.11"
      OPSCENTER_IP: dse-opscenter
    {% if loop.index > 1 %}
      SEEDS: "dse-1"
    {% endif %}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - "./dse.yaml:/opt/dse/resources/dse/conf/dse.yaml"
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    cap_add:
      - IPC_LOCK # Allow DSE to lock memory with mlock
    ulimits:
      memlock: -1
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  dse-studio:  # Datastax Studio provides convenient web interface to work directly with the Apache Cassandra or DataStax Enterprise
    image: datastax/dse-studio:latest
    container_name: dse-studio
    hostname: dse-studio
    labels:
      com.platys.name: "datastax-studio"
      com.platys.description: "DataStax Studio Web UI"
      com.platys.webui.title: "DataStax Studio Web UI"
      com.platys.webui.url: "http://dataplatform:28121"
    ports:
      - "28121:9091" # Exposes port to be available externally
    environment:
      DS_LICENSE: accept
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  dse-opscenter:
    image: "datastax/dse-opscenter:{{__DATASTAX_OPSCENTER_version}}"
    container_name: dse-opscenter
    hostname: dse-opscenter
    labels:
      com.platys.name: "datastax"
      com.platys.description: "Datastax OpsCenter"
      com.platys.webui.title: "DataStax OpsCenter Web UI"
      com.platys.webui.url: "http://dataplatform:28122"
    ports:
      - 28122:8888
    environment:
      DS_LICENSE: accept
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  DATASTAX_enable #}

{% if MONGO_enable | default(false) %}
  #  ================================== MongoDB ========================================== #
  {% for num in range(MONGO_nodes| default(1) ) %}
    {% set external_port = 27017 + loop.index - 1 %}
  mongo-{{loop.index}}:
    image: mongo:{{__MONGO_version}}
    container_name: mongo-{{loop.index}}
    hostname: mongo-{{loop.index}}
    labels:
      com.platys.name: "mongodb"
      com.platys.description: "Document NoSQL database"
      com.platys.password.envvars: "PLATYS_MONGO_PASSWORD"      
    ports:
      - {{external_port}}:27017
    environment:
      - MONGO_INITDB_DATABASE={{MONGO_init_database if MONGO_init_database is defined and MONGO_init_database and MONGO_init_database | length else omit }}
    {%if MONGO_root_username is defined and MONGO_root_username and MONGO_root_username | length > 0 %}  
      - MONGO_INITDB_ROOT_USERNAME={{MONGO_root_username}}
      - MONGO_INITDB_ROOT_PASSWORD=${PLATYS_MONGO_PASSWORD:-{{MONGO_root_password}}}
    {% endif -%}   {#  MONGO_root_username #}
      - MONGO_DATA_DIR=/data/db
      - MONGO_LOG_DIR=/data/log
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # seeding scripts
      - ./init/mongodb:/docker-entrypoint-initdb.d
    {% if MONGO_volume_map_data %}
      - ./container-volume/mongodb/mongo-{{loop.index}}/data:/data/db
    {% endif -%}   {#  MONGO_volume_map_data #}
    {% if MONGO_volume_map_log %}
      - ./container-volume/mongodb/mongo-{{loop.index}}/log:/data/log
    {% endif -%}   {#  MONGO_volume_map_log #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  {% if MONGO_EXPRESS_enable | default(false) %}
  mongo-express:
    image: mongo-express:{{__MONGO_EXPRESS_version}}
    container_name: mongo-express
    hostname: mongo-express
    labels:
      com.platys.name: "mongo-express"
      com.platys.description: "MongoDB UI"
      com.platys.webui.title: "Mongo Express UI"
      com.platys.webui.url: "http://dataplatform:28123"
      com.platys.password.envvars: "PLATYS_MONGO_EXPRESS_PASSWORD"      
    ports:
      - 28123:8081
    environment:
      - ME_CONFIG_MONGODB_ENABLE_ADMIN=true
    {%if MONGO_root_username is defined and MONGO_root_username and MONGO_root_username | length > 0 %}  
      - ME_CONFIG_MONGODB_URL=mongodb://{{MONGO_root_username}}:${PLATYS_MONGO_PASSWORD:-{{MONGO_root_password}}}@mongo-1:27017
      - ME_CONFIG_MONGODB_ADMINUSERNAME={{MONGO_root_username}}
      - ME_CONFIG_MONGODB_ADMINPASSWORD=${PLATYS_MONGO_PASSWORD:-{{MONGO_root_password}}}
      - ME_CONFIG_MONGODB_AUTH_DATABASE=admin
    {% else %}
      - ME_CONFIG_MONGODB_URL=mongodb://mongo-1:27017
    {% endif -%}   {#  MONGO_root_username #}
      - ME_CONFIG_MONGODB_ENABLE_ADMIN=true
    {%if MONGO_EXPRESS_username is defined and MONGO_EXPRESS_username and MONGO_EXPRESS_username | length > 0 %}  
      - ME_CONFIG_BASICAUTH=true
      - ME_CONFIG_BASICAUTH_USERNAME={{MONGO_EXPRESS_username}}  
      - ME_CONFIG_BASICAUTH_PASSWORD=${PLATYS_MONGO_EXPRESS_PASSWORD:-{{MONGO_EXPRESS_password}}}
    {% else %}
      - ME_CONFIG_BASICAUTH=false
    {% endif -%}   {#  MONGO_EXPRESS_username #}
      - ME_CONFIG_OPTIONS_EDITORTHEME={{MONGO_EXPRESS_editor_theme}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    {% endif %}   {# MONGO_EXPRESS_enable #}

  admin-mongo:
    image: adicom/admin-mongo:{{__MONGO_ADMIN_version}}
    container_name: admin-mongo
    hostname: admin-mongo
    labels:
      com.platys.name: "admin-mongo"
      com.platys.description: "MongoDB Admin UI"
      com.platys.webui.title: "Admin Mongo UI"
      com.platys.webui.url: "http://dataplatform:28124"
    ports:
      - 28124:1234
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MONGO_enable #}

{% if SOLR_enable | default(false) %}
  #  ================================== SolR ========================================== #
  solr:
    image: solr:{{__SOLR_version}}
    container_name: solr
    hostname: solr
    labels:
      com.platys.name: "solr"
      com.platys.description: "Search-engine NoSQL store"
      com.platys.webui.title: "SolR UI"
      com.platys.webui.url: "http://dataplatform:8983"
    ports:
      - "8983:8983"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/solr:/docker-entrypoint-initdb.d/  
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SOLR_enable #}

{% if ELASTICSEARCH_enable | default(false) %}
  #  ================================== Elasticsearch ========================================== #
  elasticsearch-1:
    image: {{ 'elasticsearch' if (ELASTICSEARCH_edition | lower) == 'oss' else 'docker.elastic.co/elasticsearch/elasticsearch' }}:{{__ELASTICSEARCH_version}}
    hostname: elasticsearch-1
    container_name: elasticsearch-1
    labels:
      com.platys.name: "elasticsearch"
      com.platys.description: "Search-engine NoSQL store"
      com.platys.restapi.title: "Elasticsearch REST API"
      com.platys.restapi.url: "http://dataplatform:9200"
    {% if ELASTICSEARCH_security_enabled | default(false) %}     
      com.platys.password.envvars: "PLATYS_ELASTICSEARCH_PASSWORD"
    {% endif -%}   {#  ELASTICSEARCH_security_enabled #}  
      com.platys.manual.step.msgs: "sudo sysctl -w vm.max_map_count=262144"
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      discovery.type: "single-node"
      xpack.security.enabled: {{ELASTICSEARCH_security_enabled | default(false)}}
      xpack.monitoring.collection.enabled: "false"
    {% if ELASTICSEARCH_security_enabled | default(false) %}     
      ELASTIC_PASSWORD: ${PLATYS_ELASTICSEARCH_PASSWORD:-{{ELASTICSEARCH_password}}} 
    {% endif -%}   {#  ELASTICSEARCH_security_enabled #}
    {% if ELASTICSEARCH_run_secure | default(false) %}     
      xpack.security.http.ssl.enabled: true # <2>
      xpack.security.http.ssl.key: $CERTS_DIR/aspire-quickstart-elastic/aspire-quickstart-elastic.key
      xpack.security.http.ssl.certificate_authorities: $CERTS_DIR/ca/ca.crt
      xpack.security.http.ssl.certificate: $CERTS_DIR/aspire-quickstart-elastic/aspire-quickstart-elastic.crt
      xpack.security.transport.ssl.enabled: true # <3>
      xpack.security.transport.ssl.verification_mode: certificate # <4>
      xpack.security.transport.ssl.certificate_authorities: $CERTS_DIR/ca/ca.crt
      xpack.security.transport.ssl.certificate: $CERTS_DIR/aspire-quickstart-elastic/aspire-quickstart-elastic.crt
      xpack.security.transport.ssl.key: $CERTS_DIR/aspire-quickstart-elastic/aspire-quickstart-elastic.key
    {% else -%}   {#  ELASTICSEARCH_run_secure #}
      xpack.security.http.ssl.enabled: false
      xpack.security.transport.ssl.enabled: false
    {% endif -%}   {#  ELASTICSEARCH_run_secure #}
      http.cors.enabled: "true"
      http.cors.allow-origin: "http://${DOCKER_HOST_IP}:28275,http://${PUBLIC_IP}:28275,http://dejavu:1358,http://dataplatform:28125,http://dataplatform:28125,http://${PUBLIC_IP}:28125,http://${DOCKER_HOST_IP}:28125,http://127.0.0.1:1358"
      http.cors.allow-headers: "X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization"
      http.cors.allow-credentials: "true"
      cluster.routing.allocation.disk.threshold_enabled: 'true'
      cluster.routing.allocation.disk.watermark.low: 2gb
      cluster.routing.allocation.disk.watermark.high: 1gb
      cluster.routing.allocation.disk.watermark.flood_stage: 512mb
      ES_JAVA_OPTS: -Xms512m -Xmx512m
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
    {% if ELASTICSEARCH_security_enabled | default(false) %}     
        - curl -u elastic:${PLATYS_ELASTICSEARCH_PASSWORD:-{{ELASTICSEARCH_password}}} -s http://localhost:9200/_cat/health?h=status | grep -q green
    {% else -%}   {#  ELASTICSEARCH_security_enabled #}  
        - curl -s http://localhost:9200/_cat/health?h=status | grep -q green
    {% endif -%}   {#  ELASTICSEARCH_security_enabled #}  
      retries: 300
      interval: 10s
{% endif %}   {#  ELASTICSEARCH_enable #}

{% if DEJAVU_enable | default(false) %}
  dejavu:
    image: appbaseio/dejavu:{{__DEJAVU_version}}
    container_name: dejavu
    hostname: dejuvu
    labels:
      com.platys.name: "dejavu"
      com.platys.description: "UI for Elasticsearch"
      com.platys.webui.title: "Elasticsearch Dejavu UI"
      com.platys.webui.url: "http://dataplatform:28125"
    ports:
      - "28125:1358"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ELASTICSEARCH_enable #}

{% if CEREBRO_enable | default(false) %}
  cerebro:
    image: lmenezes/cerebro:{{__CEREBRO_version}}
    container_name: cerebro
    hostname: cerebro
    labels:
      com.platys.name: "cerbero"
      com.platys.description: "UI for Elasticsearch"
      com.platys.webui.title: "Elasticsearch Cerbero UI"
      com.platys.webui.url: "http://dataplatform:28126"
    ports:
      - "28126:9000"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CEREBRO_enable #}

{% if ELASTICHQ_enable | default(false) %}
  elastichq:
    image: elastichq/elasticsearch-hq:{{__ELASTICSEARCHHQ_version}}
    container_name: elastichq
    hostname: elatichq
    labels:
      com.platys.name: "elastichq"
      com.platys.description: "UI for Elasticsearch"
      com.platys.webui.title: "ElasticHQ UI"
      com.platys.webui.url: "http://dataplatform:28127"
    ports:
      - "28127:5000"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ELASTICHQ_enable #}

{% if ELASTICVUE_enable | default(false) %}
  #  ================================== ElasticVue ========================================== #
  elasticvue:
    image: cars10/elasticvue:{{__ELASTICVUE_version}}
    container_name: elasticvue
    hostname: elasticvue
    labels:
      com.platys.name: "elasticvue"
      com.platys.description: "UI for Elasticsearch"
      com.platys.webui.title: "ElasticVue UI"
      com.platys.webui.url: "http://dataplatform:28275"
    ports:
      - "28275:8080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ELASTICVUE_enable #}

{% if KIBANA_enable | default(false) %}
  #  ================================== Kibana ========================================== #
  kibana:
    image: {{ 'kibana' if (ELASTICSEARCH_edition | lower) == 'oss' else 'docker.elastic.co/kibana/kibana' }}:{{__KIBANA_version}}
    hostname: kibana
    container_name: kibana
    labels:
      com.platys.name: "kibana"
      com.platys.description: "Visualization for Elasticsearch"
      com.platys.webui.title: "Kibana UI"
      com.platys.webui.url: "http://dataplatform:5601"
    depends_on:
      - elasticsearch-1
    ports:
      - "5601:5601"
    environment:
      discovery.type: "single-node"
      ELASTICSEARCH_HOSTS: http://elasticsearch-1:9200
    {% if ELASTICSEARCH_security_enabled | default(false) %}     
      ELASTICSEARCH_USERNAME: elastic
      ELASTICSEARCH_PASSWORD: ${PLATYS_ELASTICSEARCH_PASSWORD:-{{ELASTICSEARCH_password}}} 
    {% endif -%}   {#  ELASTICSEARCH_security_enabled #}  
      SERVER_HOST: "0.0.0.0"
      SERVER_NAME: "kibana"
      XPACK_GRAPH_enabled: "false"
      XPACK_MONITORING_enabled: "false"
      XPACK_REPORTING_enabled: "false"
      XPACK_SECURITY_enabled: "false"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: [ "/bin/bash", "-c", "/usr/share/kibana/bin/kibana-plugin remove x-pack; /usr/local/bin/kibana-docker" ]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: curl --cacert /ca/ca.crt -s https://localhost:5601 >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      retries: 600

  kibana-init:
    image: alpine/curl
    container_name: kibana-init
    hostname: kibana-init
    depends_on: {"kibana": {"condition": "service_healthy"}}
    {% if ELASTICSEARCH_security_enabled | default(false) %}     
    environment:
      ELASTICSEARCH_USERNAME: elastic
      ELASTICSEARCH_PASSWORD: ${PLATYS_ELASTICSEARCH_PASSWORD:-{{ELASTICSEARCH_password}}} 
    {% endif -%}   {#  ELASTICSEARCH_security_enabled #}        
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/kibana/dashboards:/dashboards
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: 
      - sh
      - -c
      - |
        echo "Installing Kibana Dashboards"
        for file in /dashboards/*.ndjson;
        do
          if [ -f "$file" ]; then
            echo "Uploading dashboard: $file"        
            response=$(curl -s -o /dev/null -X POST -u $$ELASTICSEARCH_USERNAME:$$ELASTICSEARCH_PASSWORD -F 'file=@/dashboards/$$dashboard.ndjson' -H 'kbn-xsrf:reporting' "http://kibana:5601/api/saved_objects/_import?overwrite=true")
          else
            echo "No .ndjson files found in $DASHBOARD_DIR"
          fi
        done
    {% if (ASPIRE_enable) | default(false) %}
        echo "Uploading dashboard for Aspire"        
        response=$(curl -s -o /dev/null -X POST -u $$ELASTICSEARCH_USERNAME:$$ELASTICSEARCH_PASSWORD -F 'file=@/dashboards/aspire/export.ndjson' -H 'kbn-xsrf:reporting' "http://kibana:5601/api/saved_objects/_import?overwrite=true")
    {% endif -%}   {#  use_timezone #}
    restart: "no"
    init: true
{% endif %}   {#  KIBANA_enable #}

{% if OPENSEARCH_enable | default(false) %}
  #  ================================== OpenSearch ========================================== #
  {% set nsOS = namespace(opensearchHosts='') %}
  {% for num in range(OPENSEARCH_nodes | default(1) ) %}
    {% if loop.first %}
        {% set nsOS.opensearchHosts = 'opensearch-' ~ loop.index %}
    {% else %}
        {% set nsOS.opensearchHosts = nsOS.opensearchHosts ~ ',' ~ 'opensearch-' ~ loop.index %}
    {% endif %}
  {% endfor %}
  {% for num in range(OPENSEARCH_nodes | default(1) ) %}
    {% set external_port = 29200 + loop.index - 1 %}
    {% set external_perf_port = 29600 + loop.index - 1 %}
  opensearch-{{loop.index}}:
    image: opensearchproject/opensearch:{{__OPENSEARCH_version}}
    hostname: opensearch-{{loop.index}}
    container_name: opensearch-{{loop.index}}
    labels:
      com.platys.name: "opensearch"
      com.platys.description: "Search-engine NoSQL store"
      com.platys.restapi.title: "OpenSearch REST API"
      com.platys.restapi.url: "http://dataplatform:{{external_port}}"
      com.platys.manual.step.msgs: "sudo sysctl -w vm.max_map_count=262144"
    ports:
      - "{{external_port}}:9200"
      - "{{external_perf_port}}:9600"
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-{{loop.index}}
      {% if OPENSEARCH_nodes > 1 %}
      - discovery.seed_hosts={{nsOS.opensearchHosts}}
      - cluster.initial_master_nodes={{nsOS.opensearchHosts}}
      {% else %}
      - discovery.type=single-node
      - "DISABLE_INSTALL_DEMO_CONFIG=true"   # disables execution of install_demo_configuration.sh bundled with security plugin, which installs demo certificates and security configurations to OpenSearch
      - "DISABLE_SECURITY_PLUGIN=true"   #  disables security plugin entirely in OpenSearch by setting plugins.security.disabled: true in opensearch.yml
      {% endif %}
      - "bootstrap.memory_lock=true" # along with the memlock settings below, disables swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
      - "http.cors.enabled=true"
      - http.cors.allow-origin="http://${DOCKER_HOST_IP}:28275,http://${PUBLIC_IP}:28275,http://dejavu:1358,http://dataplatform:28125,http://dataplatform:28125,http://${PUBLIC_IP}:28125,http://${DOCKER_HOST_IP}:28125,http://127.0.0.1:1358"
      - http.cors.allow-headers="X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - ./data-transfer:/data-transfer
    {% if OPENSEARCH_volume_map_data %}
      - ./container-volume/opensearch/opensearch-{{loop.index}}/data:/usr/share/opensearch/data
    {% endif -%}   {#  OPENSEARCH_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {#  OPENSEARCH_enable #}

{% if OPENSEARCH_DASHBOARDS_enable | default(false) %}
  #  ================================== OpenSearch Dashboards ========================================== #
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:{{__OPENSEARCH_DASHBOARDS_version}}
    hostname: opensearch-dashboards
    container_name: opensearch-dashboards
    labels:
      com.platys.name: "opensearch-dashboards"
      com.platys.description: "Visualization for OpenSearch"
      com.platys.webui.title: "OpenSearch Dashboards"
      com.platys.webui.url: "http://dataplatform:5603"
    ports:
      - 5603:5601
    expose:
      - "5601"
    environment:
      {% if OPENSEARCH_nodes > 1 %}
      - 'OPENSEARCH_HOSTS=["https://opensearch-1:9200","https://opensearch-2:9200"]'
      {% else %}
      - 'OPENSEARCH_HOSTS=["http://opensearch-1:9200"]'
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true"   # disables security dashboards plugin in OpenSearch Dashboards
      {% endif %}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  OPENSEARCH_DASHBOARDS_enable #}

{% if SPLUNK_enable | default(false) %}
  #  ================================== Splunk ========================================== #
  splunk:
    image: splunk/splunk:{{__SPLUNK_version}}
    hostname: splunk
    container_name: splunk
    labels:
      com.platys.name: "splunk"
      com.platys.description: "Log Files Search engine"
      com.platys.webui.title: "Splunk UI"
      com.platys.webui.url: "http://dataplatform:28308"
    ports:
      - "28308:8000"
    environment:
      - SPLUNK_START_ARGS=--accept-license
    {% if (SPLUNK_edition | lower) == 'free'  %}
      - SPLUNK_LICENSE_URI=Free
    {% endif -%}   {#  SPLUNK_edition #}
      - SPLUNK_PASSWORD=abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SPLUNK_enable #}
{# #}

{% if JANUSGRAPH_enable | default(false) %}
  #  ================================== JanusGraph ========================================== #
  janusgraph:
    image: janusgraph/janusgraph:{{__JANUSGRAPH_version}}
    hostname: janusgraph
    container_name: janusgraph
    labels:
      com.platys.name: "janusgraph"
      com.platys.description: "Property Graph Database"
    ports:
      - "8182:8182"
    environment:
      - JANUS_PROPS_TEMPLATE={{JANUSGRAPH_props_template}}
      - JANUS_INITDB_DIR=/docker-entrypoint-initdb.d
      - janusgraph.storage.hostname=cassandra-1
      - janusgraph.storage.cql.local-datacenter=se1
      - janusgraph.storage.username=cassandra
      - janusgraph.storage.password=cassandra
      - janusgraph.index.search.hostname=elasticsearch-1
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/janusgraph:/docker-entrypoint-initdb.d/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  JANUSGRAPH_enable #}

{% if GREMLIN_CONSOLE_enable | default(false) %}
  #  ================================== Gremlin Console ========================================== #
  gremlin-console:
    image: janusgraph/janusgraph:{{__JANUSGRAPH_version}}
    container_name: gremlin-console
    hostname: gremlin-console
    labels:
      com.platys.name: "gremlin-console"
      com.platys.description: "Gremlin Console"
    environment:
      GREMLIN_REMOTE_HOSTS: {{GREMLIN_CONSOLE_remote_host}}
    {% if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: tail -f /dev/null
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  GREMLIN_CONSOLE_enable #}

{% if INVANA_ENGINE_enable | default(false) %}
  #  ================================== Invana Engine ========================================== #
  invana-engine:
    image: invanalabs/invana-engine:{{__INVANA_ENGINE_version}}
    container_name: invana-engine
    hostname: invana-engine
    labels:
      com.platys.name: "invana-engine"
      com.platys.description: "Open source graph visualiser"
      com.platys.webui.title: "Invana Engine GraphiQL UI"
      com.platys.webui.url: "http://dataplatform:28291/graphql"
      com.platys.restapi.title: "Invana Engine GraphQL API"
      com.platys.restapi.url: "http://dataplatform:28291/graphql"
    ports:
      - "28291:8200"
    environment:
      GREMLIN_SERVER_URL: 'ws://janusgraph:8182/gremlin'
    {% if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  INVANA_ENGINE_enable #}

{% if INVANA_STUDIO_enable | default(false) %}
  #  ================================== Invana Studio ========================================== #
  invana-studio:
    image: invanalabs/invana-studio:{{__INVANA_STUDIO_version}}
    container_name: invana-studio
    hostname: invana-studio
    labels:
      com.platys.name: "invana-studio"
      com.platys.description: "Open source graph visualiser"
      com.platys.webui.title: "Invana Studio UI"
      com.platys.webui.url: "http://dataplatform:28292"
    ports:
      - "28292:8300"
    {% if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  INVANA_STUDIO_enable #}

{% if NEO4J_enable | default(false) %}
  #  ================================== Neo4J ========================================== #
  {% for num in range(NEO4J_nodes | default(1) ) %}
    {% set external_port_neo4j = 7474 + loop.index - 1 %}
    {% set external_port_bolt = 7687 + loop.index - 1 %}
  neo4j-{{loop.index}}:
    image: neo4j:{{__NEO4J_version}}{{ '-enterprise' if (NEO4J_edition | lower) == 'enterprise'}}
    hostname: neo4j-{{loop.index}}
    container_name: neo4j-{{loop.index}}
    labels:
      com.platys.name: "neo4j"
      com.platys.description: "Native graph database management system"
      com.platys.webui.title: "Neo4J UI"
      com.platys.webui.url: "http://dataplatform:{{external_port_neo4j}}"
      com.platys.password.envvars: "PLATYS_NEO4J_PASSWORD"
    ports:
      - "{{external_port_neo4j}}:7474"
      - "{{external_port_bolt}}:7687"
    environment:
      - NEO4J_server_bolt_listen__address=0.0.0.0:7687
    {% if (NEO4J_edition | lower) == 'enterprise'  %}
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
    {% endif -%}   {#  use_timezone #}
      - NEO4J_AUTH=neo4j/${PLATYS_NEO4J_PASSWORD:-{{NEO4J_password}}}
    {% if (NEO4J_major_version | lower) == '5' %}
      {% if not NEO4J_server_config_strict_validation_enabled %}
      - NEO4J_server_config_strict__validation_enabled=false
      {% endif -%}   {#  use_timezone #}
      - NEO4J_PLUGINS=[{{NEO4J_download_plugins.split(",") | map("to_json") | join(',')}}]
    {% elif (NEO4J_major_version | lower) == '4' %}
      - NEO4JLABS_PLUGINS=[{{NEO4J_download_plugins.split(",") | map("to_json") | join(',')}}]
    {% endif -%}   {#  NEO4J_major_version and NEO4J_server_config_strict_validation_enabled #}
    {% if NEO4J_dbms_security_procedures_allowlist is defined and NEO4J_dbms_security_procedures_allowlist and NEO4J_dbms_security_procedures_allowlist | length %}
      - NEO4J_dbms_security_procedures_allowlist={{NEO4J_dbms_security_procedures_unrestricted}}
    {% endif -%}   {#  NEO4J_dbms_security_procedures_allowlist #}
    {% if NEO4J_dbms_security_procedures_unrestricted is defined and NEO4J_dbms_security_procedures_unrestricted and NEO4J_dbms_security_procedures_unrestricted | length %}
      - NEO4J_dbms_security_procedures_unrestricted={{NEO4J_dbms_security_procedures_unrestricted}}
    {% endif -%}   {#  NEO4J_dbms_security_procedures_unrestricted #}
      - NEO4J_dbms_security_allow__csv__import__from__file__urls={{NEO4J_dbms_security_allow_csv_import_from_file_urls}}
      - NEO4J_server_directories_import={{NEO4J_server_directories_import}}
    {% if NEO4J_server_memory_pagecache_size is defined and NEO4J_server_memory_pagecache_size and NEO4J_server_memory_pagecache_size | length %}
      - NEO4J_server_memory_pagecache_size={{NEO4J_server_memory_pagecache_size}}
    {% endif -%}   {#  NEO4J_dbms_server_pagecache_size #}
    {% if NEO4J_server_memory_heap_initial_size is defined and NEO4J_server_memory_heap_initial_size and NEO4J_server_memory_heap_initial_size | length %}
      - NEO4J_server_memory_heap_initial__size={{NEO4J_server_memory_heap_initial_size}}
    {% endif -%}   {#  NEO4J_server_memory_heap_initial_size #}
    {% if NEO4J_server_memory_heap_max_size is defined and NEO4J_server_memory_heap_max_size and NEO4J_server_memory_heap_max_size | length %}
      - NEO4J_server_memory_heap_max__size={{NEO4J_server_memory_heap_max_size}}
    {% endif -%}   {#  NEO4J_server_memory_heap_max_size #}
      - NEO4J_server_logs_debug_enabled={{NEO4J_server_logs_debug_enabled | default(false)}}
      - EXTENSION_SCRIPT={{NEO4J_extension_script}}

      - NEO4J_apoc_export_file_enabled={{NEO4J_apoc_export_file_enabled | default(false)}}
      - NEO4J_apoc_import_file_enabled={{NEO4J_apoc_import_file_enabled | default(false)}}
      - NEO4J_apoc_import_file_use__neo4j__config={{NEO4J_apoc_import_file_use_neo4j_config | default(true)}}
      - NEO4J_apoc_import_file_allow__read__from__filesystem={{NEO4J_apoc_import_file_allow_read_from_fs | default(true)}}
      - NEO4J_apoc_trigger_enabled={{NEO4J_apoc_trigger_enabled | default(false)}}
      - NEO4J_apoc_trigger_refresh={{NEO4J_apoc_trigger_refresh}}

    {% if (KAFKA_enable or external['KAFKA_enable']) and 'streams' in NEO4J_download_plugins | default(false) %}
      - NEO4J_kafka_bootstrap_servers='{{ns.bootstrapServers}}'
      - NEO4J_kafka_acks={{NEO4J_kafka_acks}}
      - NEO4J_kafka_streams_log_compaction_strategy='compact'
      {% if NEO4J_kafka_transactional_id is defined and NEO4J_kafka_transactional_id | length %}
      - NEO4J_kafka_transactional_id='{{NEO4J_kafka_transactional_id}}'
      {% endif -%}   {#  NEO4J_kafka_transactional_id #}
      {% if NEO4J_streams_enabled is defined and NEO4J_streams_enabledj | length %}
      - NEO4J_streams_source_enabled='{{NEO4J_streams_enabledj}}'
      {% endif -%}   {#  NEO4J_streams_enabled #}
      - NEO4J_streams_source_topic_nodes_{{NEO4J_topic_name}}={{NEO4J_streams_source_topic_nodes}}
      - NEO4J_streams_source_topic_relationships_{{NEO4J_topic_name}}={{NEO4J_streams_source_topic_relationships}}
      - NEO4J_streams_source_schema_polling_interval=10000
    {% endif -%}   {#  KAFKA_enable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - "./init/neo4j:/init"
    {% if NEO4J_mount_plugins is defined and NEO4J_mount_plugins and NEO4J_mount_plugins | length | default(false) %}
      {% for plugin in NEO4J_mount_plugins.split(",") %}
      - "./plugins/neo4j/{{plugin}}:/plugins/{{plugin}}:rw"
      {% endfor %}
    {% endif %}
    {% if NEO4J_volume_map_data %}
      - "./container-volume/neo4j/data:/data"
    {% endif %}
    {% if NEO4J_volume_map_logs %}
      - "./container-volume/neo4j/logs:/logs"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
        test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
        interval: 15s
        timeout: 30s
        retries: 10
  {% endfor %}  {#  for NEO4J_nodes #}
{% endif %}   {#  NEO4J_enable #}

{% if NEODASH_enable | default(false) %}
  #  ================================== NeoDash ========================================== #
  neodash:
    image: neo4jlabs/neodash:{{__NEODASH_version}}
    platform: linux/amd64    
    hostname: neodash
    container_name: neodash
    labels:
      com.platys.name: "neodash"
      com.platys.description: "Visualize you Neo4J data"
      com.platys.webui.title: "NeoDash UI"
      com.platys.webui.url: "http://dataplatform:5005"
    ports:
      - "5005:5005"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  NEODASH_enable #}

{% if QUINE_enable | default(false) %}
  #  ================================== Quine ========================================== #
  quine-1:
    image: thatdot/quine:{{__QUINE_version}}
    hostname: quine-1
    container_name: quine-1
    labels:
      com.platys.name: "quine"
      com.platys.description: "Streaming graph interpreter"
      com.platys.webui.title: "Quine UI"
      com.platys.webui.url: "http://dataplatform:28268"
    ports:
      - "28268:8080"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  QUINE_enable #}

{% if MEMGRAPH_enable | default(false) %}
  #  ================================== Memgraph ========================================== #
  memgraph:
  {% if (MEMGRAPH_edition | lower) == 'platform'  %}
    image: memgraph/memgraph-platform:{{__MEMGRAPH_PLATFORM_version}}
  {% elif (MEMGRAPH_edition | lower) == 'db' and not MEMGRAPH_with_mage | default(false) %}
    image: memgraph/memgraph:{{__MEMGRAPH_version}}
  {% elif (MEMGRAPH_edition | lower) == 'db' and  MEMGRAPH_with_mage | default(false) %}
    image: memgraph/memgraph-mage:{{__MEMGRAPH_MAGE_version}}-memgraph{{__MEMGRAPH_version}}
  {% endif %}
    hostname: memgraph
    container_name: memgraph
    labels:
      com.platys.name: "memgraph"
      com.platys.description: "In-memory graph database built for streaming"
  {% if (MEMGRAPH_edition | lower) == 'platform'  %}
      com.platys.webui.title: "Memgraph Lab UI"
      com.platys.webui.url: "http://dataplatform:28288"
  {% endif %}
    ports:
      - "7699:7687"
  {% if (MEMGRAPH_edition | lower) == 'platform'  %}
      - "28288:3000"
  {% endif %}
      - "7444:7444"
    environment:
      - MEMGRAPH="--log-level=TRACE"
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MEMGRAPH_volume_map_data %}
      - "./container-volume/memgraph/data:/var/lib/memgraph"
    {% endif -%}   {#  MEMGRAPH_volume_map_data #}
    {% if MEMGRAPH_volume_map_log %}
      - "./container-volume/memgraph/log:/var/log/memgraph"
    {% endif -%}   {#  MEMGRAPH_volume_map_log #}
    {% if MEMGRAPH_volume_map_custom_conf %}
      - "./custom-conf/memgraph/:/etc/memgraph"
    {% endif -%}   {#  MEMGRAPH_volume_map_log #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
  {% if (MEMGRAPH_edition | lower) == 'platform'  %}
    entrypoint: ["/usr/bin/supervisord"]
  {% elif (MEMGRAPH_edition | lower) == 'db' and  MEMGRAPH_with_mage | default(false) %}
    entrypoint: ["/usr/lib/memgraph/memgraph", "--log-level=TRACE"]
  {% elif (MEMGRAPH_edition | lower) == 'db' and not MEMGRAPH_with_mage | default(false) %}
    entrypoint: ["/usr/lib/memgraph/memgraph", "--log-level=TRACE"]
  {% endif %}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MEMGRPAH_enable #}

{% if ARCADEDB_enable | default(false) %}
  #  ================================== ArcadeDB ========================================== #
  arcadedb:
    image: arcadedata/arcadedb:{{__ARCADEDB_version}}
    hostname: arcadedb
    container_name: arcadedb
    labels:
      com.platys.name: "arcadedb"
      com.platys.description: "Multi-Model Database (Document, Graph, Key-Value, Relational)"
      com.platys.webui.title: "ArcadeDB Studio UI"
      com.platys.webui.url: "http://dataplatform:2480"
      com.platys.restapi.title: "ArcadeDB HTTP Protocol"
      com.platys.restapi.url: "http://dataplatform:2480/api/vi"
    ports:
      - "2480:2480"
      - "2481:5432"   # postgres port
      - "2482:6379"   # redis port
      - "2483:27017"  # mongodb port
      - "2424:2424"
    environment:
      JAVA_OPTS: >
             -Darcadedb.server.rootPassword={{ARCADEDB_root_password}}
             -Darcadedb.server.databaseDirectory=/databases
             -Darcadedb.server.plugins=Redis:com.arcadedb.redis.RedisProtocolPlugin,MongoDB:com.arcadedb.mongo.MongoDBProtocolPlugin,Postgres:com.arcadedb.postgres.PostgresProtocolPlugin,GremlinServer:com.arcadedb.server.gremlin.GremlinServerPlugin
    {%if ARCADEDB_provision_sample_data | default(false) %}
             -Darcadedb.server.defaultDatabases=OpenBeer[root]{import:https://github.com/ArcadeData/arcadedb-datasets/raw/main/orientdb/OpenBeer.gz}
    {% endif -%}   {#  ARCADEDB_provision_sample_data #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ARCADEDB_volume_map_data %}
      - "./container-volume/arcadedb/data:/databases"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ARCADEDB_enable #}

{% if DGRAPH_enable | default(false) %}
  #  ================================== Dgraph ========================================== #
  dgraph-1:
    image: dgraph/standalone:{{__DGRAPH_version}}
    hostname: dgraph-1
    container_name: dgraph-1
    labels:
      com.platys.name: "dgraph"
      com.platys.description: "Native GraphQL Database With A Graph Backend"
      com.platys.webui.title: "Dgraph Ratel UI"
      com.platys.webui.url: "http://dataplatform:28182"
    ports:
      - "28180:8080"
      - "28181:9080"
      - "28182:8000"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DGRAPH_enable #}

{% if STARDOG_enable | default(false) %}
  #  ================================== Stardog ========================================== #
  stardog-1:
    image: stardog/stardog:{{__STARDOG_version}}
    hostname: stardog-1
    container_name: stardog-1
    labels:
      com.platys.name: "stardog"
      com.platys.description: "Graph Database"
      com.platys.webui.title: "Stardog API"
      com.platys.webui.url: "http://dataplatform:5820"
    ports:
      - "5820:5820"
    environment:
      STARDOG_SERVER_JAVA_ARGS: "-Djava.io.tmpdir=/var/opt/tmp -Xms4g -Xmx4g -XX:MaxDirectMemorySize=8g"
      STARDOG_EXT: "/var/opt/stardog-ext"
    {%if use_timezone | default(false) %}
      TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./license/stardog-license-key.bin:/var/opt/stardog/stardog-license-key.bin
      - "./plugins/stardog/:/var/opt/stardog-ext"
    {% if STARDOG_volume_map_data %}
      - "./container-volume/stardog/data:/var/opt/stardog"
      - "./container-volume/stardog/tmp:/var/opt/tmp"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if STARDOG_STUDIO_enable | default(false) %}
  stardog-studio:
    image: stardog/stardog-studio:{{__STARDOG_STUDIO_version}}
    hostname: stardog-studio
    container_name: stardog-studio
    labels:
      com.platys.name: "stardog-studio"
      com.platys.description: "Stardog Studio UI"
      com.platys.webui.title: "Stardog Studio UI"
      com.platys.webui.url: "http://dataplatform:28170"
    ports:
      - "28170:80"
    {%if use_timezone | default(false) %}
    environment:
      TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  STARDOG_STUDIO_enable #}
{% endif %}   {#  STARDOG_enable #}

{% if GRAPHDB_enable | default(false) %}
  #  ================================== GraphDB ========================================== #
  graphdb-1:
  {% if (GRAPHDB_edition | lower) == 'free'  %}
    image: khaller/graphdb-free:{{__GRAPHDB_FREE_version}}
  {% elif (GRAPHDB_edition | lower) == 'se' or 'ee' %}
    image: ontotext/graphdb:{{__GRAPHDB_version}}
  {% endif -%}   {#  GRAPHDB_enable #}
    hostname: graphdb-1
    container_name: graphdb-1
    labels:
      com.platys.name: "graphdb"
      com.platys.description: "Triple/RDF Store"
      com.platys.webui.title: "GraphDBs UI"
      com.platys.webui.url: "http://dataplatform:17200"
    ports:
      - "17200:7200"
    environment:
      GDB_HEAP_SIZE: "{{GRAPHDB_heap_size}}"
      GDB_JAVA_OPTS: >-
        -Xmx2g -Xms2g
        -Dgraphdb.home=/opt/graphdb
        -Dgraphdb.workbench.importDirectory={{GRAPHDB_workbench_import_dir}}
        -Dgraphdb.workbench.cors.enable=true
        -Denable-context-index=true
        -Dentity-pool-implementation=transactional
        -Dhealth.max.query.time.seconds=60
        -Dgraphdb.append.request.id.headers=true
        -Dreuse.vars.in.subselects=true
    {%if use_timezone | default(false) %}
      TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if (GRAPHDB_edition | lower) == 'free'  %}
      - ./init/graphdb:/repository.init/
    {% endif %}
    {% if (GRAPHDB_edition | lower) == 'se' or (GRAPHDB_edition | lower) == 'ee' %}
      - ./licences/graphdb:/opt/graphdb/home
    {% endif %}
    {% if GRAPHDB_volume_map_data %}
      - "./container-volume/graphdb/data:/opt/graphdb/data"
      - "./container-volume/graphdb/logs:/opt/graphdb/logs"
      - "./container-volume/graphdb/work:/opt/graphdb/work"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  GRAPHDB_enable #}

{% if JENA_FUSEKI_enable | default(false) %}
  #  ================================== Fuseki ========================================== #
  jena-fuseki-1:
    image: stain/jena-fuseki:{{__JENA_FUSEKI_version}}
    hostname: jena-fuseki-1
    container_name: jena-fuseki-1
    labels:
      com.platys.name: "jena-fuseki"
      com.platys.description: "Triple/RDF Store"
      com.platys.webui.title: "Jena-Fuseki UI"
      com.platys.webui.url: "http://dataplatform:3030"
      com.platys.restapi.title: "Jena-Fuseki REST API"
      com.platys.restapi.url: "http://dataplatform:3030/"
    ports:
      - "3030:3030"
    environment:
    {% if JENA_FUSEKI_datasets is defined and JENA_FUSEKI_datasets and JENA_FUSEKI_datasets | length | default(false) %}
      {% for dataset in JENA_FUSEKI_datasets.split(",") %}
      - FUSEKI_DATASET_{{loop.index}}={{dataset}}
      {% endfor -%}
    {% endif -%}   {#  JENA_FUSEKI_datasets #}
      - ADMIN_PASSWORD={{JENA_FUSEKI_admin_password if JENA_FUSEKI_admin_password is defined and JENA_FUSEKI_admin_password and JENA_FUSEKI_admin_password | length else omit }}
      - TDB={{2 if JENA_FUSEKI_tdb2_enabled else omit }}
      - JVM_ARGS=-Xmx2g # Default: 1200MiB
    {%if use_timezone | default(false) %}
      TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./data-transfer:/staging
    {% if JENA_FUSEKI_volume_map_data %}
      - "./container-volume/jena-fuseki/data:/fuseki"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  JENA_FUSEKI_enable #}

{% if INFLUXDB_enable | default(false) %}
  #  ================================== InfluxData InfluxDB ========================================== #
  influxdb:
    image: influxdb:{{__INFLUXDB_version}}
    container_name: influxdb
    hostname: influxdb
    labels:
      com.platys.name: "influxdb"
      com.platys.description: "Timeseries Database"
      com.platys.restapi.title: "InfluxDB Rest API"
      com.platys.restapi.url: "http://dataplatform:8086/health"
    ports:
      - "8086:8086"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/influxdb/influxdb.conf:/etc/influxdb/influxdb.conf
    {% if INFLUXDB_volume_map_data %}
      - "./container-volume/influxdb:/var/lib/influxdb"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  influxdb-ui:
    image: timeseriesadmin/timeseriesadmin:{{__INFLUXDB_UI_version}}
    container_name: influxdb-ui
    hostname: influxdb-ui
    labels:
      com.platys.name: "influxdb"
      com.platys.description: "Timeseries Database UI"
      com.platys.webui.title: "InfluxDB UI"
      com.platys.webui.url: "http://dataplatform:28128"
    ports:
      - "28128:80"
    {%if use_timezone | default(false) %}
    evironment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  INFLUXDB_enable #}

{% if INFLUXDB_CHRONOGRAF_enable | default(false) %}
  #  ================================== InfluxData Chronograf ========================================== #
  chronograf:
    image: chronograf:{{__CHRONOGRAF_version}}
    hostname: chronograf
    container_name: chronograf
    labels:
      com.platys.name: "chronograf"
      com.platys.description: "Timeseries Visualization for InfluxDB 1.x"
      com.platys.webui.title: "Chronograf UI"
      com.platys.webui.url: "http://dataplatform:28129"
    depends_on:
      - influxdb
    ports:
      - "28129:8888"
    environment:
      RESOURCES_PATH: "/usr/share/chronograf/resources"
      PORT: 8888
      HOST: chronograf
      INFLUXDB_URL: "http://influxdb:8086"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if INFLUXDB_CHRONOGRAF_volume_map_data %}
      - "./container-volume/chronograf:/var/lib/chronograf"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  INFLUXDB_CHRONOGRAF_enable #}

{% if INFLUXDB_KAPACITOR_enable | default(false) %}
  #  ================================== InfluxData Kapacitor ========================================== #
  kapacitor:
    image: kapacitor:{{__KAPACITOR_version}}
    hostname: kapacitor
    container_name: kapacitor
    labels:
      com.platys.name: "kapacitor"
      com.platys.description: "Native data processing engine for InfluxDB 1.x"
      com.platys.restapi.title: "Kapacitor REST API"
      com.platys.restapi.url: "http://dataplatform:28130/kapacitor/v1"
    ports:
      - "28130:9092"
    environment:
      KAPACITOR_HOSTNAME: kapacitor
      KAPACITOR_DATA_DIR: "var/lib/kapacitor"
      KAPACITOR_LOGGING_LEVEL: INFO
      KAPACITOR_INFLUXDB_0_URLS_0: http://influxdb:8086
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if INFLUXDB_KAPACITOR_volume_map_data %}
      - "./container-volume/kapacitor:/var/lib/kapacitor"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# INFLUXDB_KAPACITOR_enable #}

{% if INFLUXDB_TELEGRAF_enable | default(false) %}
  #  ================================== InfluxData Telegraf ========================================== #
  telegraf:
    image: telegraf:{{__TELEGRAF_version}}
    container_name: telegraf
    hostname: telegraf
    labels:
      com.platys.name: "telegraf"
      com.platys.description: "Agent for collecting, processing, aggregating, and writing metrics"
    environment:
      HOSTNAME: telegraf
      INFLUXDB_TOKEN: ${PLATYS_INFLUXDB_TOKEN:-{{INFLUXDB_TELEGRAF_influx_token}}}
      INFLUXDB_ORGANIZATION: {{INFLUXDB2_org}}
      INFLUXDB_BUCKET: {{INFLUXDB2_bucket}}
    {% if INFLUXDB_TELEGRAF_monitor_docker_host_enabled | default(false) %}
      HOST_ETC: /hostfs/etc
      HOST_PROC: /hostfs/proc
      HOST_SYS: /hostfs/sys
      HOST_VAR: /hostfs/var
      HOST_RUN: /hostfs/run
      HOST_MOUNT_PREFIX: /hostfs
    {% endif -%}   {#  INFLUXDB_TELEGRAF_monitor_docker_host_enabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if INFLUXDB_TELEGRAF_custom_conf_file is defined and INFLUXDB_TELEGRAF_custom_conf_file and INFLUXDB_TELEGRAF_custom_conf_file | length | default(false) %}
      - ./custom-conf/telegraf/{{INFLUXDB_TELEGRAF_custom_conf_file}}:/etc/telegraf/telegraf.conf
    {% else %}
      - ./conf/telegraf/telegraf.conf:/etc/telegraf/telegraf.conf
    {% endif -%}   {#  INFLUXDB_TELEGRAF_custom_conf_file #}
      # Mount for Docker API access
      - /var/run/docker.sock:/var/run/docker.sock
    {% if INFLUXDB_TELEGRAF_monitor_docker_host_enabled | default(false) %}
      - /:/hostfs:ro
    {% endif -%}   {#  INFLUXDB_TELEGRAF_monitor_docker_host_enabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  INFLUXDB_TELEGRAF_enable #}

{% if INFLUXDB2_enable | default(false) %}
  #  ================================== InfluxDB 2 ========================================== #
  influxdb2:
    image: influxdb:{{__INFLUXDB2_version}}
    hostname: influxdb2
    container_name: influxdb2
    labels:
      com.platys.name: "influxdb2"
      com.platys.description: "Timeseries Database"
      com.platys.webui.title: "InfluxDB 2.0 UI"
      com.platys.webui.url: "http://dataplatform:19999"
      com.platys.restapi.title: "InfluxDB 2.0 Rest API"
      com.platys.restapi.url: "http://dataplatform:19999/api/v2"
    ports:
      - "19999:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: {{INFLUXDB2_username}}
      DOCKER_INFLUXDB_INIT_PASSWORD: {{INFLUXDB2_password}}
      DOCKER_INFLUXDB_INIT_ORG: {{INFLUXDB2_org}}
      DOCKER_INFLUXDB_INIT_BUCKET: {{INFLUXDB2_bucket}}
      DOCKER_INFLUXDB_INIT_RETENTION: 1w
    {% if INFLUXDB2_admin_token is defined and INFLUXDB2_admin_token | length %}
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: {{INFLUXDB2_admin_token}}
    {% endif -%}   {#  DOCKER_INFLUXDB_INIT_ADMIN_TOKEN #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if INFLUXDB2_volume_map_config %}
      - "./conf/influxdb2/config.yml:/etc/influxdb2/config.yml"
    {% endif %}
    {% if INFLUXDB2_volume_map_data %}
      - "./container-volume/influxdb2:/var/lib/influxdb2"
    {% endif %}
      - "./init/influxdb2:/docker-entrypoint-initdb.d"
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: "--reporting-disabled"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# INFLUXDB2_enable #}

{% if QUESTDB_enable | default(false) %}
  #  ================================== QuestDB Timeseries ========================================== #
  questdb:
    image: questdb/questdb:{{__QUESTDB_version}}
    container_name: questdb
    hostname: questdb
    labels:
      com.platys.name: "questdb"
      com.platys.description: "Timeseries Database"
      com.platys.webui.title: "QuestDB Web Console"
      com.platys.webui.url: "http://dataplatform:28226"
      com.platys.restapi.title: "QuestDB Rest API"
      com.platys.restapi.url: "http://dataplatform:28226"
    ports:
      - "28226:9000"
      - "8812:8812"
      - "9009:9009"
    {%if use_timezone | default(false) %}
    evironment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if QUESTDB_volume_map_data %}
      - "./container-volume/questdb/data:/root/.questdb/db"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  QUESTDB_enable #}

{% if KUDU_enable | default(false) %}
  #  ================================== Apache Kudu ========================================== #
  kudu-master-1:
    image: apache/kudu:{{__KUDU_version}}
    hostname: kudu-master-1
    container_name: kudu-master-1
    labels:
      com.platys.name: "kudu"
      com.platys.description: "Distributed data storage engine"
    ports:
      - "7051:7051"
      - "8051:8051"
    command: ["master"]
    environment:
      - KUDU_MASTERS=kudu-master-1,kudu-master-2,kudu-master-3
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  kudu-master-2:
    image: apache/kudu:{{__KUDU_version}}
    hostname: kudu-master-2
    container_name: kudu-master-2
    labels:
      com.platys.name: "kudu"
      com.platys.description: "Distributed data storage engine"
    ports:
      - "7051"
      - "8051"
    command: ["master"]
    environment:
      - KUDU_MASTERS=kudu-master-1,kudu-master-2,kudu-master-3
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  kudu-master-3:
    image: apache/kudu:{{__KUDU_version}}
    hostname: kudu-master-3
    container_name: kudu-master-3
    labels:
      com.platys.name: "kudu"
      com.platys.description: "Distributed data storage engine"
    ports:
      - "7051"
      - "8051"
    command: ["master"]
    environment:
      - KUDU_MASTERS=kudu-master-1,kudu-master-2,kudu-master-3
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  kudu-tserver-1:
    image: apache/kudu:latest
    hostname: kudu-tserver-1
    container_name: kudu-tserver-1
    labels:
      com.platys.name: "kudu"
      com.platys.description: "Distributed data storage engine"
    depends_on:
      - kudu-master-1
      - kudu-master-2
      - kudu-master-3
    ports:
      - "7050:7050"
      - "8050:8050"
    command: ["tserver"]
    environment:
      - KUDU_MASTERS=kudu-master-1,kudu-master-2,kudu-master-3
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# KUDU_enable #}

{% if CHROMA_enable | default(false) %}
  #  ================================== Chroma VectorDB ========================================== #
  chroma:
    image: ghcr.io/chroma-core/chroma:{{__CHROMA_version}}
    container_name: chroma
    hostname: chroma
    labels:
      com.platys.name: "chroma"
      com.platys.description: "An AI-native vector database for embeddings"
      com.platys.restapi.title: "Chroma API"
      com.platys.restapi.url: "http://dataplatform:28339/docs"
    ports:
      - "28339:8000"
    environment:
      IS_PERSISTENT: true
      PERSIST_DIRECTORY: /chroma/chroma
    {% if CHROMA_auth_token is defined and CHROMA_auth_token and CHROMA_auth_token | length | default(false) %}
      CHROMA_SERVER_AUTH_CREDENTIALS: {{CHROMA_auth_token}}
      CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER: "AUTHORIZATION"
      CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER: "chromadb.auth.token.TokenConfigServerAuthCredentialsProvider"
      CHROMA_SERVER_AUTH_PROVIDER: "chromadb.auth.token.TokenAuthServerProvider"
    {% endif -%}   {#  CHROMA_auth_token #}
    {% if CHROMA_enable_telemetry | default(false) %}
      CHROMA_OTEL_COLLECTION_ENDPOINT: http://otel-collector:4317/
      CHROMA_OTEL_EXPORTER_HEADERS:
      CHROMA_OTEL_SERVICE_NAME: chroma
      CHROMA_OTEL_GRANULARITY: all
    {% endif -%}   {#  CHROMA_enable_telemetry #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if CHROMA_volume_map_data %}
      - "./container-volume/chroma/data:/chroma/chroma/"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CHROMA_enable #}

{% if QDRANT_enable | default(false) %}
  #  ================================== Qdrant VectorDB ========================================== #
  qdrant:
    image: qdrant/qdrant:{{__QDRANT_version}}
    container_name: qdrant
    hostname: qdrant
    labels:
      com.platys.name: "qdrant"
      com.platys.description: "AI-native vector database"
      com.platys.webui.title: "Qdrant UI"
      com.platys.webui.url: "http://dataplatform:6333/dashboard"
      com.platys.restapi.title: "Qdrant API"
      com.platys.restapi.url: "http://dataplatform:6333"
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      IS_PERSISTENT: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if QDRANT_volume_map_data %}
      - "./container-volume/qdrant/data:/qdrant/storage"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  QDRANT_enable #}

{% if WEAVIATE_enable | default(false) %}
  #  ================================== Weaviate VectorDB ========================================== #
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:{{__WEAVIATE_version}}
    container_name: weaviate
    hostname: weaviate
    labels:
      com.platys.name: "weaviate"
      com.platys.description: "AI-native vector database"
      com.platys.webui.title: "Weaviate UI"
      com.platys.webui.url: "http://dataplatform:28354"
    ports:
      - "28354:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: '{{WEAVIATE_modules if WEAVIATE_modules is defined and WEAVIATE_modules and WEAVIATE_modules | length else omit}}'
      CLUSTER_HOSTNAME: 'node1'
      LOG_LEVEL: info
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if WEAVIATE_volume_map_data %}
      - "./container-volume/weaviate/data:/var/lib/weaviate"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8080'
      - --scheme
      - http
    restart: {{container_restart_policy}}
{% endif %}   {#  WEAVIATE_enable #}

{% if MILVUS_enable | default(false) %}
  #  ================================== Milvus VectorDB ========================================== #
  milvus:
    image: milvusdb/milvus:{{__MILVUS_version}}
    container_name: milvus
    hostname: milvus
    labels:
      com.platys.name: "milvus"
    ports:
      # grpc
      - "28341:19530"
      # metrics
      - "28342:9091"
    environment:
      ETCD_ENDPOINTS: etcd-1:2379
      MINIO_ADDRESS: {{ (s3Endpoint | replace('http://','')) if s3Endpoint is defined and s3Endpoint else omit}}
      MINIO_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      MINIO_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MILVUS_volume_map_data %}
      - "./container-volume/milvus/data:/var/lib/milvus"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: ["milvus", "run", "standalone"]
    restart: {{container_restart_policy}}

  {% if ATTU_enable | default(false) %}
  attu:
    image: zilliz/attu:{{__ATTU_version}}
    container_name: attu
    hostname: attu
    labels:
      com.platys.name: "attu"
      com.platys.webui.title: "Attu UI"
      com.platys.webui.url: "http://dataplatform:28343"
    ports:
      - "28343:3000"
    environment:
      MILVUS_URL: milvus:19530
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MILVUS_volume_map_data %}
      - "./container-volume/milvus/data:/var/lib/milvus"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  ATTU_enable #}
{% endif %}   {#  MILVUS_enable #}

{% if INFINITY_enable | default(false) %}
  #  ================================== Infinity ========================================== #
  infinity:
    image: infiniflow/infinity:{{__INFINITY_version}}
    container_name: infinity
    hostname: infinity
    labels:
      com.platys.name: "infinity"
      com.platys.description: "AI-native database for LLM applications"
      com.platys.webui.title: "Infinity UI"
      com.platys.webui.url: "http://dataplatform:23820"
    ports:
      # Thrift
      - 23817:23817
      # HTTP
      - 23820:23820
      # psql
      - 5436:5432
    environment:
      IS_PERSISTENT: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if INFINITY_volume_map_data %}
      - "./container-volume/infinity/data:/var/infinity"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    mem_limit: {{INFINITY_mem_limit}}
    ulimits:
      nofile:
        soft: 500000
        hard: 500000    
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "curl", "http://localhost:23820/admin/node/current"]
      interval: 10s
      timeout: 10s
      retries: 120    
{% endif %}   {#  INFINITY_enable #}

{% if VECTOR_ADMIN_enable | default(false) %}
  vector-admin:
    image: mintplexlabs/vectoradmin:{{__VECTOR_ADMIN_version}}
    container_name: vector-admin
    hostname: vector-admin
    labels:
      com.platys.name: "vector-admin"
      com.platys.description: "Graphical Frontend for vector databases"
      com.platys.webui.title: "Vector Admin UI"
      com.platys.webui.url: "http://dataplatform:28350"
    ports:
      - "28350:3001"
      - "3355:3355"
      - "8288:8288"
    environment:
      SERVER_PORT: 3001
      DATABASE_CONNECTION_STRING: "postgresql://{{VECTOR_ADMIN_postgresql_user if VECTOR_ADMIN_postgresql_user is defined and VECTOR_ADMIN_postgresql_user and VECTOR_ADMIN_postgresql_user | length else POSTGRESQL_user}}:{{VECTOR_ADMIN_postgresql_password if VECTOR_ADMIN_postgresql_password is defined and VECTOR_ADMIN_postgresql_password and VECTOR_ADMIN_postgresql_password | length else POSTGRESQL_password}}@postgresql:5432/{{VECTOR_ADMIN_postgresql_database if VECTOR_ADMIN_postgresql_database is defined and VECTOR_ADMIN_postgresql_database and VECTOR_ADMIN_postgresql_database | length else POSTGRESQL_database}}"
      STORAGE_DIR: "./backend/storage"
      JWT_SECRET: abc123!
      INNGEST_LANDING_PAGE: true
      INNGEST_EVENT_KEY: background_workers
      INNGEST_SIGNING_KEY: random-string-goes-here
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MILVUS_volume_map_data %}
      - "./container-volume/milvus/data:/var/lib/milvus"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  VECTOR_ADMIN_enable #}

{% if DUCKDB_enable | default(false) %}
  duckdb:
    image: {{'datacatering/duckdb' if not DUCKDB_ui_enabled else 'ghcr.io/gschmutz/duckdb-ui-remote-docker'}}:{{__DUCKDB_version}}
    container_name: duckdb
    hostname: duckdb
    labels:
      com.platys.name: "duckdb"
      com.platys.description: "Analytical in-process SQL DBMS"
    {%if DUCKDB_ui_enabled | default(false) %}  
      com.platys.webui.title: "DuckDB UI"
      com.platys.webui.url: "https://dataplatform:28249"
    ports:
      - "28249:8443"
    {% endif -%}   {#  DUCKDB_ui_enabled #}
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DUCKDB_db_folder is defined and DUCKDB_db_folder and DUCKDB_db_folder | length %}
      - {{DUCKDB_db_folder}}:/db
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    {%if not DUCKDB_ui_enabled | default(false) %}  
    entrypoint: tail -F anything
    {% endif -%}   {#  DUCKDB_ui_enabled #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DUCKDB_enable #}

{% if QUACKFLIGHT_enable | default(false) %}
  quackflight:
    image: ghcr.io/quackscience/quackflight:{{__QUACKFLIGHT_version}}
    container_name: quackflight
    hostname: quackflight
    labels:
      com.platys.name: "quackflight"
      com.platys.description: "DuckDB API Server"
      com.platys.webui.title: "QuackFlight UI"
      com.platys.webui.url: "http://dataplatform:28261"
      com.platys.restapi.title: "QuackFlight UI"
      com.platys.restapi.url: "http://dataplatform:28261"
    ports:
      - "28261:8123"
      - "8815:8815"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#    {% if DUCKDB_db_folder is defined and DUCKDB_db_folder and DUCKDB_db_folder | length %}
#      - {{DUCKDB_db_folder}}:/db
#    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  QUACKFLIGHT_enable #}

{% if DRUID_enable | default(false) %}
  #  ================================== Druid ========================================== #
  {% if (DRUID_edition| lower) == 'oss-sandbox' | default(false) %}
  druid-sandbox:
    image: fokkodriesprong/docker-druid:{{__DRUID_version}}
    hostname: druid-sandbox
    container_name: druid-sandbox
    labels:
      com.platys.name: "druid"
      com.platys.description: "Real-time analytics database"
      com.platys.webui.title: "Druid Console UI"
      com.platys.webui.url: "http://dataplatform:28150"
    ports:
      - "28150:8888"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {# DRUID_edition #}

  {% if (DRUID_edition| lower) == 'oss-cluster' | default(false) %}
  druid-coordinator:
    image: apache/druid:{{__DRUID_version}}
    hostname: druid-coordinator
    container_name: druid-coordinator
    labels:
      com.platys.name: "druid"
      com.platys.description: "Real-time analytics database"
    depends_on:
      - zookeeper{{dataCenter}}-1
      - druid-postgres
    ports:
      - "28146:8081"
    env_file:
      - ./conf/druid/druid.env
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/druid/storage:/opt/data
#      - coordinator_var:/opt/druid/var
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - coordinator
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  druid-broker-1:
    image: apache/druid:{{__DRUID_version}}
    hostname: druid-broker-1
    container_name: druid-broker-1
    labels:
      com.platys.name: "druid"
      com.platys.description: "Real-time analytics database"
    depends_on:
      - zookeeper{{dataCenter}}-1
      - druid-postgres
      - druid-coordinator
    ports:
      - "28147:8082"
    env_file:
      - ./conf/druid/druid.env
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - broker
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  druid-historical-1:
    image: apache/druid:{{__DRUID_version}}
    hostname: druid-historical-1
    container_name: druid-historical-1
    labels:
      com.platys.name: "druid"
      com.platys.description: "Real-time analytics database"
    volumes:
      - ./container-volume/druid/storage:/opt/data
#      - historical_var:/opt/druid/var
    depends_on:
      - zookeeper{{dataCenter}}-1
      - druid-postgres
      - druid-coordinator
    ports:
      - "28148:8083"
    env_file:
      - ./conf/druid/druid.env
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - historical
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  druid-middlemanager:
    image: apache/druid:{{__DRUID_version}}
    container_name: druid-middlemanager
    labels:
      com.platys.name: "druid"
      com.platys.description: "Real-time analytics database"
    depends_on:
      - zookeeper{{dataCenter}}-1
      - druid-postgres
      - druid-coordinator
    ports:
      - "28149:8091"
    env_file:
      - ./conf/druid/druid.env
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/druid/storage:/opt/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - middleManager
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  druid-router:
    image: apache/druid:{{__DRUID_version}}
    container_name: druid-router
    labels:
      com.platys.name: "druid"
      com.platys.description: "Real-time analytics database"
      com.platys.webui.title: "Druid Console UI"
      com.platys.webui.url: "http://dataplatform:28150/graph"
    depends_on:
      - zookeeper{{dataCenter}}-1
      - druid-postgres
      - druid-coordinator
    ports:
      - "28150:8888"
    env_file:
      - ./conf/druid/druid.env
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - router
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  druid-postgres:
    image: postgres:latest
    hostname: druid-postgres
    container_name: druid-postgres
    labels:
      com.platys.name: "druid"
      com.platys.description: "Relational database"
    environment:
      - POSTGRES_PASSWORD=abc123!
      - POSTGRES_USER=druid
      - POSTGRES_DB=druid
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/druid/postgresql:/var/lib/postgresql/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {# DRUID_edition #}
{% endif %}   {# DRUID_enable #}

{% if PINOT_enable | default(false) %}
  #  ================================== Pinot ========================================== #
  pinot-controller:
    image: apachepinot/pinot:{{__PINOT_version}}
    hostname: pinot-controller
    container_name: pinot-controller
    labels:
      com.platys.name: "pinot"
      com.platys.description: "Real-time analytics database"
      com.platys.webui.title: "Pinot Console UI"
      com.platys.webui.url: "http://dataplatform:28210"
    ports:
      - "28210:9000"
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms1G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-controller.log"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if PINOT_volume_map_data %}
      - ./container-volume/pinot/controller:/tmp/data/controller
    {% endif -%}   {#  PINOT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: StartController -zkAddress zookeeper-1:2181
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  pinot-broker-1:
    image: apachepinot/pinot:{{__PINOT_version}}
    hostname: pinot-broker-1
    container_name: pinot-broker-1
    labels:
      com.platys.name: "pinot"
      com.platys.description: "Real-time analytics database"
    depends_on:
      - pinot-controller
    ports:
      - "28211:8099"
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-broker.log"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: StartBroker -zkAddress zookeeper-1:2181
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% for num in range(PINOT_servers | default(1) ) %}
    {% set external_port = 28212 + loop.index - 1 %}
  pinot-server-{{loop.index}}:
    image: apachepinot/pinot:{{__PINOT_version}}
    hostname: pinot-server-{{loop.index}}
    container_name: pinot-server-{{loop.index}}
    labels:
      com.platys.name: "pinot"
      com.platys.description: "Real-time analytics database"
    depends_on:
      - pinot-controller
    ports:
      - "{{external_port}}:8098"
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx16G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-server.log"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if PINOT_volume_map_data %}
      - ./container-volume/pinot/server:/tmp/data/server
    {% endif -%}   {#  PINOT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: StartServer -zkAddress zookeeper-1:2181
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}
{% endif %}   {# PINOT_enable #}

{% if STARROCKS_enable | default(false) %}
  #  ================================== Starrocks ========================================== #
  {% if STARROCKS_edition == 'quickstart' | default(false) %}
  starrocks:
    container_name: starrocks
    hostname: starrocks
    image: starrocks/allin1-ubuntu:{{__STARROCKS_version}}
  #  platform: "linux/amd64"
    labels:
      com.platys.name: "starrocks"
      com.platys.description: "Real-time analytics database"
      com.platys.webui.title: "Starrocks Console UI"
      com.platys.webui.url: "http://dataplatform:28210"
    ports: # port mapping format "[host port]:[container port]", can be configured to your preferred port
      - "8030:8030"
      - "8040:8040"
      - "9030:9030"
    volumes:
      - ./data-transfer:/data-transfer
    {% if STARROCKS_volume_map_data %}  
      - ./container-volume/starrocks/be/storage:/data/deploy/starrocks/be/storage
      - ./container-volume/starrocks/be/log:/data/deploy/starrocks/be/log
      - ./container-volume/starrocks/fe/meta:/data/deploy/starrocks/fe/meta
      - ./container-volume/starrocks/fe/log:/data/deploy/starrocks/fe/log
    {% endif -%}   {#  PINOT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}      
    healthcheck:
      test: 'mysql -uroot -h127.0.0.1 -P 9030 -e "show backends\G" |grep "Alive: true"'
      interval: 10s
      timeout: 5s
      retries: 3
  {% endif %}  {#  STARROCKS_edition == 'quickstart' #}

  {% if STARROCKS_edition == 'standalone' | default(false) %}
  starrocks-fe:
    image: astarrocks/fe-ubuntu:{{__STARROCKS_version}}
    hostname: starrocks-fe
    container_name: starrocks-fe
    labels:
      com.platys.name: "starrocks"
      com.platys.description: "Real-time analytics database"
      com.platys.webui.title: "Starrocks Console UI"
      com.platys.webui.url: "http://dataplatform:28210"
    user: root
    ports:
      - 8030:8030
      - 9020:9020
      - 9030:9030
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if STARROCKS_volume_map_data %}
      - ./container-volume/starrocks/controller:/tmp/data/controller
    {% endif -%}   {#  PINOT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: >
      bash -c "echo run_mode=shared_data >> /opt/starrocks/fe/conf/fe.conf &&
      echo aws_s3_path=starrocks >> /opt/starrocks/fe/conf/fe.conf &&
      echo aws_s3_endpoint=minio:9000 >> /opt/starrocks/fe/conf/fe.conf &&
      echo aws_s3_use_instance_profile=false >> /opt/starrocks/fe/conf/fe.conf &&
      echo cloud_native_storage_type=S3 >> /opt/starrocks/fe/conf/fe.conf &&
      echo aws_s3_use_aws_sdk_default_behavior=true >> /opt/starrocks/fe/conf/fe.conf &&
      bash /opt/starrocks/fe/bin/start_fe.sh"
    restart: {{container_restart_policy}}
    healthcheck:
      test: 'mysql -uroot -hstarrocks-fe -P 9030 -e "show frontends\G" |grep "Alive: true"'
      interval: 10s
      timeout: 5s
      retries: 3

  starrocks-cn-1:
    image: starrocks/cn-ubuntu:{{__STARROCKS_version}}
    hostname: starrocks-cn-1
    container_name: starrocks-cn-1
    labels:
      com.platys.name: "starrocks"
      com.platys.description: "Real-time analytics database"
    user: root
    depends_on:
      - starrocks-fe
    ports:
      - 8040:8040
    environment:
      JAVA_OPTS: "-Dplugins.dir=/opt/pinot/plugins -Xms4G -Xmx4G -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Xloggc:gc-pinot-broker.log"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - /bin/bash
      - -c
      - |
        sleep 15s;
        mysql --connect-timeout 2 -h starrocks-fe -P9030 -uroot -e "ALTER SYSTEM ADD COMPUTE NODE \"starrocks-cn:9050\";"
        /opt/starrocks/cn/bin/start_cn.sh
    restart: {{container_restart_policy}}
  {% endif %}  {#  STARROCKS_edition == 'standalone' #}
{% endif %}   {# STARROCKS_enable #}

{% if CLICKHOUSE_enable | default(false) %}
  #  ================================== ClickHouse ========================================== #
  clickhouse:
    image: clickhouse/clickhouse-server:{{__CLICKHOUSE_version}}
    hostname: clickhouse
    container_name: clickhouse
    labels:
      com.platys.name: "clickhouse"
      com.platys.description: "Real-time analytics database"
      com.platys.password.envvars: "PLATYS_CLICKHOUSE_PASSWORD"      
    ports:
      - "28243:8123"       
      - "28244:9000" 
    environment:
      - CLICKHOUSE_DB={{CLICKHOUSE_db_name}}
      - CLICKHOUSE_USER={{CLICKHOUSE_username}}
      - CLICKHOUSE_PASSWORD=${PLATYS_CLICKHOUSE_PASSWORD:-{{CLICKHOUSE_password}}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/clickhouse:/docker-entrypoint-initdb.d      
    {%if CLICKHOUSE_volume_map_data | default(false) %}  
      - ./container-volume/clickhouse/data:/var/lib/clickhouse
    {% endif -%}   {#  CLICKHOUSE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}  {#  CLICKHOUSE_enable #}

{% if CLICKHOUSE_UI_enable | default(false) %}
  #  ================================== ClickHouse UI ========================================== #
  clickhouse-ui:
    image: ghcr.io/caioricciuti/ch-ui:{{__CLICKHOUSE_UI_version}}
    hostname: clickhouse-ui
    container_name: clickhouse-ui
    labels:
      com.platys.name: "clickhouse-ui"
      com.platys.description: "Real-time analytics database UI"
      com.platys.webui.title: "ClickHouse UI"
      com.platys.webui.url: "http://dataplatform:5521"
    ports:
      - "5521:5521"       
    environment:
      - VITE_CLICKHOUSE_URL=http://${PUBLIC_IP}:28243
      - VITE_CLICKHOUSE_USER={{CLICKHOUSE_username}}
      - VITE_CLICKHOUSE_PASS=${PLATYS_CLICKHOUSE_PASSWORD:-{{CLICKHOUSE_password}}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}  {#  CLICKHOUSE_UI_enable #}

{% if IGNITE_enable | default(false) %}
  #  ================================== Apache Ignite ========================================== #
  {% for num in range(IGNITE_servers | default(1) ) %}
    {% set external_port = 28240 + loop.index - 1 %}
  ignite-{{loop.index}}:
    image: apacheignite/ignite:{{__IGNITE_version}}
    hostname: ignite-{{loop.index}}
    container_name: ignite-{{loop.index}}
    labels:
      com.platys.name: "ignite"
      com.platys.description: "In-Memory Data Grid"
      com.platys.restapi.title: "Ignite REST API"
      com.platys.restapi.url: "http://dataplatform:28240/ignite?cmd=version"
    ports:
      - "{{external_port}}:8080"
      - "10800:10800"
      - "11212:11211"
    environment:
      #CONFIG_URI: file:/config/ignite-config.xml
      IGNITE_QUIET: "false"
      OPTION_LIBS: {{IGNITE_option_libs}}
      IGNITE_WORK_DIR: '/storage'
      JVM_OPTS: "-server -Xms1g  -Xmx1g -XX:NewSize=512m -XX:SurvivorRatio=6 -XX:+AlwaysPreTouch
              -XX:+UseG1GC -XX:MaxGCPauseMillis=2000 -XX:GCTimeRatio=4 -XX:InitiatingHeapOccupancyPercent=30
              -XX:G1HeapRegionSize=8M -XX:ConcGCThreads=2 -XX:G1HeapWastePercent=10 -XX:+UseTLAB
              -XX:+ScavengeBeforeFullGC -XX:+DisableExplicitGC"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  PINOT_volume_map_data #}
    volumes:
      - ./data-transfer:/data-transfer
      #- ./config:/config:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  gridgain-cc-backend:
    image: gridgain/control-center-backend:{{__GRIDGAIN_CC_version}}
    hostname: gridgain-cc-backend
    container_name: gridgain-cc-backend
    labels:
      com.platys.name: "ignite"
      com.platys.description: "In-Memory Data Grid"
    volumes:
      - ./data-transfer:/data-transfer
      - ./gridgain-console-work:/opt/gridgain-web-console-server/work
      #- ./config:/config:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  gridgain-cc-frontend:
    image: gridgain/control-center-frontend:{{__GRIDGAIN_CC_version}}
    hostname: gridgain-cc-frontend
    container_name: gridgain-cc-frontend
    labels:
      com.platys.name: "ignite"
      com.platys.description: "In-Memory Data Grid"
      com.platys.webui.title: "Gridgain Control Center UI"
      com.platys.webui.url: "http://dataplatform:28245"
    ports:
      # Port mapping format (change if needed) - HOST:CONTAINER
      - 28245:8008
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/gridgain/control-center.conf:/etc/nginx/control-center.conf
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {# IGNITE_enable #}

{% if PROMETHEUS_enable | default(false) %}
  #  ================================== Prometheus ========================================== #
  prometheus-1:
    image: prom/prometheus:{{__PROMETHEUS_version}}
    container_name: prometheus-1
    hostname: prometheus-1
    labels:
      com.platys.name: "prometheus"
      com.platys.description: "Monitoring system and time series database"
      com.platys.webui.title: "Prometheus UI"
      com.platys.webui.url: "http://dataplatform:9090/graph"
      com.platys.restapi.title: "Prometheus Rest API"
      com.platys.restapi.url: "http://dataplatform:9090/api/v1"
    ports:
      - "9090:9090"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if not PROMETHEUS_volume_map_custom_config %}
      - ./conf/prometheus/prometheus-config/prometheus.yml:/etc/prometheus/prometheus.yml
    {% else %}
      - ./custom-conf/prometheus/prometheus-config/prometheus.yml:/etc/prometheus/prometheus.yml
    {% endif -%}
    {% if PROMETHEUS_volume_map_data %}
      - "./container-volume/prometheus:/prometheus"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.listen-address=0.0.0.0:9090"
      - "--storage.tsdb.retention.time=30d"
      - "--log.level=info"
    restart: {{container_restart_policy}}

  {% if PROMETHEUS_PUSHGATEWAY_enable | default(false) %}
  prometheus-pushgateway:
    image: prom/pushgateway:{{__PROMETHEUS_PUSHGATEWAY_version}}
    container_name: prometheus-pushgateway
    hostname: prometheus-pushgateway
    labels:
      com.platys.name: "prometheus-pushgateway"
      com.platys.description: "Monitoring system and time series database"
      com.platys.restapi.title: "Pushgateway API"
      com.platys.restapi.url: "http://dataplatform:9091"
    expose:
      - 9091
    ports:
      - "9091:9091"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  PROMETHEUS_PUSHGATEWAY_enable #}

  {% if PROMETHEUS_NODEEXPORTER_enable | default(false) %}
  prometheus-nodeexporter:
    image: prom/node-exporter:{{__PROMETHEUS_NODEEXPORTER_version}}
    container_name: prometheus-nodeexporter
    labels:
      com.platys.name: "prometheus-nodeexporter"
      com.platys.description: "Monitoring system and time series database"
    expose:
      - 9100
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/prometheus/proc:/host/proc:ro
      - ./container-volume/prometheus/sys:/host/sys:ro
      - ./:/rootfs:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      - "--path.procfs=/host/proc"
      - '--path.rootfs=/rootfs'
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.ignored-mount-points"
      - "^(aufs|proc|nsfs|shm|cgroup|tmpfs|binfmt_misc|debugfs|devpts|fusectl|hugetlbfs|fuse.lxcfs|mqueue|pstore|securityfs|sysfs|autofs|devtmpfs|configfs)"
  {% endif %}   {#  PROMETHEUS_NODEEXPORTER_enable #}

  {% if PROMETHEUS_ALERTMANAGER_enable | default(false) %}
  prometheus-alertmanager:
    image: prom/alertmanager:{{__PROMETHEUS_ALERTMANAGER_version}}
    container_name: prometheus-alertmanager
    labels:
      com.platys.name: "prometheus-alertmanager"
      com.platys.description: "Monitoring system and time series database"
      com.platys.webui.title: "Prometheus Alertmanager UI"
      com.platys.webui.url: "http://dataplatform:28328"
    ports:
      - "28328:9093"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/prometheus-alertmanager/alertmanager.yml:/alertmanager/alertmanager.yml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
  {% endif %}   {#  PROMETHEUS_ALERTMANAGER_enable #}

{% endif %}   {#  PROMETHEUS_enable #}

{% if TILE38_enable | default(false) %}
  #  ================================== Tile38 ========================================== #
  tile38:
    image: tile38/tile38:{{__TILE38_version}}
    container_name: tile38
    hostname: tile38
    labels:
      com.platys.name: "tile38"
      com.platys.description: "Real-time Geospatial and Geofencing"
      com.platys.webui.title: "Tile38 UI"
      com.platys.webui.url: "http://dataplatform:9851"
    ports:
      - "9851:9851"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  TILE38_enable #}

{% if YUGABYTE_enable | default(false) %}
  #  ================================== Yugabyte ========================================== #
  yb-master:
    image: yugabytedb/yugabyte:{{__YUGABYTE_version}}
    container_name: yb-master
    hostname: yb-master
    labels:
      com.platys.name: "yugabyte"
      com.platys.description: "Cloud native distributed SQL database"
    command: [ "/home/yugabyte/bin/yb-master",
                "--fs_data_dirs=/mnt/master",
                "--master_addresses=yb-master:7100",
                "--rpc_bind_addresses=yb-master:7100",
                "--replication_factor=1"]
    ports:
      - "7000:7000"
    environment:
      SERVICE_7000_NAME: yb-master
#    volumes:
#      - yb-master-data-1:/mnt/master
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  yb-tserver-1:
    image: yugabytedb/yugabyte:{{__YUGABYTE_version}}
    container_name: yb-tserver-1
    hostname: yb-tserver-1
    labels:
      com.platys.name: "yugabyte"
      com.platys.description: "Cloud native distributed SQL database"
    depends_on:
      - yb-master
    ports:
      - "19042:9042"
      - "16379:6379"
      - "15433:5433"
      - "19000:9000"
    environment:
      SERVICE_5433_NAME: ysql
      SERVICE_9042_NAME: ycql
      SERVICE_6379_NAME: yedis
      SERVICE_9000_NAME: yb-tserver-1
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
#    volumes:
#      - yb-tserver-data-1:/mnt/tserver
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: [ "/home/yugabyte/bin/yb-tserver",
                "--fs_data_dirs=/mnt/tserver",
                "--start_pgsql_proxy",
                "--rpc_bind_addresses=yb-tserver-1:9100",
                "--tserver_master_addrs=yb-master:7100",
                "--replication_factor=1"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  YUGABYTE_enable #}

{% if SINGLE_STORE_enable | default(false) %}
  #  ================================== SingleStore ========================================== #
  single-store:
    image: ghcr.io/singlestore-labs/singlestoredb-dev:{{__SINGLE_STORE_version}}
    container_name: single-store
    hostname: single-store
    labels:
      com.platys.name: "single-store"
      com.platys.description: "Distributed SQL database"
      com.platys.webui.title: "SingleStore Studio UI"
      com.platys.webui.url: "http://dataplatform:28351"
      com.platys.restapi.title: "SingleStore Data API"
      com.platys.restapi.url: "http://dataplatform:28352"
    ports:
      - 3308:3306
      - 28351:8080
      - 28352:9000
    environment:
      - SINGLESTORE_LICENSE={{SINGLE_STORE_license}}
      - ROOT_PASSWORD=abc123!
      - INIT_SQL=/tmp/init/init.sql
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/single-store/:/tmp/init
      #- my_cool_volume:/data
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SINGLE_STORE_enable #}

{% if ORACLE_EE_enable | default(false) %}
  #  ================================== Oracle EE ========================================== #
  {% for num in range(ORACLE_EE_nodes | default(1) ) %}
    {% set external_1521_port = 1530 + loop.index - 1 %}
    {% set external_8080_port = 28410 + loop.index - 1 %}
    {% set external_5500_port = 5500 + loop.index - 1 %}
  oracledb-ee-{{loop.index}}:
    image: {{private_docker_repository_name}}/ora_db:{{__ORACLE_EE_version}}
    container_name: oracledb-ee-{{loop.index}}
    hostname: oracledb-ee-{{loop.index}}
    labels:
      com.platys.name: "oracledb-ee"
      com.platys.description: "Relational Database"
      com.platys.webui.title: "Oracle UI"
      com.platys.webui.url: "http://dataplatform:{{external_8080_port}}/ords"
    ports:
      - "{{external_1521_port}}:1521"
      - "{{external_8080_port}}:8080"
      - "{{external_5500_port}}:5500"
    environment:
      CONTAINER: '{{ORACLE_EE_container_enable}}'
      ORACLE_SID: ORCLCDB
      ORACLE_PDB: ORCLPDB1
      ORACLE_PWD: {{ORACLE_EE_password}}
      ORACLE_CHARACTERSET: {{ORACLE_EE_characterset}}
      INSTANCE_INIT: '/u01/config'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ORACLE_EE_volume_map_data %}
      - ./container-volume/oracle-ee-{{loop.index}}/data:/u01
    {% endif %}
      - ./init/oracle-ee-{{loop.index}}/startup:/u01/config/startup
      - ./init/oracle-ee-{{loop.index}}/setup:/u01/config/setup
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}    
{% endif %}   {#  ORACLE_EE_enable #}

{% if ORACLE_XE_enable | default(false) %}
  #  ================================== Oracle XE ========================================== #
  oracledb-xe:
    image: gvenzl/oracle-xe:{{__ORACLE_XE_version}}{{"-" ~ ORACLE_XE_edition if ORACLE_XE_edition is defined and ORACLE_XE_edition != 'regular'}}{{'-faststart' if ORACLE_XE_use_faststart}}
    container_name: oracledb-xe
    hostname: oracledb-xe
    labels:
      com.platys.name: "oracledb-xe"
      com.platys.description: "Relational Database"
    ports:
      - "1522:1521"
    environment:
    {% if ORACLE_XE_database is defined and ORACLE_XE_database | length %}
      ORACLE_DATABASE: {{ORACLE_XE_database}}
    {% endif -%}   {#  ORACLE_XE_database #}
      ORACLE_PASSWORD: {{ORACLE_XE_password}}
    {% if ORACLE_XE_random_password is defined and ORACLE_XE_random_password | length %}
      ORACLE_RANDOM_PASSWORD: '{{ORACLE_XE_random_password}}'
    {% endif -%}   {#  ORACLE_XE_random_password #}
    {% if ORACLE_XE_app_user is defined and ORACLE_XE_app_user | length %}
      APP_USER: {{ORACLE_XE_app_user}}
      APP_USER_PASSWORD: {{ORACLE_XE_app_user_password}}
    {% endif -%}   {#  ORACLE_XE_app_user #}
      ORACLE_CHARACTERSET: AL32UTF8
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/oracle-xe:/container-entrypoint-initdb.d/
    {% if ORACLE_XE_volume_map_data %}
      - ./container-volume/oracle-xe/data:/opt/oracle/oradata
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ORACLE_XE_enable #}

{% if ORACLE_FREE_enable | default(false) %}
  #  ================================== Oracle Database Free (official) ========================================== #
  oracledb-free:
    image: container-registry.oracle.com/database/free:{{__ORACLE_FREE_version}}{{"-" ~ ORACLE_FREE_edition if ORACLE_FREE_edition is defined and ORACLE_FREE_edition != 'regular'}}
    container_name: oracledb-free
    hostname: oracledb-free
    labels:
      com.platys.name: "oracledb-free"
      com.platys.description: "Oracle Relational Database"
      com.platys.password.envvars: "PLATYS_ORACLE_FREE_PASSWORD"
    ports:
      - "1524:1521"
    environment:
      ORACLE_PWD: ${PLATYS_ORACLE_FREE_PASSWORD:-{{ORACLE_FREE_password}}}
      ORACLE_CHARACTERSET: {{ORACLE_FREE_characterset if ORACLE_FREE_characterset is defined and ORACLE_FREE_characterset and ORACLE_FREE_characterset | length else omit}}
      ENABLE_ARCHIVELOG: {{ORACLE_FREE_enable_archivelog}}
      ENABLE_FORCE_LOGGING: {{ORACLE_FREE_enable_force_logging}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/oracle-free/startup:/opt/oracle/scripts/startup
      - ./init/oracle-free/setup:/opt/oracle/scripts/setup
    {% if ORACLE_FREE_volume_map_custom_conf %}
      - ./custom-conf/oracle-free/data:/opt/oracle/scripts/custom
    {% endif %}
    {% if ORACLE_FREE_volume_map_data %}
      - ./container-volume/oracle-free:/opt/oracle/oradata
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ORACLE_FREE_enable #}

{% if ORACLE_OCI_FREE_enable | default(false) %}
  #  ================================== Oracle Database Free (OCI) ========================================== #
  oracledb-oci-free:
    image: container-registry.oracle.com/database/free:{{__ORACLE_OCI_FREE_version}}{{"-" ~ ORACLE_OCI_FREE_edition if ORACLE_OCI_FREE_edition is defined and ORACLE_OCI_FREE_edition != 'regular'}}{{'-faststart' if ORACLE_OCI_FREE_use_faststart}}
    container_name: oracledb-oci-free
    hostname: oracledb-oci-free
    labels:
      com.platys.name: "oracledb-oci-free"
      com.platys.description: "Oracle Relational Database"
      com.platys.password.envvars: "PLATYS_ORACLE_OCI_FREE_PASSWORD"
    ports:
      - "1523:1521"
    environment:
    {% if ORACLE_OCI_FREE_database is defined and ORACLE_OCI_FREE_database and ORACLE_OCI_FREE_database | length %}
      ORACLE_DATABASE: {{ORACLE_OCI_FREE_database}}
    {% endif -%}   {#  ORACLE_OCI_FREE_database #}    
    {% if ORACLE_OCI_FREE_random_password | default(false) %}
      ORACLE_RANDOM_PASSWORD: '{{ORACLE_OCI_FREE_random_password}}'
    {% else -%}   {#  ORACLE_OCI_FREE_random_password #}
      ORACLE_PASSWORD: '${PLATYS_ORACLE_OCI_FREE_PASSWORD:-{{ORACLE_OCI_FREE_password}}}'
    {% endif -%}   {#  ORACLE_OCI_FREE_random_password #}
    {% if ORACLE_OCI_FREE_app_user is defined and ORACLE_OCI_FREE_app_user | length %}
      APP_USER: {{ORACLE_OCI_FREE_app_user}}
      APP_USER_PASSWORD: {{ORACLE_OCI_FREE_app_user_password}}
    {% endif -%}   {#  ORACLE_OCI_FREE_app_user #}
      ORACLE_CHARACTERSET:  {{ORACLE_OCI_FREE_characterset if ORACLE_OCI_FREE_characterset is defined and ORACLE_OCI_FREE_characterset and ORACLE_OCI_FREE_characterset | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/oracle-oci-free:/container-entrypoint-initdb.d/
    {% if ORACLE_OCI_FREE_volume_map_data %}
      - ./container-volume/oracle-oci-free/data:/opt/oracle/oradata
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ORACLE_OCI_FREE_enable #}

{% if ORACLE_ADB_FREE_enable | default(false) %}
  #  ================================== Oracle Autonomous Database Free ========================================== #
  oracle-adb:
    image: ghcr.io/oracle/adb-free:{{__ORACLE_ADB_FREE_version}}
    container_name: oracle-adb
    hostname: oracle-adb
    labels:
      com.platys.name: "oracle-adb"
      com.platys.description: "Oracle Autonomous Database"
      com.platys.webui.title: "Oracle Database Actions"
      com.platys.webui.url: "https://dataplatform:8447/ords/sql-developer"
      com.platys.restapi.title: "ORDS Rest API"
      com.platys.restapi.url: "https://dataplatform:8447/ords"
      com.platys.password.envvars: "PLATYS_ORACLE_ADB_FREE_PASSWORD"
    ports:
      - "1525:1521"
      - "1526:1522"
      - "8447:8443"
      - "27022:27017"      
    environment:
      WORKLOAD_TYPE: {{ORACLE_ADB_FREE_workload_type | upper}}
      DATABASE_NAME: {{ORACLE_ADB_FREE_database if ORACLE_ADB_FREE_database is defined and ORACLE_ADB_FREE_database and ORACLE_ADB_FREE_database | length else omit}}
      ADMIN_PASSWORD: ${PLATYS_ORACLE_ADB_FREE_ADMIN_PASSWORD:-{{ORACLE_ADB_FREE_admin_password}}}
      WALLET_PASSWORD: ${PLATYS_ORACLE_ADB_FREE_WALLET_PASSWORD:-{{ORACLE_ADB_FREE_wallet_password}}}
      ENABLE_ARCHIVE_LOG: {{ORACLE_ADB_FREE_enable_archivelog}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ORACLE_ADB_FREE_volume_map_data %}
      - ./container-volume/oracle-adb-free/data:/u01/data
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ORACLE_ADB_FREE_enable #}

{% if ORACLE_SQLCL_enable | default(false) %}
  #  ================================== Oracle SQLcl ========================================== #
  oracle-sqlcl:
    image: trivadis/oracle-sqlcl:{{__ORACLE_SQLCL_version}}
    container_name: oracle-sqlcl
    hostname: oracle-sqlcl
    labels:
      com.platys.name: "oracle-sqlcl"
      com.platys.description: "Oracle CLI"
    environment:
      ORACLE_HOST: oracledb-xe
      ORACLE_PORT: 1521
      ORACLE_SERVICE: XEPDB1
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: tail -f /dev/null
    restart: {{container_restart_policy}}
{% endif %}   {#  ORACLE_SQLCL_enable #}

{% if ORACLE_REST_DATA_SERVICE_enable | default(false) %}
  #  ================================== Oracle REST Data Service (ORDS) ========================================== #
  oracle-rest-1:
    image: trivadis/restdataservices:{{__ORACLE_REST_DATA_SERVICE_version}}
    container_name: oracle-rest-1
    hostname: oracle-rest-1
    labels:
      com.platys.name: "oracle-rest"
      com.platys.description: "REST API on Oracle RDBMS"
      com.platys.restapi.title: "ORDS Rest API"
      com.platys.restapi.url: "http://dataplatform:28252/ords"
    ports:
      - "28252:8888"
    environment:
      ORACLE_HOST: oracledb-xe
      ORACLE_PORT: 1521
      ORACLE_SERVICE: XEPDB1
      ORACLE_PWD: 'EAo4KsTfRR'
      ORDS_PWD: abc123!
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ORACLE_REST_DATA_SERVICE_enable #}

{% if MYSQL_enable | default(false) %}
  #  ================================== MySQL ========================================== #
  mysql:
    image: mysql:{{__MYSQL_version}}
    container_name: mysql
    labels:
      com.platys.name: "mysql"
      com.platys.description: "Relational Database"
    ports:
      - 3306:3306
    environment:
      - MYSQL_DATABASE={{MYSQL_database}}
      - MYSQL_USER={{MYSQL_user}}
      - MYSQL_PASSWORD={{MYSQL_password}}
      - MYSQL_ROOT_PASSWORD=manager
      - MYSQL_LOG_CONSOLE=true
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "mysqladmin" ,"ping", "-ur{{MYSQL_user}}", "-p{{MYSQL_password}}"]
      interval: 10s
      timeout: 10s
      retries: 3    
{% endif %}   {#  MYSQL_enable #}

{% if MARIADB_enable | default(false) %}
  #  ================================== MariaDB ========================================== #
  mariadb:
    image: mariadb:{{__MARIADB_version}}
    container_name: mariadb
    labels:
      com.platys.name: "mariadb"
      com.platys.description: "Relational database"
    ports:
      - 3309:3306
    environment:
      - MYSQL_DATABASE={{MARIADB_database}}
      - MYSQL_USER={{MARIADB_user}}
      - MYSQL_PASSWORD={{MARIADB_password}}
      - MYSQL_ROOT_PASSWORD=manager
      - MYSQL_LOG_CONSOLE=true
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/mariadb:/docker-entrypoint-initdb.d/
    {% if MARIADB_volume_map_data %}
      - ./container-volume/mariadb/data:/var/lib/mysql:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MARIADB_enable #}

{% if SQLSERVER_enable | default(false) %}
  #  ================================== SQL Server ========================================== #
  sqlserver:
  {% if SQLSERVER_provision_adventure_works | default(false) %}
    image: chriseaton/adventureworks:{{SQLSERVER_provision_adventure_works_edition}}
  {% else %}
    image: mcr.microsoft.com/mssql/server:{{__SQLSERVER_version}}
  {% endif -%}   {#  SQLSERVER_provision_adventure_works #}
    hostname: sqlserver
    container_name: sqlserver
    labels:
      com.platys.name: "sqlserver"
      com.platys.description: "Relational Database"
    ports:
      - "1433:1433"
    environment:
      ACCEPT_EULA: "Y"
      SA_PASSWORD: "abc123abc123!"
      MSSQL_PID: "Express"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SQLSERVER_enable #}

{% if POSTGRESQL_enable | default(false) %}
  #  ================================== PostgreSQL ========================================== #
  postgresql:
  {% if POSTGRESQL_alternative_docker_image is defined and POSTGRESQL_alternative_docker_image and POSTGRESQL_alternative_docker_image | length | default(false) %}
    image: {{POSTGRESQL_alternative_docker_image}}
  {% else -%}
    {% if (POSTGRESQL_extension | lower) == '' | default(false) %}
    image: postgres:{{__POSTGRESQL_version}}
    {% elif (POSTGRESQL_extension | lower) == 'postgis' | default(false) %}
    image: postgis/postgis:{{__POSTGRESQL_POSTGIS_version}}
    platform: linux/amd64    
    {% elif (POSTGRESQL_extension | lower) == 'age' | default(false) %}
    image: apache/age:{{__POSTGRESQL_AGE_version}}
    {% elif (POSTGRESQL_extension | lower) == 'pgvector' | default(false) %}
    image: pgvector/pgvector:{{__POSTGRESQL_PGVECTOR_version}}
    {% elif (POSTGRESQL_extension | lower) == 'pgvecto-rs' | default(false) %}
    image: tensorchord/pgvecto-rs:{{__POSTGRESQL_PGVECTO_RS_version}}
    {% elif (POSTGRESQL_extension | lower) == 'postgis_we' | default(false) %}
    image: ivanlonel/postgis-with-extensions:{{__POSTGRESQL_POSTGIS_WE_version}}
    platform: linux/amd64    
    {% elif (POSTGRESQL_extension | lower) == 'pgduckdb' | default(false) %}
    image: pgduckdb/pgduckdb:{{__POSTGRESQL_PGDUCKDB_version}}
    {% endif -%}   {#  POSTGRESQL_extension #}
  {% endif -%}   {#  POSTGRESQL_alternative_docker_image is defined #}
    container_name: postgresql
    hostname: postgresql
    labels:
      com.platys.name: "postgresql"
      com.platys.description: "Open-Source object-relational database system"
      com.platys.password.envvars: "PLATYS_POSTGRESQL_PASSWORD,PLATYS_POSTGRESQL_MULTIPLE_PASSWORD"
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_PASSWORD=${PLATYS_POSTGRESQL_PASSWORD:-{{POSTGRESQL_password | default('abc123!') }}}
      - POSTGRES_USER={{POSTGRESQL_user | default('demo') }}
      - POSTGRES_DB={{POSTGRESQL_database | default('demodb') }}
    {% if POSTGRESQL_multiple_databases is defined and POSTGRESQL_multiple_databases | length %}
      - POSTGRES_MULTIPLE_DATABASES={{POSTGRESQL_multiple_databases}}
      - POSTGRES_MULTIPLE_USERS={{POSTGRESQL_multiple_users}}
      - POSTGRES_MULTIPLE_PASSWORDS=${PLATYS_POSTGRESQL_MULTIPLE_PASSWORD:-{{POSTGRESQL_multiple_passwords}}}
      - POSTGRES_MULTIPLE_ADDL_ROLES={{POSTGRESQL_multiple_addl_roles}}
    {% endif -%}   {#  POSTGRESQL_multiple_databases #}
      - PGDATA=/var/lib/postgresql/data/pgdata
      - DB_SCHEMA={{POSTGRESQL_schema | default('demo') }}
    {% if POSTGRESQL_anon_role is defined and POSTGRESQL_anon_role | length %}
      - DB_ANON_ROLE={{POSTGRESQL_anon_role | default('demo') }}
    {% endif -%}   {#  POSTGRESQL_anon_role #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    {% if POSTGRESQL_volume_map_data %}
      - ./container-volume/postgresql/data:/var/lib/postgresql/data/pgdata:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if POSTGRESQL_wal_level is defined and POSTGRESQL_wal_level and POSTGRESQL_wal_level | length | default(false) %}
    command:
      - "postgres"
      - "-c"
      - "wal_level={{POSTGRESQL_wal_level | lower }}"
    {% endif -%}   {#  POSTGRESQL_wal_level #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U {{POSTGRESQL_user | default('demo') }}"]
      interval: 10s
      timeout: 5s
      retries: 5

  {% if POSTGREST_enable| default(false) %}
  #  ================================== PostgREST ========================================== #
  postgrest:
    image: postgrest/postgrest:{{__POSTGREST_version}}
    container_name: postgrest
    hostname: postgrest
    labels:
      com.platys.name: "postgrest"
      com.platys.description: "REST API on PostgreSQL"
      com.platys.restapi.title: "PostgREST API"
      com.platys.restapi.url: "http://dataplatform:28206"
    ports:
      - "28206:3000"
    environment:
      # The standard connection URI format, documented at
      # https://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-CONNSTRING
      - PGRST_DB_URI=postgres://{{POSTGRESQL_user}}:{{POSTGRESQL_password}}@postgresql:5432/{{POSTGRESQL_database}}
      # The name of which database schema to expose to REST clients
      - PGRST_DB_SCHEMA={{POSTGRESQL_schema}}
      # The database role to use when no client authentication is provided
    {% if POSTGRESQL_anon_role is defined and POSTGRESQL_anon_role | length %}
      - PGRST_DB_ANON_ROLE={{POSTGRESQL_anon_role}}
    {% endif -%}   {#  POSTGRESQL_anon_role #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    {% if POSTGRESQL_volume_map_data %}
      - ./container-volume/postgresql/data:/var/lib/postgresql/data/pgdata:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  POSTGREST_enable #}
{% endif %}   {#  POSTGRESQL_enable #}

{% if POSTGRESQL_enable and PGADMIN_enable| default(false) %}
  #  ================================== pgAdmin ========================================== #
  pgadmin:
    image: dpage/pgadmin4:{{__PGADMIN_version}}
    container_name: pgadmin
    hostname: pgadmin
    depends_on:
      - postgresql
    labels:
      com.platys.name: "pgadmin"
      com.platys.description: "PostgreSQL Admin UI"
      com.platys.webui.title: "pgAdmin UI"
      com.platys.webui.url: "http://dataplatform:28247"
    ports:
      - "28247:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# PGADMIN_enable #}

{% if SUPABASE_enable | default(false) %}
  #  ================================== Supabase ========================================== #
  supabase-studio:
    image: supabase/studio:{{__SUPABASE_STUDIO_version}}
    container_name: supabase-studio
    hostname: supabase-studio
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Studio"
      com.platys.webui.title: "Supabase Studio UI"
      com.platys.webui.url: "http://dataplatform:28260"
    ports:
      - 28260:8082      
    environment:
      STUDIO_PG_META_URL: http://supabase-meta:8080
      POSTGRES_PASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}

      DEFAULT_ORGANIZATION_NAME: Default Organization
      DEFAULT_PROJECT_NAME: Default Project
    {% if ( external['OPENAI_enable']) | default(false) %}
      OPENAI_API_KEY: external['OPENAI_api_key'])
    {% endif -%}   {#  OPENAI_enable #}      

      SUPABASE_URL: http://supabase-kong:8000
      SUPABASE_PUBLIC_URL: http://${PUBLIC_IP}:8000
      SUPABASE_ANON_KEY: {{SUPABASE_anon_key}}
      SUPABASE_SERVICE_KEY: {{SUPABASE_service_role_key}}
      AUTH_JWT_SECRET: {{SUPABASE_jwt_secret}}

      LOGFLARE_API_KEY: {{SUPABASE_logflare_api_key}}
      LOGFLARE_URL: http://supabase-analytics:4000
      NEXT_PUBLIC_ENABLE_LOGS: true
      # Comment to use Big Query backend for analytics
      NEXT_ANALYTICS_BACKEND_PROVIDER: postgres
      # Uncomment to use Big Query backend for analytics
      # NEXT_ANALYTICS_BACKEND_PROVIDER: bigquery
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "fetch('http://supabase-studio:3000/api/platform/profile').then((r) => {if (r.status !== 200) throw new Error(r.status)})"
        ]
      timeout: 10s
      interval: 5s
      retries: 3

  supabase-kong:
    image: kong:2.8.1
    container_name: supabase-kong
    hostname: supabase-kong
    labels:
      com.platys.name: "kong"
      com.platys.description: "Supabase Kong"
      com.platys.webui.title: "Supabase UI"
      com.platys.webui.url: "http://dataplatform:28258"
      com.platys.password.envvars: "PLATYS_SUPABASE_DASHBOARD_PASSWORD"      
    ports:
      - 28258:8000/tcp
      - 28259:8443/tcp      
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      # https://github.com/supabase/cli/issues/14
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
      SUPABASE_ANON_KEY: {{SUPABASE_anon_key}}
      SUPABASE_SERVICE_KEY: {{SUPABASE_service_role_key}}
      DASHBOARD_USERNAME: {{SUPABASE_dashboard_username}}
      DASHBOARD_PASSWORD: ${PLATYS_SUPABASE_DASHBOARD_PASSWORD:-{{SUPABASE_dashboard_password}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/supabase/kong/kong.yml:/home/kong/temp.yml:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint: bash -c 'eval "echo \"$$(cat ~/temp.yml)\"" > ~/kong.yml && /docker-entrypoint.sh kong docker-start'
    restart: {{container_restart_policy}}

  supabase-auth:
    image: supabase/gotrue:{{__SUPABASE_AUTH_version}}
    container_name: supabase-auth
    hostname: supabase-auth
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Authentication"
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      API_EXTERNAL_URL: http://${PUBLIC_IP}:8000

      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DB_DATABASE_URL: postgres://supabase_auth_admin:${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}@supabase-db:5432/postgres

      GOTRUE_SITE_URL: http://${PUBLIC_IP}:3000
      GOTRUE_URI_ALLOW_LIST: 
      GOTRUE_DISABLE_SIGNUP: false

      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      GOTRUE_JWT_EXP: 3600
      GOTRUE_JWT_SECRET: {{SUPABASE_jwt_secret}}

      GOTRUE_EXTERNAL_EMAIL_ENABLED: {{SUPABASE_email_enabled}}
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: false
      GOTRUE_MAILER_AUTOCONFIRM: false

      # Uncomment to bypass nonce check in ID Token flow. Commonly set to true when using Google Sign In on mobile.
      # GOTRUE_EXTERNAL_SKIP_NONCE_CHECK: true

      # GOTRUE_MAILER_SECURE_EMAIL_CHANGE_ENABLED: true
      # GOTRUE_SMTP_MAX_FREQUENCY: 1s
      GOTRUE_SMTP_ADMIN_EMAIL: admin@example.com
      GOTRUE_SMTP_HOST: {{SUPABASE_email_host if SUPABASE_email_host is defined and SUPABASE_email_host and SUPABASE_email_host | length else omit}}
      GOTRUE_SMTP_PORT: {{SUPABASE_email_port if SUPABASE_email_port is defined and SUPABASE_email_port else omit}}
      GOTRUE_SMTP_USER: {{SUPABASE_email_host_user if SUPABASE_email_host_user is defined and SUPABASE_email_host_user and SUPABASE_email_host_user | length else omit}}
      GOTRUE_SMTP_PASS: {{SUPABASE_email_host_password if SUPABASE_email_host_password is defined and SUPABASE_email_host_password and SUPABASE_email_host_password | length else omit}}
      GOTRUE_SMTP_SENDER_NAME: {{SUPABASE_email_from if SUPABASE_email_from is defined and SUPABASE_email_from and SUPABASE_email_from | length else omit}}
      GOTRUE_MAILER_URLPATHS_INVITE: "/auth/v1/verify"
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: "/auth/v1/verify"
      GOTRUE_MAILER_URLPATHS_RECOVERY: "/auth/v1/verify"
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: "/auth/v1/verify"

      GOTRUE_EXTERNAL_PHONE_ENABLED: true
      GOTRUE_SMS_AUTOCONFIRM: true
      # Uncomment to enable custom access token hook. Please see: https://supabase.com/docs/guides/auth/auth-hooks for full list of hooks and additional details about custom_access_token_hook

      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_ENABLED: "true"
      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_URI: "pg-functions://postgres/public/custom_access_token_hook"
      # GOTRUE_HOOK_CUSTOM_ACCESS_TOKEN_SECRETS: "<standard-base64-secret>"

      # GOTRUE_HOOK_MFA_VERIFICATION_ATTEMPT_ENABLED: "true"
      # GOTRUE_HOOK_MFA_VERIFICATION_ATTEMPT_URI: "pg-functions://postgres/public/mfa_verification_attempt"

      # GOTRUE_HOOK_PASSWORD_VERIFICATION_ATTEMPT_ENABLED: "true"
      # GOTRUE_HOOK_PASSWORD_VERIFICATION_ATTEMPT_URI: "pg-functions://postgres/public/password_verification_attempt"

      # GOTRUE_HOOK_SEND_SMS_ENABLED: "false"
      # GOTRUE_HOOK_SEND_SMS_URI: "pg-functions://postgres/public/custom_access_token_hook"
      # GOTRUE_HOOK_SEND_SMS_SECRETS: "v1,whsec_VGhpcyBpcyBhbiBleGFtcGxlIG9mIGEgc2hvcnRlciBCYXNlNjQgc3RyaW5n"

      # GOTRUE_HOOK_SEND_EMAIL_ENABLED: "false"
      # GOTRUE_HOOK_SEND_EMAIL_URI: "http://host.docker.internal:54321/functions/v1/email_sender"
      # GOTRUE_HOOK_SEND_EMAIL_SECRETS: "v1,whsec_VGhpcyBpcyBhbiBleGFtcGxlIG9mIGEgc2hvcnRlciBCYXNlNjQgc3RyaW5n"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9999/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3

  supabase-rest:
    image: postgrest/postgrest:{{__SUPABASE_REST_version}}
    container_name: supabase-rest
    hostname: supabase-rest
    labels:
      com.platys.name: "postgrest"
      com.platys.description: "Supabase Authentication"
    environment:
      PGRST_DB_URI: postgres://authenticator:${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}@supabase-db:5432/postgres
      PGRST_DB_SCHEMAS: public,storage,graphql_public
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: {{SUPABASE_jwt_secret}}
      PGRST_DB_USE_LEGACY_GUCS: "false"
      PGRST_APP_SETTINGS_JWT_SECRET: {{SUPABASE_jwt_secret}}
      PGRST_APP_SETTINGS_JWT_EXP: 3600
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      [
        "postgrest"
      ] 
    restart: {{container_restart_policy}}

  supabase-realtime:
    image: supabase/realtime:{{__SUPABASE_REALTIME_version}}
    container_name: supabase-realtime
    hostname: supabase-realtime
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Authentication"
    environment:
      PORT: 4000
      DB_HOST: supabase-db
      DB_PORT: 5432
      DB_USER: supabase_admin
      DB_PASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}
      DB_NAME: postgres
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO _realtime'
      DB_ENC_KEY: supabaserealtime
      API_JWT_SECRET: {{SUPABASE_jwt_secret}}
      SECRET_KEY_BASE: {{SUPABASE_secret_key_base}}
      ERL_AFLAGS: -proto_dist inet_tcp
      DNS_NODES: "''"
      RLIMIT_NOFILE: "10000"
      APP_NAME: realtime
      SEED_SELF_HOST: true
      RUN_JANITOR: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "-H",
          "Authorization: Bearer {{SUPABASE_anon_key}}",
          "http://localhost:4000/api/tenants/realtime-dev/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3    

  supabase-storage:
    image: supabase/storage-api:{{__SUPABASE_STORAGE_version}}
    container_name: supabase-storage
    hostname: supabase-storage
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Storage"
    environment:
      ANON_KEY: {{SUPABASE_anon_key}}
      SERVICE_KEY: {{SUPABASE_service_role_key}}
      POSTGREST_URL: http://supabase-rest:3000
      PGRST_JWT_SECRET: {{SUPABASE_jwt_secret}}
      DATABASE_URL: postgres://supabase_storage_admin:${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}@supabase-db:5432/postgres
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: file
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      TENANT_ID: stub
      # TODO: https://github.com/supabase/storage-api/issues/55
      REGION: stub
      GLOBAL_S3_BUCKET: stub
      ENABLE_IMAGE_TRANSFORMATION: "true"
      IMGPROXY_URL: http://imgproxy:5001
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if SUPABASE_volume_map_data | default(false) %}  
      - ./container-volume/supabase/storage:/var/lib/storage:z
    {% endif -%}   {#  SUPABASE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://storage:5000/status"
        ]
      timeout: 5s
      interval: 5s
      retries: 3

  supabase-imgproxy:
    image: darthsim/imgproxy:{{__SUPABASE_IMGPROXY_version}}
    container_name: supabase-imgproxy
    hostname: supabase-imgproxy
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Storage"
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
      IMGPROXY_USE_ETAG: "true"
      IMGPROXY_ENABLE_WEBP_DETECTION: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if SUPABASE_volume_map_data | default(false) %}  
      - ./container-volume/supabase/imgproxy:/var/lib/storage:z      
    {% endif -%}   {#  SUPABASE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "imgproxy",
          "health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3  

  supabase-meta:
    image: supabase/postgres-meta:{{__SUPABASE_META_version}}
    container_name: supabase-meta
    hostname: supabase-meta
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Meta"
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: supabase-db
      PG_META_DB_PORT: 5432
      PG_META_DB_NAME: postgres
      PG_META_DB_USER: supabase_admin
      PG_META_DB_PASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  supabase-functions:
    image: supabase/edge-runtime:{{__SUPABASE_EDGE_RUNTIME_version}}
    container_name: supabase-functions
    hostname: supabase-functions
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Functions"
    environment:
      JWT_SECRET: {{SUPABASE_jwt_secret}}
      SUPABASE_URL: http://supabase-kong:8000
      SUPABASE_ANON_KEY: {{SUPABASE_anon_key}}
      SUPABASE_SERVICE_ROLE_KEY: {{SUPABASE_service_role_key}}
      SUPABASE_DB_URL: postgresql://postgres:${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}@supabase-db:5432/postgres
      # TODO: Allow configuring VERIFY_JWT per function. This PR might help: https://github.com/supabase/cli/pull/786
      VERIFY_JWT: false
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/supabase/functions:/home/deno/functions:Z
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command:
      [
        "start",
        "--main-service",
        "/home/deno/functions/main"
      ]    
    restart: {{container_restart_policy}}

  supabase-analytics:
    image: supabase/logflare:{{__SUPABASE_ANALYTICS_version}}
    container_name: supabase-analytics
    hostname: supabase-analytics
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Analytics"
    ports:
      - 4001:4000
    environment:
      LOGFLARE_NODE_HOST: 127.0.0.1
      DB_USERNAME: supabase_admin
      DB_DATABASE: _supabase
      DB_HOSTNAME: supabase-db
      DB_PORT: 5432
      DB_PASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}
      DB_SCHEMA: _analytics
      LOGFLARE_API_KEY: {{SUPABASE_logflare_api_key}}
      LOGFLARE_SINGLE_TENANT: true
      LOGFLARE_SUPABASE_MODE: true
      LOGFLARE_MIN_CLUSTER_SIZE: 1

      # Comment variables to use Big Query backend for analytics
      POSTGRES_BACKEND_URL: postgresql://supabase_admin:${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}@supabase-db:5432/_supabase
      POSTGRES_BACKEND_SCHEMA: _analytics
      LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true
      # Uncomment to use Big Query backend for analytics
      # GOOGLE_PROJECT_ID: 
      # GOOGLE_PROJECT_NUMBER:
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging 
    {% endif -%}   {#  logging_driver is defined ... #}    
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "http://localhost:4000/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 10    

  supabase-db:
    image: supabase/postgres:{{__SUPABASE_POSTGRESQL_version}}
    container_name: supabase-db
    hostname: supabase-db
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase DB"
      com.platys.password.envvars: "PLATYS_SUPABASE_DB_PASSWORD"
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: 5432
      POSTGRES_PORT: 5432
      PGPASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}
      POSTGRES_PASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}
      PGDATABASE: postgres
      POSTGRES_DB: postgres
      JWT_SECRET: {{SUPABASE_jwt_secret}}
      JWT_EXP: 3600
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      # PGDATA directory is persisted between restarts
    {% if SUPABASE_volume_map_data | default(false) %}  
      - ./container-volume/supabase/db:/var/lib/postgresql/data:Z
    {% endif -%}   {#  SUPABASE_volume_map_data #}
    {% if SUPABASE_volume_map_custom_conf | default(false) %}
      # Use  volume to persist pgsodium decryption key between restarts      
      - ./custom-conf/supabase:/etc/postgresql-custom      
    {% endif -%}   {#  SUPABASE_volume_map_custom_conf #}

      - ./init/supabase/db/realtime.sql:/docker-entrypoint-initdb.d/migrations/99-realtime.sql:Z
      # Must be superuser to create event trigger
      - ./init/supabase/db/webhooks.sql:/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql:Z
      # Must be superuser to alter reserved role
      - ./init/supabase/db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      # Initialize the database settings with JWT_SECRET and JWT_EXP
      - ./init/supabase/db/jwt.sql:/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql:Z
      # Changes required for internal supabase data such as _analytics
      - ./init/supabase/db/_supabase.sql:/docker-entrypoint-initdb.d/migrations/97-_supabase.sql:Z
      # Changes required for Analytics support
      - ./init/supabase/db/logs.sql:/docker-entrypoint-initdb.d/migrations/99-logs.sql:Z
      # Changes required for Pooler support
      - ./init/supabase/db/pooler.sql:/docker-entrypoint-initdb.d/migrations/99-pooler.sql:Z
    {% if SUPABASE_db_provision_seed_data | default(false) %}
      # Seed data should be inserted last (alphabetical order)
      - ./init/supabase/db/seed/data.sql:/docker-entrypoint-initdb.d/999-seed.sql      
    {% endif -%}   {#  SUPABASE_db_provision_seed_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging 
    {% endif -%}   {#  logging_driver is defined ... #}   
    command:
      [
        "postgres",
        "-c",
        "config_file=/etc/postgresql/postgresql.conf",
        "-c",
        "log_min_messages=fatal" # prevents Realtime polling queries from appearing in logs
      ]     
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
        "CMD",
        "pg_isready",
        "-U",
        "postgres",
        "-h",
        "localhost"
        ]
      interval: 5s
      timeout: 5s
      retries: 10 

  supabase-vector:
    image: timberio/vector:{{__SUPABASE_VECTOR_version}}
    container_name: supabase-vector
    hostname: supabase-vector
    labels:
      com.platys.name: "vector"
      com.platys.description: "Supabase DB"
    ports:
      - 8686:8686
    environment:
      LOGFLARE_API_KEY: {{SUPABASE_logflare_api_key}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/supabase/vector/vector.yml:/etc/vector/vector.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro        
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging 
    {% endif -%}   {#  logging_driver is defined ... #}   
    command:
      [
        "--config",
        "/etc/vector/vector.yml"
      ]
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://vector:9001/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3

  supabase-supavisor:
    image: supabase/supavisor:{{__SUPABASE_SUPAVISOR_version}}
    container_name: supabase-supavisor
    hostname: supabase-supavisor
    labels:
      com.platys.name: "supabase"
      com.platys.description: "Supabase Supavisor"
    ports:
      - 5441:5432
      - 6543:6543
    environment:
      PORT: 4000
      POSTGRES_PORT: 5432
      POSTGRES_DB: postgres
      POSTGRES_PASSWORD: ${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}
      DATABASE_URL: ecto://supabase_admin:${PLATYS_SUPABASE_DB_PASSWORD:-{{SUPABASE_db_password}}}@db:5432/_supabase
      CLUSTER_POSTGRES: true
      SECRET_KEY_BASE: {{SUPABASE_secret_key_base}}
      VAULT_ENC_KEY: {{SUPABASE_vault_enc_key}}
      API_JWT_SECRET: {{SUPABASE_jwt_secret}}
      METRICS_JWT_SECRET: {{SUPABASE_jwt_secret}}
      REGION: local
      ERL_AFLAGS: -proto_dist inet_tcp
      POOLER_TENANT_ID: {{SUPABASE_tenant_id if SUPABASE_tenant_id is defined and SUPABASE_tenant_id and SUPABASE_tenant_id | length else omit}}
      POOLER_DEFAULT_POOL_SIZE: 20
      POOLER_MAX_CLIENT_CONN: 100
      POOLER_POOL_MODE: transaction
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/supabase/pooler/pooler.exs:/etc/pooler/pooler.exs:ro     
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging 
    {% endif -%}   {#  logging_driver is defined ... #}   
    command:
      [
        "/bin/sh",
        "-c",
        "/app/bin/migrate && /app/bin/supavisor eval \"$$(cat /etc/pooler/pooler.exs)\" && /app/bin/server"
      ]
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "http://127.0.0.1:4000/api/health"
        ]
      interval: 10s
      timeout: 5s
      retries: 5
{% endif %}   {#  SUPABASE_enable #}

{% if TIMESCALEDB_enable | default(false) %}
  #  ================================== Timescale DB ========================================== #
  timescaledb:
    image: timescale/timescaledb:{{__TIMESCALEDB_version}}
    container_name: timescaledb
    hostname: timescaledb
    labels:
      com.platys.name: "timescaledb"
      com.platys.description: "Timeseries Database on PostgreSQL"
    ports:
      - "5433:5432"
    environment:
      POSTGRES_PASSWORD: "6AXL3g7}+2G2Kc;"
      POSTGRES_USER: "timescaledb"
      POSTGRES_DB: "timescaledb"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if TIMESCALEDB_volume_map_data %}
      - /container-volume/timescaledb/data:/var/lib/postgresql/data:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: always
{% endif %}   {#  TIMESCALEDB_enable #}

{% if ADMINER_enable | default(false) %}
  #  ================================== Adminer ========================================== #
  adminer:
    image: adminer:{{__ADMINER_version}}
    container_name: adminer
    hostname: adminer
    labels:
      com.platys.name: "adminer"
      com.platys.description: "Relational Database Admin UI"
      com.platys.webui.title: "Adminer UI"
      com.platys.webui.url: "http://dataplatform:28131"
    ports:
      - 28131:8080
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: php -S 0.0.0.0:8080 -t /var/www/html
    restart: {{container_restart_policy}}
{% endif %}   {#  ADMINER_enable #}

{% if CLOUDBEAVER_enable | default(false) %}
  #  ================================== Cloudbeaver ========================================== #
  cloudbeaver:
    image: dbeaver/cloudbeaver:{{__CLOUDBEAVER_version}}
    container_name: cloudbeaver
    hostname: cloudbeaver
    labels:
      com.platys.name: "cloudbeaver"
      com.platys.description: "Cloud Database Manager"
      com.platys.webui.title: "Cloudbeaver UI"
      com.platys.webui.url: "http://dataplatform:8978"
      com.platys.password.envvars: "PLATYS_CLOUDBEAVER_ADMIN_PASSWORD"
    ports:
      - "8978:8978"
    environment:
      CB_SERVER_NAME: Cloudbeaver
    {% if CLOUDBEAVER_admin_username is defined and CLOUDBEAVER_admin_username and CLOUDBEAVER_admin_username | length %}  
      CB_ADMIN_NAME: {{CLOUDBEAVER_admin_username}}
      CB_ADMIN_PASSWORD: ${PLATYS_CLOUDBEAVER_ADMIN_PASSWORD:-{{CLOUDBEAVER_admin_password}}}
    {% endif -%}   {#  CLOUDBEAVER_admin_username #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/cloudbeaver/data-sources.json:/opt/cloudbeaver/workspace/GlobalConfiguration/.dbeaver/data-sources.json
    {% if CLOUDBEAVER_volume_map_workspace %}
      - "./container-volume/cloudbeaver/workspace:/opt/cloudbeaver/workspace"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CLOUDBEAVER_enable #}

{% if SQLPAD_enable | default(false) %}
  #  ================================== SQLPad ========================================== #
  sqlpad:
    image: sqlpad/sqlpad:{{__SQLPAD_version}}
    container_name: sqlpad
    hostname: sqlpad
    labels:
      com.platys.name: "sqlpad"
      com.platys.description: "Web-based SQL editor"
      com.platys.webui.title: "SQLPad UI"
      com.platys.webui.url: "http://dataplatform:28194"
    ports:
      - "28194:3000"
    environment:
      SQLPAD_ADMIN: 'admin@sqlpad.com'
      SQLPAD_ADMIN_PASSWORD: 'admin'
      SQLPAD_APP_LOG_LEVEL: debug
      SQLPAD_WEB_LOG_LEVEL: warn
      SQLPAD_SEED_DATA_PATH: /etc/sqlpad/seed-data
      SQLPAD_CONNECTIONS__pgdemo__name: Postgres demo
      SQLPAD_CONNECTIONS__pgdemo__driver: postgres
      SQLPAD_CONNECTIONS__pgdemo__host: postgres
      SQLPAD_CONNECTIONS__pgdemo__database: demodb
      SQLPAD_CONNECTIONS__pgdemo__username: demo
      SQLPAD_CONNECTIONS__pgdemo__password: abc123!
      SQLPAD_CONNECTIONS__pgdemo__multiStatementTransactionEnabled: 'true'
      SQLPAD_CONNECTIONS__pgdemo__idleTimeoutSeconds: 86400
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SQLPAD_enable #}

{% if SQLCHAT_enable | default(false) %}
  #  ================================== SQLChat ========================================== #
  sqlchat:
    image: sqlchat/sqlchat:{{__SQLCHAT_version}}
    container_name: sqlchat
    hostname: sqlchat
    labels:
      com.platys.name: "sqlchat"
      com.platys.description: "Chat-based SQL Client and Editor"
      com.platys.webui.title: "SQL Chat UI"
      com.platys.webui.url: "http://dataplatform:28305"
    ports:
      - "28305:3000"
    environment:
      OPENAI_API_KEY: {{SQLCHAT_api_key}}
      OPENAI_API_ENDPOINT: {{SQLCHAT_api_endpoint}}
      NEXT_PUBLIC_DATABASE_LESS: {{SQLCHAT_database_less}}
      DATABASE_URL: 'postgresql://postgresql:postgres@abc123!:5432/postgres?schema=postgres'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SQLCHAT_enable #}

{% if QUERYBOOK_enable | default(false) %}
  #  ================================== Querybook ========================================== #
  querybook-web:
    image: querybook/querybook:{{__QUERYBOOK_version}}
    container_name: querybook-web
    hostname: querybook-web
    labels:
      com.platys.name: "querybook"
      com.platys.description: "Big Data Querying UI"
      com.platys.webui.title: "Querybook UI"
      com.platys.webui.url: "http://dataplatform:28301"
    ports:
      - "28301:10001"
    environment:
      PORT: 10001
      APIPORT: 3000
      HOT_RELOAD: true
      FLASK_SECRET_KEY: SOME_RANDOM_SECRET_KEY
      DATABASE_CONN: mysql+pymysql://test:passw0rd@querybook-mysql:3306/querybook2?charset=utf8mb4
      REDIS_URL: redis://querybook-redis:6379/0
      ELASTICSEARCH_HOST: querybook-elasticsearch:9200
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: './querybook/scripts/runservice prod_web'
    restart: {{container_restart_policy}}

  querybook-worker:
    image: querybook/querybook:{{__QUERYBOOK_version}}
    container_name: querybook-worker
    hostname: querybook-worker
    labels:
      com.platys.name: "querybook"
      com.platys.description: "Big Data Querying UI"
    environment:
      FLASK_SECRET_KEY: SOME_RANDOM_SECRET_KEY
      DATABASE_CONN: mysql+pymysql://test:passw0rd@querybook-mysql:3306/querybook2?charset=utf8mb4
      REDIS_URL: redis://querybook-redis:6379/0
      ELASTICSEARCH_HOST: querybook-elasticsearch:9200
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    tty: true
    stdin_open: true
    command: ./querybook/scripts/runservice prod_worker -c 150 -l info -Ofair -n celery@%h
    restart: {{container_restart_policy}}

  querybook-scheduler:
    image: querybook/querybook:{{__QUERYBOOK_version}}
    container_name: querybook-scheduler
    hostname: querybook-scheduler
    labels:
      com.platys.name: "querybook"
      com.platys.description: "Big Data Querying UI"
    environment:
      FLASK_SECRET_KEY: SOME_RANDOM_SECRET_KEY
      DATABASE_CONN: mysql+pymysql://test:passw0rd@querybook-mysql:3306/querybook2?charset=utf8mb4
      REDIS_URL: redis://querybook-redis:6379/0
      ELASTICSEARCH_HOST: querybook-elasticsearch:9200
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    tty: true
    stdin_open: true
    command: './querybook/scripts/runservice prod_scheduler'
    restart: {{container_restart_policy}}

  querybook-redis:
    image: redis:5.0.9
    container_name: querybook-redis
    hostname: querybook-redis
    labels:
      com.platys.name: "redis"
      com.platys.description: "Key-Value Store"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: ["redis-server", "--appendonly", "yes"]
    restart: {{container_restart_policy}}

  querybook-mysql:
    image: mysql:8.0
    container_name: querybook-mysql
    hostname: querybook-mysql
    labels:
      com.platys.name: "mysql"
      com.platys.description: "Relational Database"
    environment:
      MYSQL_HOST: querybook-mysql:3306
      MYSQL_DATABASE: querybook2
      MYSQL_USER: test
      MYSQL_PASSWORD: passw0rd
      # Password for root access
      MYSQL_ROOT_PASSWORD: hunter2
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: ["--character-set-server=utf8mb4", "--collation-server=utf8mb4_unicode_ci"]
    restart: {{container_restart_policy}}

  querybook-elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
    container_name: querybook-elasticsearch
    hostname: querybook-elasticsearch
    labels:
      com.platys.name: "elasticsearch"
      com.platys.description: "Search-Engine Datastore"
    environment:
      cluster.name: docker-cluster
      bootstrap.memory_lock: 'true'
      discovery.type: single-node
      ES_JAVA_OPTS: -Xms750m -Xmx750m
      cluster.routing.allocation.disk.threshold_enabled: 'true'
      cluster.routing.allocation.disk.watermark.low: 2gb
      cluster.routing.allocation.disk.watermark.high: 1gb
      cluster.routing.allocation.disk.watermark.flood_stage: 512mb
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  QUERYBOOK_enable #}

{% if DBGATE_enable | default(false) %}
  #  ================================== DbGate ========================================== #
  dbgate:
    image: dbgate/dbgate{{'-premium' if DBGATE_edition == 'premium'}}:{{__DBGATE_version}}
    container_name:   dbgate
    hostname: dbgate
    labels:
      com.platys.name: "dbgate"
      com.platys.description: "Open Source DB Management Tool"
      com.platys.webui.title: "DbGate UI"
      com.platys.webui.url: "http://dataplatform:28120"
      com.platys.password.envvars: "PLATYS_DBGATE_PASSWORD"
    ports:
      - "28120:3000"
    environment:
      LOG_LEVEL: info
    {% if DBGATE_user is defined and DBGATE_user and DBGATE_user | length %}  
      LOGIN: {{DBGATE_user}}
      PASSWORD: ${PLATYS_DBGATE_PASSWORD:-{{DBGATE_password}}}
    {% endif -%}   {#  DBGATE_user #}
      DBGATE_LICENSE: {{DBGATE_license if DBGATE_license is defined and DBGATE_license and DBGATE_license | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      {% if DBGATE_volume_map_data | default(false) %}
      - ./container-volume/dbgate/data:/root/.dbgate
      {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DBGATE_enable #}


{% if NOCODB_enable | default(false) %}
  #  ================================== NocoDB ========================================== #
  nocodb:
    image: nocodb/nocodb:{{__NOCODB_version}}
    container_name: nocodb
    hostname: nocodb
    labels:
      com.platys.name: "nocodb"
      com.platys.description: "Open Source Airtable Alternative"
      com.platys.webui.title: "NocoDB UI"
      com.platys.webui.url: "http://dataplatform:28276"
    ports:
      - "28276:8080"
    environment:
      NC_DB: 'pg://postgresql:5432?u=postgres&p=abc123!&d=postgres'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      {% if NOCODB_volume_map_data | default(false) %}
      - ./container-volume/nocodb/data:/usr/app/data
      {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  NOCODB_enable #}

{% if QUIX_enable | default(false) %}
  #  ================================== Quix ========================================== #
  quix-backend:
    image: wixquix/quix-backend:{{__QUIX_version}}
    container_name: quix-backend
    hostname: quix-backend
    labels:
      com.platys.name: "quix"
      com.platys.description: "Notebook manager with support for Trino, Athena, RDBMS, ClickHouse, .."
      com.platys.restapi.title: "Quix REST API"
      com.platys.restapi.url: "http://dataplatform:28159"
    ports:
      - "28159:8081"
    environment:
      MODULES: presto
      MODULES_PRESTO_ENGINE: presto
    {% if TRINO_enable | default(false) %}
      MODULES_PRESTO_API: http://trino-1:8080/v1
    {% else -%}   {#  TRINO_enable #}
      MODULES_PRESTO_API: http://presto-1:8080/v1
    {% endif -%}   {#  TRINO_enable #}
      MODULES_PRESTO_CATALOG: system
      MODULES_PRESTO_SCHEMA: runtime
      MODULES_PRESTO_SOURCE: quix
      MODULES_PRESTO_DB_EMPTY_TIMEOUT: 60000
      MODULES_PRESTO_DB_REQUEST_TIMEOUT: 10000
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  quix-frontend:
    image: wixquix/quix-frontend:{{__QUIX_version}}
    container_name: quix-frontend
    hostname: quix-frontend
    labels:
      com.platys.name: "quix"
      com.platys.description: "Notebook manager with support for Trino, Athena, RDBMS, ClickHouse, .."
      com.platys.webui.title: "Quix UI"
      com.platys.webui.url: "http://dataplatform:28160"
    ports:
      - "28160:3000"
    environment:
      BACKEND_INTERNAL_URL: http://quick-backend:8081
      BACKEND_PUBLIC_URL: http://${PUBLIC_IP}:28159
      DB_NAME: quix
      DB_USER: root
      DB_HOST: quix-db
      DB_PORT: 3306
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  quix-db:
    image: "mysql:5.7"
    container_name: quix-db
    hostname: quix-db
    labels:
      com.platys.name: "quix"
      com.platys.description: "Notebook manager with support for Trino, Athena, RDBMS, ClickHouse, .."
    environment:
      MYSQL_ALLOW_EMPTY_PASSWORD: "'yes'"
      MYSQL_DATABASE: "quix"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  QUIX_enable #}

{% if HAZELCAST_enable | default(false) %}
  #  ================================== Hazelcast IMDG ========================================== #
  {% for num in range(HAZELCAST_nodes | default(1) ) %}
    {% set port = 5701 %}
    {% set external_port = 5701 + loop.index - 1 %}
  hazelcast-{{loop.index}}:
    image: hazelcast/hazelcast:{{__HAZELCAST_version}}
    container_name: hazelcast-{{loop.index}}
    hostname: hazelcast-{{loop.index}}
    labels:
      com.platys.name: "hazelcast"
      com.platys.description: "In-Memory Data Grid"
    ports:
      - "{{external_port}}:{{port}}"
    environment:
      - LOGGING_LEVEL=INFO
      - HZ_NETWORK_PUBLICADDRESS=${PUBLIC_IP}:{{external_port}}
      - HZ_NETWORK_JOIN_AUTODETECTION_ENABLED=false
      - HZ_NETWORK_JOIN_MULTICAST_ENABLED=true
      - HZ_NETWORK_JOIN_AWS_ENABLED=false
      - HZ_JET_ENABLED={{HAZELCAST_use_jet}}
    {% if HAZELCAST_volume_map_custom_config | default(false) %}
      - HAZELCAST_CONFIG=hazelcast.yml
    {% endif -%}   {#  HAZELCAST_volume_map_custom_config #}
      - CLASSPATH=/opt/hazelcast/ext/*
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/hazelcast:/opt/hazelcast/ext
    {% if HAZELCAST_volume_map_custom_config | default(false) %}
      - ./hazelcast.yml:/opt/hazelcast/hazelcast.yml
    {% endif -%}   {#  HAZELCAST_volume_map_custom_config #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endfor %}

  {% if HAZELCAST_MC_enable | default(false) %}
  hazelcast-mc:
    image: hazelcast/management-center:{{__HAZELCAST_MC_version}}
    container_name: hazelcast-mc
    hostname: hazelcast-mc
    labels:
      com.platys.name: "hazelcast-mc"
      com.platys.description: "In-Memory Data Grid"
      com.platys.webui.title: "Hazelcast Mangement Center UI"
      com.platys.webui.url: "http://dataplatform:28209"
    ports:
      - 28209:8080
    environment:
      - LOGGING_LEVEL=INFO
#      - MC_INIT_CMD="/opt/hazelcast/management-center/bin/mc-conf.sh cluster add -cn='my-cluster' -ma='hazelcast-1'"
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
#    command: bash -c "set -euo pipefail && /opt/hazelcast/management-center/bin/mc-conf.sh cluster add --cluster-name='my-cluster' --member-addresses='hazelcast-1' && /opt/hazelcast/management-center/bin/mc-start.sh"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  HAZELCAST_MC_enable #}
{% endif %}   {#  HAZELCAST_enable #}

{% if AXON_enable | default(false) %}
  #  ================================== Axon Server ========================================== #
  axon-server:
    image: axoniq/axonserver:{{__AXON_version}}
    container_name: axon-server
    hostname: axon-server
    labels:
      com.platys.name: "axon-server"
      com.platys.description: "EventStore Database"
      com.platys.webui.title: "Axon UI"
      com.platys.webui.url: "http://dataplatform:8024"
    ports:
      - 8024:8024
      - 8124:8124
    environment:
      - AXONSERVER_HOSTNAME=axon-server
      - AXONSERVER_EVENTSTORE=/eventstore
      - AXONSERVER_CONTROLDB=/controldb
      - AXONSERVER_HTTP_PORT=8024
      - AXONSERVER_GRPC_PORT=8124
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  AXON_enable #}

{% if EVENTSTORE_enable | default(false) %}
  #  ================================== Axon Server ========================================== #
  eventstore:
    image: eventstore/eventstore:{{__EVENTSTORE_version}}
    container_name: eventstore
    hostname: eventstore
    labels:
      com.platys.name: "eventstore"
      com.platys.description: "EventStore Database"
      com.platys.webui.title: "EventStore UI"
      com.platys.webui.url: "http://dataplatform:2113"
    ports:
      - "1113:1113"
      - "2113:2113"
    environment:
      - EVENTSTORE_CLUSTER_SIZE=1
      - EVENTSTORE_RUN_PROJECTIONS=All
      - EVENTSTORE_START_STANDARD_PROJECTIONS=true
      - EVENTSTORE_EXT_TCP_PORT=1113
      - EVENTSTORE_EXT_HTTP_PORT=2113
      - EVENTSTORE_INSECURE=true
      - EVENTSTORE_ENABLE_EXTERNAL_TCP=true
      - EVENTSTORE_ENABLE_ATOM_PUB_OVER_HTTP=true
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  enable #}

{% if TRINO_enable | default(false) %}
  {% set ns2 = namespace(eventListnerPropertyFiles='') %}
  {% if TRINO_event_listeners is defined and TRINO_event_listeners | length | default(false) %}
    {% for eventListener in TRINO_event_listeners.split(',') %}
      {% if loop.first %}
       {% set ns2.eventListnerPropertyFiles = '/etc/trino/' ~ eventListener ~ ".properties" %}
      {% else %}
       {% set ns2.eventListnerPropertyFiles = ns2.eventListnerPropertyFiles ~ ',/etc/trino/' ~ eventListener ~ ".properties" %}
      {% endif %}
    {% endfor %}
  {% endif %}
  #  ================================== Trino ========================================== #
  trino-1:
  {% if (TRINO_edition | lower) == 'starburstdata'  %}
    image: {{TRINO_custom_image_name if TRINO_custom_image_name is defined and TRINO_custom_image_name | length else ' starburstdata/starburst-enterprise'}}:{{__STARBURSTDATA_version}}
    {% set trino_etc_dir = '/etc/starburst' %}
    {% set conf_dir = 'starburstdata' %}
  {% elif (TRINO_edition | lower) == 'oss' %}
    image: {{TRINO_custom_image_name if TRINO_custom_image_name is defined and TRINO_custom_image_name | length else ' trinodb/trino'}}:{{__TRINO_version}}
    {% set trino_etc_dir = '/etc/trino' %}
    {% set conf_dir = 'trino' %}
  {% endif -%}   {#  TRINO_edition #}
    hostname: trino-1
    container_name: trino-1
    labels:
      com.platys.name: "trino"
      com.platys.description: "SQL Virtualization Engine"
      com.platys.webui.title: "Trino UI"
      com.platys.webui.url: "http{{"s" if TRINO_auth_enabled}}://dataplatform:{{28083 if TRINO_auth_enabled else 28082}}/{{'ui/preview' if (TRINO_edition | lower) == 'oss'}}"
    ports:
      - '28082:8080'
      - '28083:8443'
    environment:
      # this is only generated to keep the structure valid if no other env variables are present
      DUMMY: make-it-valid
  {% if (TRINO_edition | lower) == 'starburstdata' %}
      DATA_PRODUCT_ENABLED: '{{'true' if TRINO_starburstdata_enable_data_product is defined and TRINO_starburstdata_enable_data_product else 'false'}}'
  {% endif %}
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      S3_ENDPOINT: {{s3Endpoint | default(omit) }}
      S3_REGION: {{s3DefaultRegion | default(omit) }}
      S3_AWS_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      S3_AWS_SECRET_KEY: {{s3SecretAccessKey}}
      S3_PATH_STYLE_ACCESS: '{{s3PathStyleAccess}}'
      HIVE_STORAGE_FORMAT: '{{TRINO_hive_storage_format if TRINO_hive_storage_format is defined and TRINO_hive_storage_format | length else omit}}'
      HIVE_COMPRESSION_CODEC: '{{TRINO_hive_compression_codec if TRINO_hive_compression_codec is defined and TRINO_hive_compression_codec | length else omit}}'
      HIVE_VIEWS_ENABLED: '{{'true' if TRINO_hive_views_enabled else 'false'}}'
      HIVE_RUN_AS_INVOKER: '{{'true' if TRINO_hive_run_as_invoker else 'false'}}'
      HIVE_LEGACY_TRANSLATION: '{{'true' if TRINO_hive_legacy_translation else 'false'}}'
      NESSIE_CATALOG_WAREHOUSE_DIR: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/nessie/warehouse'}}
  {% endif %}
  {%if external['ADLS_enable'] | default(false) %}
      ADLS_STORAGE_ACCOUNT: {{adlsStorageAccount | default(omit) }}
      ADLS_ACCESS_KEY: {{adlsAccessKey}}
  {% endif -%}   {#  external['ADLS_enable'] #}
  {% if TRINO_kafka_table_names is defined and TRINO_kafka_table_names|length and (KAFKA_enable or external['KAFKA_enable']) | default(false)  %}
      KAFKA_NODES: {{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      KAFKA_SECURITY_PROTOCOL: '{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      KAFKA_CONFIG_RESOURCES: '{{trino_etc_dir}}/kafka-configuration.properties'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: "https"
      KAFKA_SSL_TRUSTSTORE_LOCATION: /certs/client.ts.p12
      KAFKA_SSL_TRUSTSTORE_PASSWORD: {{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      KAFKA_SSL_KEYSTORE_TYPE: "PKCS12"
      KAFKA_SSL_KEYSTORE_LOCATION: /certs/client.ks.p12
      KAFKA_SSL_KEYSTORE_PASSWORD: {{KAFKA_client_keystore_password}}
      KAFKA_SSL_KEY_PASSWORD: {{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
      KAFKA_TABLE_NAMES: {{TRINO_kafka_table_names}}
      KAFKA_DEFAULT_SCHEMA: {{TRINO_kafka_default_schema}}
      KAFKA_TABLE_DESCRIPTOR_DIR: '{{trino_etc_dir}}/kafka/'
  {% endif %}
  {% if POSTGRESQL_enable | default(false) %}
      POSTGRESQL_DATABASE: {{TRINO_postgresql_database if TRINO_postgresql_database is defined and TRINO_postgresql_database and TRINO_postgresql_database | length else POSTGRESQL_database}}
      POSTGRESQL_USER: {{TRINO_postgresql_user if TRINO_postgresql_user is defined and TRINO_postgresql_user and TRINO_postgresql_user | length else POSTGRESQL_user}}
      POSTGRESQL_PASSWORD: {{TRINO_postgresql_password if TRINO_postgresql_password is defined and TRINO_postgresql_password and TRINO_postgresql_password | length else POSTGRESQL_password}}
  {% endif %}
  {% if MYSQL_enable | default(false) %}
      MYSQL_DATABASE: {{MYSQL_database | default('demodb') }}
      MYSQL_USER: {{MYSQL_user | default('demo') }}
      MYSQL_PASSWORD: {{MYSQL_password | default('abc123!') }}
  {% endif %}
  {% if MARIADB_enable | default(false) %}
      MARIADB_DATABASE: {{MARIADB_database | default('demodb') }}
      MARIADB_version_USER: {{MARIADB_user | default('demo') }}
      MARIADB_PASSWORD: {{MARIADB_password | default('abc123!') }}
  {% endif %}
  {% if SQLSERVER_enable | default(false) %}
      SQLSERVER_DATABASE: {{TRINO_sqlserver_database | default('demodb') }}
      SQLSERVER_USER: {{TRINO_sqlserver_user | default('demo') }}
      SQLSERVER_PASSWORD: {{TRINO_sqlserver_password | default('abc123!') }}
  {% endif %}
  {% if ORACLE_EE_enable or ORACLE_XE_enable | default(false) %}
      ORACLE_USER: {{TRINO_oracle_user | default('demo') }}
      ORACLE_PASSWORD: {{TRINO_oracle_password | default('abc123!') }}
  {% endif %}
  {% if REDIS_enable | default(false) %}
      REDIS_TABLE_NAMES: '{{TRINO_redis_table_names if TRINO_redis_table_names is defined and TRINO_redis_table_names and TRINO_redis_table_names | length else '' }}'
      REDIS_NODES: redis-1:6379
  {% endif %}
  {% if REDIS_STACK_enable | default(false) %}
      REDIS_TABLE_NAMES: '{{TRINO_redis_stack_table_names if TRINO_redis_stack_table_names is defined and TRINO_redis_stack_table_names and TRINO_redis_stack_table_names | length else '' }}'
      REDIS_NODES: redis-stack-1:6379

      REDISEARCH_URI: redis://redis-stack-1:6379
      REDISEARCH_USERNAME:
      REDISEARCH_PASSWORD:
      REDISEARCH_CASE_INSENSITIVE_NAMES: false
      REDISEARCH_CURSOR_COUNT: 1000
      REDISEARCH_DEFAULT_LIMIT: 10000
  {% endif %}
  {% if TRINO_event_listeners is defined and TRINO_event_listeners | length | default(false) %}
      EVENT_LISTENER_CONFIG_FILES: {{ns2.eventListnerPropertyFiles}}
  {% else %}
      EVENT_LISTENER_CONFIG_FILES: ''
  {% endif %}
  {% if OPA_enable | default(false) %}
      OPA_ENDPOINT: http://opa:8181
      OPA_NAMESPACE: {{TRINO_opa_namespace if TRINO_opa_namespace is defined and TRINO_opa_namespace and TRINO_opa_namespace | length else 'trino'}}
  {% endif %}
  {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
  {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
  {% if (TRINO_install | lower) == 'cluster'  %}
      - ./conf/{{conf_dir}}/cluster/coordinator.config{{'-dp' if TRINO_starburstdata_enable_data_product is defined and TRINO_starburstdata_enable_data_product else ''}}.properties:{{trino_etc_dir}}/config.properties
      - ./conf/{{conf_dir}}/cluster/node.properties:{{trino_etc_dir}}/node.properties
  {% else %}
      - ./conf/{{conf_dir}}/single/config{{'-dp' if TRINO_starburstdata_enable_data_product is defined and TRINO_starburstdata_enable_data_product else ''}}{{'-auth' if TRINO_auth_enabled}}.properties:{{trino_etc_dir}}/config.properties
      - ./conf/{{conf_dir}}/single/node.properties:{{trino_etc_dir}}/node.properties
  {% endif %}
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - './conf/{{conf_dir}}/catalog/minio.properties:{{trino_etc_dir}}/catalog/minio.properties'
    {%if SPARK_table_format_type == 'iceberg' | default(false) %}
      - './conf/{{conf_dir}}/catalog/iceberg.properties:{{trino_etc_dir}}/catalog/iceberg.properties'
      {%if NESSIE_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/iceberg-nessie.properties:{{trino_etc_dir}}/catalog/iceberg-nessie.properties'
      - './conf/{{conf_dir}}/catalog/iceberg-rest.properties:{{trino_etc_dir}}/catalog/iceberg-rest.properties'
      {% endif %}
    {%elif SPARK_table_format_type == 'delta' | default(false) %}
      - './conf/{{conf_dir}}/catalog/delta.properties:{{trino_etc_dir}}/catalog/delta.properties'
    {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      - './conf/{{conf_dir}}/catalog/hudi.properties:{{trino_etc_dir}}/catalog/hudi.properties'
    {% endif %}
    {%if UNITY_CATALOG_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/iceberg-unity.properties:{{trino_etc_dir}}/catalog/iceberg-unity.properties'
    {% endif %}
  {% endif %}
  {%if external['ADLS_enable'] | default(false) %}
      - './conf/{{conf_dir}}/catalog/adls.properties:{{trino_etc_dir}}/catalog/adls.properties'
  {% endif %}
  {% if TRINO_kafka_table_names is defined and TRINO_kafka_table_names|length and (KAFKA_enable or external['KAFKA_enable']) | default(false)  %}
      - './conf/{{conf_dir}}/catalog/kafka.properties:{{trino_etc_dir}}/catalog/kafka.properties'
      - './conf/{{conf_dir}}/kafka/:{{trino_etc_dir}}/kafka/'
    {% if ns.secureKafkaEnabled %}
      - './security/kafka/sasl-scram/client.properties:{{trino_etc_dir}}/kafka-configuration.properties'
    {% endif %}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
  {% endif %}
  {%if HADOOP_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/hdfs.properties:{{trino_etc_dir}}/catalog/hdfs.properties'
  {% endif %}
  {% if POSTGRESQL_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/postgresql.properties:{{trino_etc_dir}}/catalog/postgresql.properties'
  {% endif %}
  {% if MYSQL_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/mysql.properties:{{trino_etc_dir}}/catalog/mysql.properties'
  {% endif %}
  {% if MARIADB_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/mariadb.properties:{{trino_etc_dir}}/catalog/mariadb.properties'
  {% endif %}
  {% if SQLSERVER_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/sqlserver.properties:{{trino_etc_dir}}/catalog/sqlserver.properties'
  {% endif %}
  {% if ORACLE_EE_enable or ORACLE_XE_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/oracle.properties:{{trino_etc_dir}}/catalog/oracle.properties'
  {% endif %}
  {% if REDIS_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/redis.properties:{{trino_etc_dir}}/catalog/redis.properties'
  {% endif %}
  {% if REDIS_STACK_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/redis-stack.properties:{{trino_etc_dir}}/catalog/redis-stack.properties'
      - './conf/{{conf_dir}}/catalog/redis-search.properties:{{trino_etc_dir}}/catalog/redis-search.properties'
   {% endif %}
  {% if MONGO_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/mongo.properties:{{trino_etc_dir}}/catalog/mongo.properties'
  {% endif %}
  {% if ELASTICSEARCH_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/elasticsearch.properties:{{trino_etc_dir}}/catalog/elasticsearch.properties'
  {% endif %}
  {% if PINOT_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/pinot.properties:{{trino_etc_dir}}/catalog/pinot.properties'
  {% endif %}
  {% if DRUID_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/druid.properties:{{trino_etc_dir}}/catalog/druid.properties'
  {% endif %}
  {% if TRINO_with_tpch_catalog | default(false) %}
      - './conf/{{conf_dir}}/catalog/tpch.properties:{{trino_etc_dir}}/catalog/tpch.properties'
  {% endif %}
  {% if TRINO_with_tpcds_catalog | default(false) %}
      - './conf/{{conf_dir}}/catalog/tpcds.properties:{{trino_etc_dir}}/catalog/tpcds.properties'
  {% endif %}
  {% if TRINO_with_memory_catalog | default(false) %}
      - './conf/{{conf_dir}}/catalog/memory.properties:{{trino_etc_dir}}/catalog/memory.properties'
  {% endif %}
  {% if DRUID_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/druid.properties:{{trino_etc_dir}}/catalog/druid.properties'
  {% endif %}
  {% if TRINO_event_listeners is defined and TRINO_event_listeners | length | default(false) %}
    {% for eventListener in (TRINO_event_listeners.split(',')) %}
      - ./conf/trino/{{eventListener}}.properties:{{trino_etc_dir}}/{{eventListener}}.properties
      - ./plugins/trino/event-listener/{{eventListener}}:/usr/lib/trino/plugin/{{eventListener}}
    {% endfor %}
  {% endif %}
  {% if (TRINO_edition | lower) == 'starburstdata' and TRINO_starburstdata_use_license is defined and TRINO_starburstdata_use_license | default(false) %}
      - ./licenses/starburstdata/starburstdata.license:/etc/starburst/starburstdata.license
  {% endif %}
  {% if TRINO_auth_enabled | default(false) %}
      - ./security/{{conf_dir}}/password-authenticator.properties:{{trino_etc_dir}}/password-authenticator.properties
      - ./{{'custom-conf' if TRINO_auth_use_custom_password_file else 'security' }}/{{conf_dir}}/{{'security/' if TRINO_auth_use_custom_password_file}}password.db:{{trino_etc_dir}}/password.db
      - ./{{'custom-conf' if TRINO_auth_use_custom_certs else 'security' }}/{{conf_dir}}/certs:{{trino_etc_dir}}/certs
  {% endif %}
  {% if TRINO_auth_with_groups | default(false) %}
      - ./security/{{conf_dir}}/group-provider.properties:{{trino_etc_dir}}/group-provider.properties
  {% endif %}
  {% if TRINO_access_control_enabled and ((TRINO_access_control_method | lower) == 'file' or (TRINO_access_control_method | lower) == 'opa') | default(false) %}
      - ./security/{{conf_dir}}/access-control-{{TRINO_access_control_method | lower}}.properties:{{trino_etc_dir}}/access-control.properties
  {% endif %}
  {% if TRINO_additional_catalogs is defined and TRINO_additional_catalogs | default(false) %}
    {% for catalogName in (TRINO_additional_catalogs.split(',')) %}
      - './custom-conf/{{conf_dir}}/catalog/{{catalogName}}.properties:{{trino_etc_dir}}/catalog/{{catalogName}}.properties'
    {% endfor %}
  {% endif %}
  {% if TRINO_additional_plugins is defined and TRINO_additional_plugins | default(false) %}
    {% for pluginName in (TRINO_additional_plugins.split(',')) %}
      - ./plugins/{{conf_dir}}/plugin/{{pluginName}}:/usr/lib/trino/plugin/{{pluginName}}
    {% endfor %}
  {% endif %}
      - ./custom-conf/{{conf_dir}}/security:{{trino_etc_dir}}/security
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if (TRINO_install | lower) == 'cluster'  %}
    {% for num in range(TRINO_workers | default(1) ) %}
  trino-worker-{{loop.index}}:
  {% if (TRINO_edition | lower) == 'starburstdata'  %}
    image: starburstdata/starburst-enterprise:{{__STARBURSTDATA_version}}
    {% set trino_etc_dir = '/etc/starburst' %}
    {% set conf_dir = 'starburstdata' %}
  {% elif (TRINO_edition | lower) == 'oss' %}
    image: trinodb/trino:{{__TRINO_version}}
    {% set trino_etc_dir = '/etc/trino' %}
    {% set conf_dir = 'trino' %}
  {% endif -%}   {#  TRINO_edition #}
    hostname: trino-worker-{{loop.index}}
    container_name: trino-worker-{{loop.index}}
    labels:
      com.platys.name: "trino"
      com.platys.description: "SQL Virtualization Engine"
    environment:
      # this is only generated to keep the structure valid if no other env variables are present
      DUMMY: make-it-valid
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      S3_ENDPOINT: {{s3Endpoint | default(omit) }}
      S3_REGION: {{s3DefaultRegion | default(omit) }}
      S3_AWS_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      S3_AWS_SECRET_KEY: {{s3SecretAccessKey}}
      S3_PATH_STYLE_ACCESS: '{{s3PathStyleAccess}}'
      HIVE_STORAGE_FORMAT: '{{TRINO_hive_storage_format if TRINO_hive_storage_format is defined and TRINO_hive_storage_format | length else omit}}'
      HIVE_COMPRESSION_CODEC: '{{TRINO_hive_compression_codec if TRINO_hive_compression_codec is defined and TRINO_hive_compression_codec | length else omit}}'
      NESSIE_CATALOG_WAREHOUSE_DIR: {{SPARK_sql_warehouse_dir if SPARK_sql_warehouse_dir is defined and SPARK_sql_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/nessie/warehouse'}}
  {% endif %}
  {%if external['ADLS_enable'] | default(false) %}
      ADLS_STORAGE_ACCOUNT: {{adlsStorageAccount | default(omit) }}
      ADLS_ACCESS_KEY: {{adlsAccessKey}}
  {% endif -%}   {#  external['ADLS_enable'] #}
  {% if (KAFKA_enable or external['KAFKA_enable']) | default(false) %}
      KAFKA_NODES: {{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      KAFKA_SECURITY_PROTOCOL: '{{ns.securityProtocolBROKER}}'
      KAFKA_CONFIG_RESOURCES: '{{trino_etc_dir}}/kafka-configuration.properties'
    {% endif -%}   {#  secureKafkaEnabled #}
      KAFKA_TABLE_NAMES: {{TRINO_kafka_table_names}}
      KAFKA_DEFAULT_SCHEMA: {{TRINO_kafka_default_schema}}
      KAFKA_TABLE_DESCRIPTOR_DIR: '{{trino_etc_dir}}/kafka/'
  {% endif %}
  {% if POSTGRESQL_enable | default(false) %}
      POSTGRESQL_DATABASE: {{TRINO_postgresql_database if TRINO_postgresql_database is defined else POSTGRESQL_database}}
      POSTGRESQL_USER: {{TRINO_postgresql_user if TRINO_postgresql_user is defined else POSTGRESQL_user}}
      POSTGRESQL_PASSWORD: {{TRINO_postgresql_password if TRINO_postgresql_user is defined else POSTGRESQL_password}}
  {% endif %}
  {% if MYSQL_enable | default(false) %}
      MYSQL_DATABASE: {{MYSQL_database | default('demodb') }}
      MYSQL_USER: {{MYSQL_user | default('demo') }}
      MYSQL_PASSWORD: {{MYSQL_password | default('abc123!') }}
  {% endif %}
  {% if MARIADB_enable | default(false) %}
      MARIADB_DATABASE: {{MARIADB_database | default('demodb') }}
      MARIADB_version_USER: {{MARIADB_user | default('demo') }}
      MARIADB_PASSWORD: {{MARIADB_password | default('abc123!') }}
  {% endif %}
  {% if SQLSERVER_enable | default(false) %}
      SQLSERVER_DATABASE: {{TRINO_sqlserver_database | default('demodb') }}
      SQLSERVER_USER: {{TRINO_sqlserver_user | default('demo') }}
      SQLSERVER_PASSWORD: {{TRINO_sqlserver_password | default('abc123!') }}
  {% endif %}
  {% if ORACLE_EE_enable or ORACLE_XE_enable | default(false) %}
      ORACLE_USER: {{TRINO_oracle_user | default('demo') }}
      ORACLE_PASSWORD: {{TRINO_oracle_password | default('abc123!') }}
  {% endif %}
  {% if REDIS_enable | default(false) %}
      REDIS_TABLE_NAMES: '{{TRINO_redis_table_names if TRINO_redis_table_names is defined and TRINO_redis_table_names and TRINO_redis_table_names | length else '' }}'
      REDIS_NODES: redis-1:6379
  {% endif %}
  {% if REDIS_STACK_enable | default(false) %}
      REDIS_TABLE_NAMES: '{{TRINO_redis_stack_table_names if TRINO_redis_stack_table_names is defined and TRINO_redis_stack_table_names and TRINO_redis_stack_table_names | length else '' }}'
      REDIS_NODES: redis-stack-1:6379

      REDISEARCH_URI: redis://redis-stack-1:6379
      REDISEARCH_USERNAME:
      REDISEARCH_PASSWORD:
      REDISEARCH_CASE_INSENSITIVE_NAMES: false
      REDISEARCH_CURSOR_COUNT: 1000
      REDISEARCH_DEFAULT_LIMIT: 10000
  {% endif %}
  {% if TRINO_event_listeners is defined and TRINO_event_listeners | length | default(false) %}
      EVENT_LISTENER_CONFIG_FILES: {{ns2.eventListnerPropertyFiles}}
  {% else %}
      EVENT_LISTENER_CONFIG_FILES: ''
  {% endif %}
  {% if OPA_enable | default(false) %}
      OPA_ENDPOINT: http://opa:8181
  {% endif %}  
  {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
  {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/{{conf_dir}}/cluster/worker.config.properties:{{trino_etc_dir}}/config.properties
      - ./conf/{{conf_dir}}/cluster/node.properties:{{trino_etc_dir}}/node.properties
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - './conf/{{conf_dir}}/catalog/minio.properties:{{trino_etc_dir}}/catalog/minio.properties'
    {%if SPARK_table_format_type == 'iceberg' | default(false) %}
      - './conf/{{conf_dir}}/catalog/iceberg.properties:{{trino_etc_dir}}/catalog/iceberg.properties'
      {%if NESSIE_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/iceberg-nessie.properties:{{trino_etc_dir}}/catalog/iceberg-nessie.properties'
      - './conf/{{conf_dir}}/catalog/iceberg-rest.properties:{{trino_etc_dir}}/catalog/iceberg-rest.properties'
      {% endif %}
    {%elif SPARK_table_format_type == 'delta' | default(false) %}
      - './conf/{{conf_dir}}/catalog/delta.properties:{{trino_etc_dir}}/catalog/delta.properties'
    {%elif SPARK_table_format_type == 'hudi' | default(false) %}
      - './conf/{{conf_dir}}/catalog/hudi.properties:{{trino_etc_dir}}/catalog/hudi.properties'
    {% endif %}
    {%if UNITY_CATALOG_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/iceberg-unity.properties:{{trino_etc_dir}}/catalog/iceberg-unity.properties'
    {% endif %}    
  {% endif %}
  {%if external['ADLS_enable'] | default(false) %}
      - './conf/{{conf_dir}}/catalog/adls.properties:{{trino_etc_dir}}/catalog/adls.properties'
  {% endif %}
  {%if KAFKA_enable or external['KAFKA_enable'] | default(false) %}
      - './conf/{{conf_dir}}/catalog/kafka.properties:{{trino_etc_dir}}/catalog/kafka.properties'
      - './conf/{{conf_dir}}/kafka/:{{trino_etc_dir}}/kafka/'
    {% if ns.secureKafkaEnabled %}
      - './security/kafka/sasl-scram/client.properties:{{trino_etc_dir}}/kafka-configuration.properties'
    {% endif %}
  {% endif %}
  {%if HADOOP_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/hdfs.properties:{{trino_etc_dir}}/catalog/hdfs.properties'
  {% endif %}
  {% if POSTGRESQL_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/postgresql.properties:{{trino_etc_dir}}/catalog/postgresql.properties'
  {% endif %}
  {% if MYSQL_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/mysql.properties:{{trino_etc_dir}}/catalog/mysql.properties'
  {% endif %}
  {% if MARIADB_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/mariadb.properties:{{trino_etc_dir}}/catalog/mariadb.properties'
  {% endif %}
  {% if SQLSERVER_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/sqlserver.properties:{{trino_etc_dir}}/catalog/sqlserver.properties'
  {% endif %}
  {% if ORACLE_EE_enable or ORACLE_XE_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/oracle.properties:{{trino_etc_dir}}/catalog/oracle.properties'
  {% endif %}
  {% if REDIS_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/redis.properties:{{trino_etc_dir}}/catalog/redis.properties'
  {% endif %}
  {% if REDIS_STACK_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/redis-stack.properties:{{trino_etc_dir}}/catalog/redis-stack.properties'
      - './conf/{{conf_dir}}/catalog/redis-search.properties:{{trino_etc_dir}}/catalog/redis-search.properties'
  {% endif %}
  {% if MONGO_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/mongo.properties:{{trino_etc_dir}}/catalog/mongo.properties'
  {% endif %}
  {% if ELASTICSEARCH_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/elasticsearch.properties:{{trino_etc_dir}}/catalog/elasticsearch.properties'
  {% endif %}
  {% if PINOT_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/pinot.properties:{{trino_etc_dir}}/catalog/pinot.properties'
  {% endif %}
  {% if DRUID_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/druid.properties:{{trino_etc_dir}}/catalog/druid.properties'
  {% endif %}
  {% if TRINO_with_tpch_catalog | default(false) %}
      - './conf/{{conf_dir}}/catalog/tpch.properties:{{trino_etc_dir}}/catalog/tpch.properties'
  {% endif %}
  {% if TRINO_with_tpcds_catalog | default(false) %}
      - './conf/{{conf_dir}}/catalog/tpcds.properties:{{trino_etc_dir}}/catalog/tpcds.properties'
  {% endif %}
  {% if TRINO_with_memory_catalog | default(false) %}
      - './conf/{{conf_dir}}/catalog/memory.properties:{{trino_etc_dir}}/catalog/memory.properties'
  {% endif %}
  {% if TRINO_event_listeners is defined and TRINO_event_listeners | length | default(false) %}
    {% for eventListener in (TRINO_event_listeners.split(',')) %}
      - ./conf/trino/{{eventListener}}.properties:{{trino_etc_dir}}/{{eventListener}}.properties
      - ./plugins/trino/event-listener/{{eventListener}}:/usr/lib/trino/plugin/{{eventListener}}
    {% endfor %}
  {% endif %}
  {% if (TRINO_edition | lower) == 'starburstdata' and TRINO_starburstdata_use_license is defined and TRINO_starburstdata_use_license | default(false) %}
      - ./licenses/starburstdata/starburstdata.license:/etc/starburst/starburstdata.license
  {% endif %}
  {% if TRINO_auth_enabled | default(false) %}
      - ./security/{{conf_dir}}/password-authenticator.properties:{{trino_etc_dir}}/password-authenticator.properties
      - ./{{'custom-conf' if TRINO_auth_use_custom_password_file else 'security' }}/{{conf_dir}}/{{'security/' if TRINO_auth_use_custom_password_file}}password.db:{{trino_etc_dir}}/password.db
      - ./{{'custom-conf' if TRINO_auth_use_custom_certs else 'security' }}/{{conf_dir}}/certs:{{trino_etc_dir}}/certs
  {% endif %}
  {% if TRINO_auth_with_groups | default(false) %}
      - ./security/{{conf_dir}}/group-provider.properties:{{trino_etc_dir}}/group-provider.properties
  {% endif %}
  {% if TRINO_access_control_enabled and ((TRINO_access_control_method | lower) == 'file' or (TRINO_access_control_method | lower) == 'opa') | default(false) %}
      - ./security/{{conf_dir}}/access-control-{{TRINO_access_control_method | lower}}.properties:{{trino_etc_dir}}/access-control.properties
  {% endif %}
  {% if TRINO_additional_catalogs is defined and TRINO_additional_catalogs | default(false) %}
    {% for catalogName in (TRINO_additional_catalogs.split(',')) %}
      - './custom-conf/{{conf_dir}}/catalog/{{catalogName}}.properties:{{trino_etc_dir}}/catalog/{{catalogName}}.properties'
    {% endfor %}
  {% endif %}
  {% if TRINO_additional_plugins is defined and TRINO_additional_plugins | default(false) %}
    {% for pluginName in (TRINO_additional_plugins.split(',')) %}
      - ./plugins/{{conf_dir}}/plugin/{{pluginName}}:/usr/lib/trino/plugin/{{pluginName}}
    {% endfor %}
  {% endif %}
      - ./custom-conf/{{conf_dir}}/security:{{trino_etc_dir}}/security
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    {% endfor %}
  {% endif -%}   {#  TRINO_install #}

  {% if TRINO_CLI_enable | default(false) %}
  trino-cli:
  {% if (TRINO_edition | lower) == 'starburstdata'  %}
    image: trivadis/trino-cli:{{__TRINO_CLI_version}}
  {% elif (TRINO_edition | lower) == 'oss' %}
    image: trivadis/trino-cli:{{__TRINO_CLI_version}}
  {% endif -%}   {#  TRINO_edition #}
    hostname: trino-cli
    container_name: trino-cli
    labels:
      com.platys.name: "trino"
      com.platys.description: "Trino CLI"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
      - ./security/{{conf_dir}}/certs:{{trino_etc_dir}}/certs
    {% endif -%}   {#  use_timezone #}
    tty: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  TRINO_CLI_enable #}
{% endif %}   {#  TRINO_enable #}

{% if PRESTO_enable | default(false) %}
  #  ================================== Presto ========================================== #
  presto-1:
  {% if (PRESTO_edition | lower) == 'prestodb' %}
    image: trivadis/prestodb:{{__PRESTO_version}}
    {% set presto_etc_dir = '/opt/presto/default/etc' %}
  {% elif (PRESTO_edition | lower) == 'ahana' %}
    image: ahanaio/prestodb:{{__AHANA_version}}
    {% set presto_etc_dir = '/opt/presto/default/etc' %}
  {% endif -%}   {#  PRESTO_edition #}
    hostname: presto-1
    container_name: presto-1
    labels:
      com.platys.name: "presto"
      com.platys.description: "SQL Virtualization Engine"
      com.platys.webui.title: "Presto UI"
      com.platys.webui.url: "http://dataplatform:28081"
    ports:
      - '28081:8080'
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
  {% if (PRESTO_install | lower) == 'cluster'  %}
      - ./conf/presto/cluster/coordinator.config.properties:{{presto_etc_dir}}/config.properties
  {% else %}
      - ./conf/presto/single/config.properties:{{presto_etc_dir}}/config.properties
  {% endif %}
      - ./conf/presto/cluster/node.properties:{{presto_etc_dir}}/node.properties
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - './conf/presto/catalog/minio.properties:{{presto_etc_dir}}/catalog/minio.properties'
  {% endif %}
  {%if HADOOP_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/hdfs.properties:{{trino_etc_dir}}/catalog/hdfs.properties'
  {% endif %}
  {% if POSTGRESQL_enable | default(false) %}
      - './conf/presto/catalog/postgresql.properties:/usr/lib/presto/etc/catalog/postgresql.properties'
  {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if (PRESTO_install | lower) == 'cluster'  %}
    {% for num in range(PRESTO_workers | default(1) ) %}
  presto-worker-{{loop.index}}:
  {% if (PRESTO_edition | lower) == 'prestodb' %}
    image: trivadis/prestodb:{{__PRESTO_version}}
    {% set presto_etc_dir = '/opt/presto/default/etc' %}
  {% elif (PRESTO_edition | lower) == 'ahana' %}
    image: ahanaio/prestodb:{{__AHANA_version}}
    {% set presto_etc_dir = '/opt/presto/default/etc' %}
  {% endif -%}   {#  PRESTO_edition #}
    hostname: presto-worker-{{loop.index}}
    container_name: presto-worker-{{loop.index}}
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/presto/cluster/worker.config.properties:{{presto_etc_dir}}/config.properties
      - ./conf/presto/cluster/node.properties:{{presto_etc_dir}}/node.properties
  {%if MINIO_enable or external['S3_enable'] | default(false) %}
      - './conf/presto/catalog/minio.properties:{{presto_etc_dir}}/catalog/minio.properties'
  {% endif %}
  {%if HADOOP_enable | default(false) %}
      - './conf/{{conf_dir}}/catalog/hdfs.properties:{{trino_etc_dir}}/catalog/hdfs.properties'
  {% endif %}
  {% if POSTGRESQL_enable | default(false) %}
      - './conf/presto/catalog/postgresql.properties:/usr/lib/presto/etc/catalog/postgresql.properties'
  {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    {% endfor %}
  {% endif -%}   {#  PRESTO_install #}

  {% if PRESTO_CLI_enable | default(false) %}
  presto-cli:
  {% if (PRESTO_edition | lower) == 'prestodb' %}
    image: ahanaio/prestodb-cli:{{__PRESTO_CLI_version}}
    {% set presto_etc_dir = '/opt/presto/default/etc' %}
  {% elif (PRESTO_edition | lower) == 'ahana' %}
    image: ahanaio/prestodb-cli:{{__PRESTO_CLI_version}}
    {% set presto_etc_dir = '/opt/presto/default/etc' %}
  {% endif -%}   {#  PRESTO_edition #}
    hostname: presto-cli
    container_name: presto-cli
    labels:
      com.platys.name: "presto"
      com.platys.description: "SQL Virtualization Engine"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    tty: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  PRESTO_CLI_enable #}

{% endif %}   {#  PRESTO_enable #}

{% if DREMIO_enable | default(false) %}
  #  ================================== Presto ========================================== #
  dremio-1:
    image: dremio/dremio-oss:{{__DREMIO_version}}
    container_name: dremio-1
    hostname: dremio-1
    labels:
      com.platys.name: "dremio"
      com.platys.description: "SQL Virtualization Engine"
      com.platys.webui.title: "Dremio UI"
      com.platys.webui.url: "http://dataplatform:9047"
    ports:
      - '9047:9047'
      - '31010:31010'
      - 32010:32010      
      - '45678:45678'
    environment:
      - DREMIO_JAVA_SERVER_EXTRA_OPTS=-Dpaths.dist=file:///opt/dremio/data/dist
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DREMIO_enable #}

{% if DRILL_enable | default(false) %}
  #  ================================== Presto ========================================== #
  drill-1:
    image: apache/drill:{{__DRILL_version}}
    container_name: drill-1
    hostname: drill-1
    labels:
      com.platys.name: "drill"
      com.platys.description: "SQL Virtualization Engine"
      com.platys.webui.title: "Drill UI"
      com.platys.webui.url: "http://dataplatform:8047"
    ports:
      - '8047:8047'
    stdin_open: true
    environment:
      - SERVICE_8047_NAME=drillbit
      - DRILL_HEAP=512M
      - DRILL_MAX_DIRECT_MEMORY=1G
      - DRILL_ZOOKEEPER_QUORUM=zookeeper-1:2181
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DREMIO_enable #}

{% if HASURA_enable | default(false) %}
  #  ================================== Hasura ========================================== #
  hasura:
    image: hasura/graphql-engine:{{__HASURA_version}}
    container_name: hasura
    hostname: hasura
    labels:
      com.platys.name: "hasura"
      com.platys.description: "GraphQL Engine"
      com.platys.webui.title: "Hasura UI"
      com.platys.webui.url: "http://dataplatform:28177"
    ports:
      - '28177:8080'
    environment:
      # postgresql database where the business data is held
      HASURA_GRAPHQL_DATABASE_URL: postgres://{{HASURA_postgres_user}}:{{HASURA_postgres_password}}@postgresql:5432/{{HASURA_postgres_database}}
      # for metadata storage
      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://{{HASURA_postgres_meta_user}}:{{HASURA_postgres_meta_password}}@postgresql:5432/{{HASURA_postgres_meta_database}}
      HASURA_GRAPHQL_ADMIN_SECRET: {{HASURA_admin_secret if HASURA_admin_secret is defined and HASURA_admin_secret else omit}}
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true" # set to "false" to disable console
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_LOG_LEVEL: {{HASURA_log_level}}
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_LIVE_QUERIES_MULTIPLEXED_REFETCH_INTERVAL: 1000
      HASURA_GRAPHQL_PRO_KEY: {{HASURA_pro_key if HASURA_pro_key is defined and HASURA_pro_key else omit}}
      ## uncomment next line to set an admin secret
      # HASURA_GRAPHQL_ADMIN_SECRET: myadminsecretkey
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  HASURA_enable #}

{% if GRAPHQL_MESH_enable | default(false) %}
  #  ================================== GraphQL Mesh ========================================== #
  graphql-mesh:
    image: hiroyukiosaki/graphql-mesh:{{__GRAPHQL_MESH_version}}
    container_name: graphql-mesh
    hostname: graphql-mesh
    labels:
      com.platys.name: "graphql-mesh"
      com.platys.description: "Federated GraphQL Engine"
      com.platys.webui.title: "GraphQL Mesh UI"
      com.platys.webui.url: "http://dataplatform:4000"
    ports:
      - '4000:4000'
    environment:
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  GRAPHQL_MESH_enable #}

{% if DIRECTUS_enable | default(false) %}
  #  ================================== Directus ========================================== #
  directus:
    image: ghcr.io/directus/directus:{{__DIRECTUS_version}}
    container_name: directus
    hostname: directus
    labels:
      com.platys.name: "directus"
      com.platys.description: "Instant REST+GraphQL API for any SQL database"
      com.platys.webui.title: "Directus UI"
      com.platys.webui.url: "http://dataplatform:8055"
    ports:
      - 8055:8055
    environment:
      KEY: '255d861b-5ea1-5996-9aa3-922530ec40b1'
      SECRET: '6116487b-cda1-52c2-b5b5-c8022c45e263'

      DB_CLIENT: 'pg'
      DB_HOST: 'postgresql'
      DB_PORT: '5432'
      DB_DATABASE: 'postgres'
      DB_USER: 'postgres'
      DB_PASSWORD: 'abc123!'

      CACHE_ENABLED: 'true'
      CACHE_STORE: 'redis'
      CACHE_REDIS: 'redis://directus-cache:6379'

      ADMIN_EMAIL: 'admin@example.com'
      ADMIN_PASSWORD: 'd1r3ctu5'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  directus-cache:
    image: redis:6
    container_name: directus-cache
    hostname: directus-cache
{% endif %}   {#  DIRECTUS_enable #}

{% if TYK_enable | default(false) %}
  #  ================================== Tyk Gateway ========================================== #
  tyk-gateway:
    image: docker.tyk.io/tyk-gateway/tyk-gateway:{{__TYK_version}}
    container_name: tyk-gateway
    hostname: tyk-gateway
    labels:
      com.platys.name: "tyk-gateway"
      com.platys.description: "API Gateway"
      com.platys.restapi.title: "Tyk API Gateway Admin API"
      com.platys.restapi.url: "http://dataplatform:28280"
    ports:
      - 28280:8080
    {%if TYK_edition == 'pro' | default(false) %}
    env_file:
      - ./conf/tyk/tyk.env
    {% endif -%}   {#  TYK_edition #}
    environment:
      - TYK_GW_LISTENPORT=8080
      - TYK_GW_SECRET={{TYK_secret}}
    {%if TYK_edition == 'pro' | default(false) %}
      - TYK_GW_NODESECRET={{TYK_secret}}
      - TYK_GW_POLICIES_POLICYSOURCE=service
      - TYK_GW_POLICIES_POLICYCONNECTIONSTRING=http://tyk-dashboard:3000
      - TYK_GW_POLICIES_POLICYRECORDNAME=tyk_policies
      - TYK_GW_POLICIES_ALLOWEXPLICITPOLICYID=true
      - TYK_GW_USEDBAPPCONFIGS=true
      - TYK_GW_DBAPPCONFOPTIONS_CONNECTIONSTRING=http://tyk-dashboard:3000
      - TYK_GW_DBAPPCONFOPTIONS_NODEISSEGMENTED=false
      - TYK_GW_DBAPPCONFOPTIONS_TAGS=
      - TYK_GW_DISABLEDASHBOARDZEROCONF=false
      - TYK_GW_STORAGE_TYPE=redis
      - TYK_GW_STORAGE_ADDRS=tyk-redis:6379
      - TYK_GW_ENABLEANALYTICS=true
      - TYK_GW_ANALYTICSCONFIG_TYPE=mongo
    {% endif -%}   {#  TYK_edition #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/tyk/tyk.standalone.conf:/opt/tyk-gateway/tyk.conf
#      - ./apps:/opt/tyk-gateway/apps
#      - ./middleware:/opt/tyk-gateway/middleware
#      - ./security/tyk/certs:/opt/tyk-gateway/certs
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  tyk-redis:
    image: redis:6.2.7-alpine
    container_name: tyk-redis
    hostname: tyk-redis
    labels:
      com.platys.name: "redis"
      com.platys.description: "Key-Value Store"
    ports:
      - 6386:6379
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if TYK_PUMP_enable | default(false) %}
  tyk-pump:
    image: tykio/tyk-pump-docker-pub:{{__TYP_PUMP_version}}
    container_name: tyk-pump
    hostname: tyk-pump
    labels:
      com.platys.name: "tyk-pump"
      com.platys.description: ""
    envionment:
      - TYK_PMP_OMITCONFIGFILE=true
      - TYK_PMP_ANALYTICSSTORAGETYPE=redis
      - TYK_PMP_ANALYTICSSTORAGECONFIG_TYPE=redis
      - TYK_PMP_ANALYTICSSTORAGECONFIG_ADDRS=tyk-redis:6379
      - TYK_PMP_ANALYTICSSTORAGECONFIG_USERNAME=
      - TYK_PMP_ANALYTICSSTORAGECONFIG_PASSWORD=
      - TYK_PMP_ANALYTICSSTORAGECONFIG_DATABASE=0
      - TYK_PMP_ANALYTICSSTORAGECONFIG_MAXIDLE=100
      - TYK_PMP_ANALYTICSSTORAGECONFIG_MAXACTIVE=100
      - TYK_PMP_ANALYTICSSTORAGECONFIG_ENABLECLUSTER=false
      - TYK_PMP_PURGEDELAY=2
      - TYK_PMP_DONTPURGEUPTIMEDATA=false
    {%if TYK_PUMP_backend_type == 'postgres' | default(false) %}
      - TYK_PMP_PUMPS_MAIN_TYPE=SQL
      - TYK_PMP_PUMPS_MAIN_META_TYPE=postgres
      - TYK_PMP_PUMPS_MAIN_META_CONNECTIONSTRING=user=default password=topsecretpassword host=tyk-postgres port=5432 database=tyk_analytics
      - TYK_PMP_PUMPS_MAINAGG_TYPE=sql_aggregate
      - TYK_PMP_PUMPS_MAINAGG_META_TYPE=postgres
      - TYK_PMP_PUMPS_MAINAGG_META_CONNECTIONSTRING=user=default password=topsecretpassword host=tyk-postgres port=5432 database=tyk_analytics
      - TYK_PMP_UPTIMEPUMPCONFIG_UPTIMETYPE=sql
      - TYK_PMP_UPTIMEPUMPCONFIG_TYPE=postgres
      - TYK_PMP_UPTIMEPUMPCONFIG_CONNECTIONSTRING=user=default password=topsecretpassword host=tyk-postgres port=5432 database=tyk_analytics
    {% endif -%}   {#  use_timezone #}
    {%if TYK_PUMP_backend_type == 'mongo' | default(false) %}
      - TYK_PMP_PUMPS_MAIN_TYPE=mongo
      - TYK_PMP_PUMPS_MAIN_META_COLLECTIONNAME=tyk_analytics
      - TYK_PMP_PUMPS_MAIN_META_MONGOURL=mongodb://tyk-mongo:27017/tyk_analytics
      - TYK_PMP_PUMPS_MAIN_META_MAXINSERTBATCHSIZEBYTES=80000
      - TYK_PMP_PUMPS_MAIN_META_MAXDOCUMENTSIZEBYTES=20112
      - TYK_PMP_PUMPS_MAINAGG_TYPE=mongo-pump-aggregate
      - TYK_PMP_PUMPS_MAINAGG_META_MONGOURL=mongodb://tyk-mongo:27017/tyk_analytics
      - TYK_PMP_PUMPS_MAINAGG_META_USEMIXEDCOLLECTION=true
      - TYK_PMP_UPTIMEPUMPCONFIG_COLLECTIONNAME=tyk_uptime_analytics
      - TYK_PMP_UPTIMEPUMPCONFIG_MONGOURL=mongodb://tyk-mongo:27017/tyk_analytics
      - TYK_PMP_UPTIMEPUMPCONFIG_MAXINSERTBATCHSIZEBYTES=500000
      - TYK_PMP_UPTIMEPUMPCONFIG_MAXDOCUMENTSIZEBYTES=200000
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./confs/pump.env
      - ./confs/pump.mongo.env
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  TYK_PUMP_enable #}

  {%if TYK_edition == 'pro' | default(false) %}
  tyk-dashboard:
    image: tykio/tyk-dashboard:{{__TYK_version}}
    container_name: tyk-dashboard
    hostname: tyk-dashboard
    labels:
      com.platys.name: "tyk-dashboard"
      com.platys.description: "API Management UI"
      com.platys.webui.title: "Tyk Dashboard UI"
      com.platys.webui.url: "http://dataplatform:28281"
    ports:
      - "28281:3000"
    environment:
      - TYK_DB_LICENSEKEY=${TYK_DB_LICENSEKEY}
      - TYK_DB_STORAGE_MAIN_TYPE=mongo
      - TYK_DB_STORAGE_MAIN_CONNECTIONSTRING=mongodb://tyk-mongo:27017/tyk_analytics
    env_file:
      - ./conf/tyk/tyk_analytics.env
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  tyk-mongo:
    image: mongo:4.0
    container_name: tyk-mongo
    hostname: tyk-mongo
    labels:
      com.platys.name: "mongo"
      com.platys.decription: "Document NoSQL Datastore"
    ports:
      - "27020:27017"
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: ["mongod", "--smallfiles"]
    restart: {{container_restart_policy}}
  {% endif %}   {#  TYK_edition == 'pro' #}

{% endif %}   {#  TYK_enable #}

{% if KONG_enable | default(false) %}
  #  ================================== Kong Gateway ========================================== #
  {% for num in range(KONG_nodes| default(1) ) %}
  kong-gateway-{{loop.index}}:
    image: kong/kong-gateway:{{__KONG_version}}
    container_name: kong-gateway-{{loop.index}}
    hostname: kong-gateway-{{loop.index}}
    labels:
      com.platys.name: "kong-gateway"
      com.platys.description: "API Gateway"
    {% if loop.index == 1 %}
      com.platys.webui.title: "Kong API Gateway Admin GUI"
      com.platys.webui.url: "http://dataplatform:8002"
      com.platys.restapi.title: "Kong API Gateway Admin API"
      com.platys.restapi.url: "http://dataplatform:8001"
    {% endif -%}  {# loop.index == 1 #}
#    user: "${KONG_USER:-kong}"
    {% if loop.index == 1 %}
    ports:
      # The following two environment variables default to an insecure value (0.0.0.0)
      # according to the CIS Security test.
      - 8000:8000
      - 8443:8443
      # Making them mandatory but undefined, like so would be backwards-breaking:
      # - "${KONG_INBOUND_PROXY_LISTEN?Missing inbound proxy host}:8000:8000/tcp"
      # - "${KONG_INBOUND_SSL_PROXY_LISTEN?Missing inbound proxy ssl host}:8443:8443/tcp"
      # Alternative is deactivating check 5.13 in the security bench, if we consider Kong's own config to be enough security here
      - 8001:8001
      - 8444:8444
      - 8002:8002
    {% endif -%}  {# loop.index == 1 #}
    environment:
      - KONG_LOG_LEVEL={{KONG_log_level | default('info')}}
      - KONG_PLUGINS={{KONG_plugins}}
  {%if not KONG_use_db | default(false) %}
      - KONG_DATABASE=off
  {% else -%}   {#  KONG_db_type = 'postgres' #}
    {%if KONG_db_type == 'postgres' | default(false) %}
      - KONG_DATABASE=postgres
      - KONG_PG_DATABASE=kongdb
      - KONG_PG_HOST=kong-db
      - KONG_PG_USER=kong
      - KONG_PG_PASSWORD=abc123!
#      - KONG_PG_PASSWORD_FILE= /run/secrets/kong_postgres_password
    {% endif -%}   {#  KONG_db_type = 'postgres' #}
    {%if KONG_db_type == 'cassandra' | default(false) %}
      - KONG_DATABASE=cassandra
      - KONG_CASSANDRA_CONTACT_POINTS=cassandra-1
    {% endif -%}   {#  KONG_db_type = 'cassandra' #}
  {% endif -%}   {# KONG_use #}
      - KONG_DECLARATIVE_CONFIG={{'/kong' if KONG_use_declarative_config else omit}}
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_PROXY_ERROR_LOG=/dev/stderr
    {% if loop.index == 1 %}
      - KONG_PROXY_LISTEN=0.0.0.0:8000
      - KONG_ADMIN_LISTEN=0.0.0.0:8001
      - KONG_ADMIN_GUI_LISTEN=0.0.0.0:8002
      - KONG_ADMIN_GUI_URL=http://${PUBLIC_IP}:8002
      - KONG_ADMIN_API_URI=http://${PUBLIC_IP}:8001
    {% endif -%}  {# loop.index == 1 #}
    {% if KONG_volume_map_working %}
      - KONG_PREFIX=/var/run/kong
    {% endif -%}   {#  KONG_volume_map_working #}
      - KONG_LICENSE_DATA={{KONG_license_data}}
      #- KONG_ENFORCE_RBAC=
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if KONG_volume_map_working %}
      - ./container-volume/kong:/var/run/kong
    {% endif -%}   {#  KONG_volume_map_working #}
#      - kong_tmp_vol:/tmp
    {%if KONG_use_declarative_config | default(false) %}
      - ./custom-conf/kong:/kong
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
#    secrets:
#      - kong_postgres_password
    security_opt:
      - no-new-privileges
    restart: {{container_restart_policy}}
  {% endfor %}

  {% if KONG_db_type == 'postgres' | default(false) %}
  kong-db:
    image: postgres:9.5
    container_name: kong-db
    hostname: kong-dbp
    labels:
      com.platys.name: "postgresql"
      com.platys.description: "Relational Database"
    environment:
      - POSTGRES_DB=kongdb
      - POSTGRES_USER=kong
#      - POSTGRES_PASSWORD_FILE=/run/secrets/kong_postgres_password
      - POSTGRES_PASSWORD=abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if KONG_volume_map_data %}
      - ./container-volume/kong/data:/var/lib/postgresql/data
    {% endif -%}   {#  KONG_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
#    secrets:
#      - kong_postgres_password
    stdin_open: true
    tty: true
    restart: {{container_restart_policy}}
  {% endif -%}   {#  KONG_db_type == 'postgres' #}

  {% if KONG_use_db| default(false) %}
  kong-migrations:
    image: kong:{{__KONG_version}}
    container_name: kong-migrations
    hostname: kong-migrations
    labels:
      com.platys.name: "kong"
      com.platys.description: ""
    command: kong migrations bootstrap
    environment:
    {%if KONG_db_type == 'postgres' | default(false) %}
      - KONG_DATABASE=postgres
      - KONG_PG_DATABASE=kongdb
      - KONG_PG_HOST=kong-db
      - KONG_PG_USER=kong
#      - KONG_PG_PASSWORD_FILE= /run/secrets/kong_postgres_password
      - KONG_PG_PASSWORD=abc123!
    {% endif -%}   {#  KONG_db_type = 'postgres' #}
    {%if KONG_db_type == 'cassandra' | default(false) %}
      - KONG_DATABASE=cassandra
      - KONG_CASSANDRA_CONTACT_POINTS=cassandra-1
    {% endif -%}   {#  KONG_db_type = 'cassandra' #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
#    secrets:
#      - kong_postgres_password
    restart: on-failure

  kong-migrations-up:
    image: kong:{{__KONG_version}}
    container_name: kong-migrations-up
    hostname: kong-migrations-up
    labels:
      com.platys.name: "kong"
      com.platys.description: ""
    command: kong migrations up && kong migrations finish
    environment:
    {%if KONG_db_type == 'postgres' | default(false) %}
      - KONG_DATABASE=postgres
      - KONG_PG_DATABASE=kongdb
      - KONG_PG_HOST=kong-db
      - KONG_PG_USER=kong
#      - KONG_PG_PASSWORD_FILE= /run/secrets/kong_postgres_password
      - KONG_PG_PASSWORD=abc123!
    {% endif -%}   {#  KONG_db_type = 'postgres' #}
    {%if KONG_db_type == 'cassandra' | default(false) %}
      - KONG_DATABASE=cassandra
      - KONG_CASSANDRA_CONTACT_POINTS=cassandra-1
    {% endif -%}   {#  KONG_db_type = 'cassandra' #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
#    secrets:
#      - kong_postgres_password
    restart: on-failure
  {% endif -%}   {#  KONG_use_db #}

  {% if KONG_DECK_enable | default(false) %}
  #  ================================== Kong decK  ========================================== #
  kong-deck:
    image: kong/deck:{{__KONG_DECK_version}}
    container_name: kong-deck
    hostname: kong-deck
    labels:
      com.platys.name: "kong-deck"
      com.platys.description: "Configuration management and drift detection for Kong"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {# KONG_DECK_enable #}

  {% if KONGA_enable | default(false) %}
  #  ================================== Konga  ========================================== #
  konga:
    image: pantsel/konga:{{__KONGA_version}}
    container_name: konga
    hostname: konga
    labels:
      com.platys.name: "konga"
      com.platys.description: "Kong Admin GUI"
      com.platys.webui.title: "Konga Admin GUI"
      com.platys.webui.url: "http://dataplatform:1337"
    ports:
      - "1337:1337/tcp"
    environment:
      NODE_ENV: production
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if KONGA_volume_map_data %}
      - "./container-volume/konga:/app/kongadata"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    restart: {{container_restart_policy}}
  {% endif %}   {# KONGA_enable #}

  {% if KONG_ADMIN_UI_enable | default(false) %}
  #  ================================== Kong Admin UI ========================================== #
  kong-admin-ui:
    image: pocketdigi/kong-admin-ui:{{__KONG_ADMIN_UI_version}}
    container_name: kong-admin-ui
    hostname: kong-admin-ui
    labels:
      com.platys.name: "kong-admin-ui"
      com.platys.description: "Kong Admin GUI"
      com.platys.webui.title: "Kong Admin UI"
      com.platys.webui.url: "http://dataplatform:28286"
    ports:
      - "28286:80"
    environment:
      - KONG_DATABASE={{KONG_db_type}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    restart: {{container_restart_policy}}
  {% endif %}   {# KONG_ADMIN_UI_enable #}

  {% if KONG_MAP_enable | default(false) %}
  #  ================================== KongMap  ========================================== #
  kong-map:
    image: yesinteractive/kongmap:{{__KONG_MAP_version}}
    container_name: kong-map
    hostname: kong-map
    labels:
      com.platys.name: "kong-map"
      com.platys.description: "Visualization of Kong API Gateway Clusters"
      com.platys.webui.title: "Kong Map GUI"
      com.platys.webui.url: "http://dataplatform:8100"
    ports:
      - "8100:8100"
    environment:
      KONGMAP_CLUSTERS_JSON: '{  "my enterprise cluster": {    "kong_admin_api_url": "http://kong-gateway-1:8001",    "kong_edit_config": "true",   "kong_ent": "true",    "kong_ent_token": "admin",    "kong_ent_token_name": "kong-admin-token",    "kong_ent_manager_url": "http://kong-gateway-1:8002"  }}'
      KONGMAP_URL: http://${PUBLIC_IP}:8100
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    restart: {{container_restart_policy}}
  {% endif %}   {# KONG_MAP_enable #}
{% endif %}   {#  KONG_enable #}

{% if CURITY_enable | default(false) %}
  #  ================================== Curity Identity Server ========================================== #
  curity:
    image: curity.azurecr.io/curity/idsvr:{{__CURITY_version}}
    container_name: curity
    hostname: curity
    labels:
      com.platys.name: "curity"
      com.platys.description: "Identity Server"
      com.platys.webui.title: "Curity UI"
      com.platys.webui.url: "https://dataplatform:6749/admin"
    ports:
      - 6749:6749
      - 4466:4466
      - 8446:8443
    environment:
      - PASSWORD={{CURITY_password}}
      - LOGGING_LEVEL={{CURITY_logging_level}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if CURITY_volume_map_license_file | default(false) %}
      - ./licenses/curity/license.json:/opt/idsvr/etc/init/license/license.json
    {% endif -%}   {#  CURRTIY_volume_map_license_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CURITY_enable #}

{% if NUCLIO_enable | default(false) %}
  #  ================================== Nuclio Dashboard ========================================== #
  nuclio:
    image: quay.io/nuclio/dashboard:{{__NUCLIO_version}}
    container_name: nuclio
    hostname: nuclio
    labels:
      com.platys.name: "nuclio"
      com.platys.description: "Serverless event and data processing platform"
      com.platys.webui.title: "Nuclio Dashboard UI"
      com.platys.webui.url: "http://dataplatform:8070"
    ports:
      - '8070:8070'
    environment:
      - NUCLIO_CHECK_FUNCTION_CONTAINERS_HEALTHINESS=true
      - NUCLIO_DASHBOARD_DEFAULT_FUNCTION_MOUNT_MODE=volume
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if NUCLIO_map_tmp_folder | default(false) %}
      - /tmp:/tmp
    {% endif -%}   {#  NUCLIO_map_tmp_folder #}
      - /var/run/docker.sock:/var/run/docker.sock
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  NUCLIO_enable #}

{% if MINIO_enable | default(false) %}
  #  ================================== Minio ========================================== #
  {% for num in range(MINIO_nodes | default(1) ) %}
    {% set port = 9000 + loop.index - 1 %}
    {% set uiport = 9010 + loop.index - 1 %}
    {% set serviceName = "minio-" ~ loop.index %}
  {{serviceName}}:
    image: minio/minio:{{__MINIO_version}}
    container_name: {{serviceName}}
    hostname: {{serviceName}}
    labels:
      com.platys.name: "minio"
      com.platys.description: "Software-defined Object Storage"
      com.platys.webui.title: "MinIO UI"
      com.platys.webui.url: "http://dataplatform:{{uiport}}"
      com.platys.password.envvars: "PLATYS_AWS_SECRET_ACCESS_KEY"
    ports:
      # S3 API Port
      - '{{port}}:9000'
      # UI Port
      - '{{uiport}}:{{uiport}}'
    environment:
      MINIO_ROOT_USER: {{s3AccessKey}}
      MINIO_ROOT_PASSWORD: {{s3SecretAccessKey}}
      # remove region due to problems with RisingWave
      #MINIO_REGION_NAME: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      #MINIO_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      MINIO_DOMAIN: minio
      MINIO_SERVER_URL: "http://${PUBLIC_IP}:9000"
      MINIO_COMPRESSION_ENABLE: "off"
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: "http://prometheus-1:9090"
    {% if MINIO_key_encryption_service_enabled | default(false) %}
      MINIO_KMS_KES_ENDPOINT: "https://minio-kes:7373"
      MINIO_KMS_KES_KEY_FILE: /root/.minio/certs/private.key
      MINIO_KMS_KES_CERT_FILE: /root/.minio/certs/public.crt
      MINIO_KMS_KES_KEY_NAME: my-minio-key
    {% endif -%}   {#  MINIO_key_encryption_service_enabled #}
    {% if MINIO_policy_plugin_enabled and MINIO_policy_type | lower == 'opa' | default(false) %}
      MINIO_POLICY_PLUGIN_URL: "http://opa:8181/v1/data/{{MINIO_policy_namespace}}allow"
      MINIO_POLICY_PLUGIN_AUTH_TOKEN: {{"Bearer "  ~ MINIO_policy_token if MINIO_policy_token is defined and MINIO_policy_token and MINIO_policy_token | length else omit}}
      MINIO_POLICY_PLUGIN_ENABLE_HTTP2: "OFF"
      MINIO_POLICY_PLUGIN_COMMENT: "External Access Management using OPA"
    {% endif -%}   {#  MINIO_policy_plugin_enabled #}
    {% if MINIO_audit_webhook_enable | default(false) %}
      MINIO_AUDIT_WEBHOOK_ENABLE_primary: 'on'
      MINIO_AUDIT_WEBHOOK_ENDPOINT_primary: {{MINIO_audit_webhook_endpoint}}
      MINIO_AUDIT_WEBHOOK_AUTH_TOKEN_primary: {{MINIO_audit_webhook_auth_token if MINIO_audit_webhook_auth_token is defined and MINIO_audit_webhook_auth_token and MINIO_audit_webhook_auth_token | length else omit}}
    {% endif -%}   {#  MINIO_audit_webhook_enable #}
    {% if (KAFKA_enable or external['KAFKA_enable']) and MINIO_audit_kafka_enable | default(false) %}
      MINIO_AUDIT_KAFKA_ENABLE: 'on'
      MINIO_AUDIT_KAFKA_BROKERS: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      MINIO_AUDIT_KAFKA_SASL: 'on'
      MINIO_AUDIT_KAFKA_SASL_MECHANISM: '{{ns.saslMechanismBROKER}}'
      MINIO_AUDIT_KAFKA_SASL_USERNAME: '{{KAFKA_RESTPROXY_kafka_username if KAFKA_RESTPROXY_kafka_username is defined and KAFKA_RESTPROXY_kafka_username | length else ns.kafkaToolsDefaultUsername}}'
      MINIO_AUDIT_KAFKA_SASL_PASSWORD: '{{KAFKA_RESTPROXY_kafka_password if KAFKA_RESTPROXY_kafka_password is defined and KAFKA_RESTPROXY_kafka_password | length else ns.kafkaToolsDefaultPassword}}'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      MINIO_AUDIT_KAFKA_TLS: 'on'
      MINIO_AUDIT_KAFKA_TLS_SKIP_VERIFY: "off"
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      MINIO_AUDIT_KAFKA_TLS_CLIENT_AUTH: 1
      MINIO_AUDIT_KAFKA_CLIENT_TLS_CERT: /certs/client.pem
      MINIO_AUDIT_KAFKA_CLIENT_TLS_KEY: /certs/client.key
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
      MINIO_AUDIT_KAFKA_TOPIC: '{{MINIO_audit_kafka_topic}}'
    {% endif -%}   {#  MINIO_audit_kafka_enable #}
      MINIO_API_SYNC_EVENTS: {{'on' if MINIO_notify_snyc_enable else omit}}
    {% if MINIO_notify_webhook_enable | default(false) %}
      MINIO_NOTIFY_WEBHOOK_ENABLE_primary: 'on'
      MINIO_NOTIFY_WEBHOOK_ENDPOINT_primary: {{MINIO_notify_webhook_endpoint}}
      MINIO_NOTIFY_WEBHOOK_AUTH_TOKEN_primary: {{MINIO_notify_webhook_auth_token if MINIO_notify_webhook_auth_token is defined and MINIO_notify_webhook_auth_token and MINIO_notify_webhook_auth_token | length else omit}}
    {% endif -%}   {#  MINIO_notify_webhook_enable #}
    {% if (KAFKA_enable or external['KAFKA_enable']) and MINIO_notify_kafka_enable | default(false) %}
      MINIO_NOTIFY_KAFKA_ENABLE_primary: 'on'
      MINIO_NOTIFY_KAFKA_BROKERS_primary: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      MINIO_NOTIFY_KAFKA_SASL_primary: 'on'
      MINIO_NOTIFY_KAFKA_SASL_MECHANISM_primary: '{{ns.saslMechanismBROKER}}'
      MINIO_NOTIFY_KAFKA_SASL_USERNAME_primary: '{{KAFKA_RESTPROXY_kafka_username if KAFKA_RESTPROXY_kafka_username is defined and KAFKA_RESTPROXY_kafka_username | length else ns.kafkaToolsDefaultUsername}}'
      MINIO_NOTIFY_KAFKA_SASL_PASSWORD_primary: '{{KAFKA_RESTPROXY_kafka_password if KAFKA_RESTPROXY_kafka_password is defined and KAFKA_RESTPROXY_kafka_password | length else ns.kafkaToolsDefaultPassword}}'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      MINIO_NOTIFY_KAFKA_TLS_primary: 'on'
      MINIO_NOTIFY_KAFKA_TLS_SKIP_VERIFY_primary: "off"
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      MINIO_NOTIFY_KAFKA_TLS_CLIENT_AUTH_primary: 1
      MINIO_NOTIFY_KAFKA_CLIENT_TLS_CERT_primary: /certs/client.pem
      MINIO_NOTIFY_KAFKA_CLIENT_TLS_KEY_primary: /certs/client.key
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
      MINIO_NOTIFY_KAFKA_TOPIC_primary: '{{MINIO_notify_kafka_topic}}'
    {% endif -%}   {#  MINIO_notify_kafka_enable #}
    {% if MINIO_notify_mqtt_enable | default(false) %}
      MINIO_NOTIFY_MQTT_ENABLE_primary: 'on'
      MINIO_NOTIFY_MQTT_BROKERS_primary: '{{MINIO_notify_mqtt_broker_endpoint}}'
      MINIO_NOTIFY_MQTT_USERNAME: '{{MINIO_notify_mqtt_username if MINIO_notify_mqtt_username is defined and MINIO_notify_mqtt_username and MINIO_notify_mqtt_username | length else omit}}'
      MINIO_NOTIFY_MQTT_PASSWORD: '{{MINIO_notify_mqtt_password if MINIO_notify_mqtt_password is defined and MINIO_notify_mqtt_password and MINIO_notify_mqtt_password | length else omit}}'
      MINIO_NOTIFY_MQTT_TOPIC: '{{MINIO_notify_mqtt_topic}}'
      MINIO_NOTIFY_MQTT_QOS: 0
    {% endif -%}   {#  MINIO_notify_kafka_enable #}
    {% if MINIO_notify_redis_enable | default(false) %}
      MINIO_NOTIFY_REDIS_ENABLE_primary: 'on'
      MINIO_NOTIFY_REDIS_ADDRESS_primary: '{{MINIO_notify_redis_endpoint}}'
      MINIO_NOTIFY_REDIS_KEY: '{{MINIO_notify_redis_key}}'
      MINIO_NOTIFY_REDIS_FORMAT: '{{MINIO_notify_redis_format}}'
      MINIO_NOTIFY_MQTT_PASSWORD: '{{MINIO_notify_redis_password if MINIO_notify_redis_password is defined and MINIO_notify_redis_password and MINIO_notify_redis_password | length else omit}}'
    {% endif -%}   {#  MINIO_notify_redis_enable #}
    {% if MINIO_lambda_webhook_enable | default(false) %}
      MINIO_LAMBDA_WEBHOOK_ENABLE_primary: 'on'
      MINIO_LAMBDA_WEBHOOK_ENDPOINT_primary: {{MINIO_lambda_webhook_endpoint}}
      MINIO_LAMBDA_WEBHOOK_AUTH_TOKEN_primary: {{MINIO_lambda_webhook_auth_token if MINIO_lambda_webhook_auth_token is defined and MINIO_lambda_webhook_auth_token and MINIO_lambda_webhook_auth_token | length else omit}}
    {% endif -%}   {#  MINIO_lambda_webhook_enable #}

    {%if not MINIO_browser_enable | default(false) %}
      MINIO_BROWSER: "off"
    {% endif -%}   {#  MINIO_browser_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MINIO_volume_map_data %}
      - "./container-volume/minio:/data/"
    {% endif %}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%}   {# ns.secureKafkaEnabled #}
    {% if MINIO_key_encryption_service_enable | default(false) %}
      - ./security/minio/certs:/root/.minio/certs
    {% endif -%} {# MINIO_key_encryption_service_enable #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {% if MINIO_volume_map_data %}
    user: "{{uid | default(1000)}}"
    {% endif -%}   {#  MINIO_volume_map_data #}
    command: server /data --console-address ":{{uiport}}"
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://minio-1:9000/minio/health/live"]
      interval: 15s
      timeout: 20s
      retries: 3
  {% endfor %}

  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:{{__MINIO_MC_version}}
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: "minio"
      com.platys.description: "MinIO Console"
    environment:
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
      MC_HOST_minio-1: http://{{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}:{{s3SecretAccessKey}}@minio-1:9000    
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - ./security/aws/credentials:/tmp/credentials.templ
      - aws-credentials-vol:/tmp/.aws:RO
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 minio-1:9000
        mkdir -p /tmp/.aws
        eval "echo \"$$(cat /tmp/credentials.templ)\"" >> /tmp/.aws/credentials
        mc mb --ignore-existing minio-1/{{s3AdminBucketName}}
    {% if MINIO_upload_data_transfer_to_bucket_enabled | default(false) %}
        mc mb --ignore-existing minio-1/{{MINIO_upload_data_transfer_bucket_name | default("data-transfer-bucket")}};

      {% if MINIO_upload_data_transfer_subfolders is defined and MINIO_upload_data_transfer_subfolders and MINIO_upload_data_transfer_subfolders | length | default(false) %}
        for subfolder in $$(echo '{{MINIO_upload_data_transfer_subfolders}}' | tr ',' '\n'); do
          mc cp --recursive /data-transfer/$$subfolder minio-1/{{MINIO_upload_data_transfer_bucket_name | default("data-transfer-bucket")}}/; 
        done
      {% else %}
          mc cp --recursive /data-transfer/ minio-1/{{MINIO_upload_data_transfer_bucket_name | default("data-transfer-bucket")}}/; 
      {% endif %}   {#  MINIO_upload_data_transfer_subfolders #}
    {% endif %}   {#  MINIO_provide_data_transfer_as_bucket #}
    {% if MLFLOW_SERVER_enable | default(false) %}
        mc mb --ignore-existing minio-1/mlruns
    {% endif %}   {#  MLFLOW_SERVER_enable #}
        for bucket in $$(tr ',' '\n' <<< "{{MINIO_buckets}}")
        do
          mc mb --ignore-existing minio-1/$$bucket
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MINIO_enable #}

{% if MINIO_CONSOLE_enable | default(false) %}
  #  ================================== Minio Console ========================================== #
  minio-console:
    image: minio/console:{{__MINIO_CONSOLE_version}}
    container_name: minio-console
    hostname: minio-console
    labels:
      com.platys.name: "minio-console"
      com.platys.description: "MinIO Console"
      com.platys.webui.title: "Minio Console UI"
      com.platys.webui.url: "http://dataplatform:28193"
    ports:
      - "28193:9090"
    environment:
      CONSOLE_MINIO_SERVER: "{{s3EndpointStorage if s3EndpointStorage is defined and s3EndpointStorage else omit}}"
      CONSOLE_PBKDF_PASSPHRASE: abc123!
      CONSOLE_PBKDF_SALT: abc123!
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: server
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MINIO_CONSOLE_enable #}

{% if ADMINIO_UI_enable | default(false) %}
  #  ================================== Adminio UI ========================================== #
  adminio-ui:
    image: rzrbld/adminio-ui:{{__ADMINIO_UI_version}}
    container_name: adminio-ui
    hostname: adminio-ui
    labels:
      com.platys.name: "adminio"
      com.platys.description: "MinIO Admin UI"
      com.platys.webui.title: "Adminio UI"
      com.platys.webui.url: "http://dataplatform:28190"
    ports:
      - "28190:80"
    environment:
      API_BASE_URL: "http://${PUBLIC_IP}:28191"
      ADMINIO_MULTI_BACKEND: "false"
      ADMINIO_BACKENDS: '[{"name":"myminio","url":"http://${PUBLIC_IP}:28191"}]'
      NGX_ROOT_PATH: "/"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  adminio-api:
    image: rzrbld/adminio-api:{{__ADMINIO_API_version}}
    container_name: adminio-api
    hostname: adminio-api
    labels:
      com.platys.name: "adminio"
      com.platys.description: "MinIO Admin UI"
      com.platys.restapi.title: "Adminio API"
      com.platys.restapi.url: "http://dataplatform:28191"
    ports:
      - "28191:8080"
    environment:
      MINIO_ACCESS: {{MINIO_access_key}}
      MINIO_SECRET: {{MINIO_secret_key}}
      MINIO_HOST_PORT: {{s3EndpointStorage if s3EndpointStorage is defined and s3EndpointStorage else omit}}
      #MINIO_SSE_MASTER_KEY: 1:da2f4cfa32bed76507dcd44b42872328a8e14f25cd2a1ec0fb85d299a192a447
      ADMINIO_HOST_PORT: ":8080"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ADMINIO_UI_enable #}

{% if MINIO_WEB_enable | default(false) %}
  #  ================================== Minio Web ========================================== #
  minio-web:
    image: e2fyi/minio-web:{{__MINIO_WEB_version}}
    container_name: minio-web
    hostname: minio-web
    labels:
      com.platys.name: "minio-web"
      com.platys.description: "MinIO Web UI"
      com.platys.webui.title: "Minio Web"
      com.platys.webui.url: "http://dataplatform:28306"
    ports:
      - 28306:8080
    environment:
      MINIO_ENDPOINT: {{ (s3Endpoint | replace('http://','')) if s3Endpoint is defined and s3Endpoint else omit}}
      MINIO_ACCESSKEY: {{s3AccessKey}}
      MINIO_SECRETKEY: {{s3SecretAccessKey}}
      MINIO_SECURE: false
      MINIO_REGION: {{s3DefaultRegion}}
      EXT_BUCKETNAME: {{MINIO_WEB_s3_bucket_name}}
      EXT_PREFIX: {{MINIO_WEB_s3_prefix}}
      EXT_DEFAULTHTML: {{MINIO_WEB_default_html}}
      EXT_FAVICON: 'assets/{{MINIO_WEB_favicon if MINIO_WEB_favicon is defined and MINIO_WEB_favicon and MINIO_WEB_favicon | length else omit }}'
      EXT_MARKDOWNTEMPLATE: 'assets/{{MINIO_WEB_md_template if MINIO_WEB_md_template is defined and MINIO_WEB_md_template and MINIO_WEB_md_template | length else omit }}'
      EXT_LISTFOLDER: true
      EXT_LISTFOLDEROBJECTS: "*.{md,html,jpg,jpeg,png,txt}"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MINIO_WEB_favicon is defined and MINIO_WEB_favicon and MINIO_WEB_favicon | length %}
      - ./custom-conf/minio-web/{{MINIO_WEB_favicon}}:/goapp/assets/{{MINIO_WEB_favicon}}
    {% endif -%}
    {% if MINIO_WEB_md_template is defined and MINIO_WEB_md_template and MINIO_WEB_md_template | length %}
      - ./custom-conf/minio-web/{{MINIO_WEB_md_template}}:/goapp/assets/{{MINIO_WEB_md_template}}
    {% endif -%}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MINIO_WEB_enable #}

{% if MINIO_KES_enable | default(false) %}
  #  ================================== Minio KES ========================================== #
  minio-kes:
    image: minio/kes:{{__MINIO_KES_version}}
    container_name: minio-kes
    hostname: minio-kes
    labels:
      com.platys.name: "minio-kes"
      com.platys.description: "Key Management Server"
      com.platys.webui.title: "Minio KES"
      com.platys.webui.url: "http://dataplatform:7373"
    ports:
      - 7373:7373
    environment:
      KES_SERVER: "https://minio-kes:7373"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/minio-kes:/etc/kes
      - ./security/minio/certs:/etc/kes/certs
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: bin/sh -c 'cat /etc/kes/certs/public.crt >>/etc/pki/tls/certs/ca-bundle.crt && /kes server --config /etc/kes/server-config.yaml'
    command: ''
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MINIO_KES_enable #}

{% if ICEBERG_REST_CATALOG_enable | default(false) %}
  #  ================================== Iceberg REST Catalog ========================================== #
  iceberg-rest-catalog:
    image: tabulario/iceberg-rest:{{__ICEBERG_REST_CATALOG_version}}
    container_name: iceberg-rest-catalog
    hostname: iceberg-rest-catalog
    labels:
      com.platys.name: "iceberg-rest-catalog"
      com.platys.description: "Iceberg Rest Catalog"
      com.platys.restapi.title: "Iceberg REST Catalog"
      com.platys.restapi.url: "http://dataplatform:28287/v1/namespaces"
    ports:
      - 28287:8181
    environment:
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      AWS_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      CATALOG_S3_ENDPOINT: {{ (s3Endpoint | replace('http://','')) if s3Endpoint is defined and s3Endpoint else omit}}
      CATALOG_WAREHOUSE: {{ICEBERG_REST_CATALOG_warehouse_dir if ICEBERG_REST_CATALOG_warehouse_dir is defined and ICEBERG_REST_CATALOG_warehouse_dir | length else 's3a://' ~ s3AdminBucketName ~ '/iceberg/warehouse'}}
    {%if (ICEBERG_REST_CATALOG_type | lower) =='jdbc'  | default(false) %}
      CATALOG_CATALOG__IMPL: org.apache.iceberg.jdbc.JdbcCatalog
      CATALOG_URI: jdbc:sqlite:file:/tmp/iceberg_rest_mode=memory
      CATALOG_JDBC_USERNAME: user
      CATALOG_JDBC_PASSWORD: password
    {%elif (ICEBERG_REST_CATALOG_type | lower) =='hive'  | default(false) %}
      CATALOG_CATALOG__IMPL: org.apache.iceberg.hive.HiveCatalog      #
      CATALOG_URI: jdbc:sqlite:file:/tmp/iceberg_rest_mode=memory
    {%elif (ICEBERG_REST_CATALOG_type | lower) =='nessie'  | default(false) %}
      CATALOG_CATALOG__IMPL: org.apache.iceberg.nessie.NessieCatalog
      CATALOG_URI: http://nessie:19120/api/v1
      CATALOG_REF: main
    {% endif -%}   {#  ICEBERG_REST_CATALOG_type #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ICEBERG_REST_CATALOG_enable #}

{% if FILESTASH_enable | default(false) %}
  #  ================================== Filestash ========================================== #
  filestash:
    image: machines/filestash:{{__FILESTASH_version}}
    platform: linux/amd64    
    container_name: filestash
    hostname: filestash
    labels:
      com.platys.name: "filestash"
      com.platys.description: "Dropbox-like file manager"
      com.platys.webui.title: "Filestash UI"
      com.platys.webui.url: "http://dataplatform:28192"
    ports:
      - 28192:8334
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if FILESTASH_set_default_config %}
      - ./conf/filestash/config.json:/app/data/state/config/config.json
    {% endif -%}   {#  FILESTASH_set_default_config #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  FILESTASH_enable #}

{% if S3MANAGER_enable | default(false) %}
  #  ================================== S3 Manager ========================================== #
  s3manager:
    image: mastertinner/s3manager:{{__S3MANAGER_version}}
    container_name: s3manager
    hostname: s3manager
    labels:
      com.platys.name: "s3manager"
      com.platys.description: "S3 UI"
      com.platys.webui.title: "S3 Manager UI"
      com.platys.webui.url: "http://dataplatform:28176"
    ports:
      - 28176:8080
    environment:
      ENDPOINT: {{ (s3Endpoint | replace('http://','')) if s3Endpoint is defined and s3Endpoint else omit}}
      ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      USE_SSL: 'false'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  S3MANAGER_enable #}

{% if AWSCLI_enable | default(false) %}
  #  ================================== Awscli ========================================== #
  awscli:
    image: trivadis/awscli-s3cmd:{{__AWSCLI_version}}
    container_name: awscli
    hostname: awscli
    labels:
      com.platys.name: "awscli"
      com.platys.description: "AWS CLI"
    environment:
      AWS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      AWS_ENDPOINT: {{ (s3Endpoint | replace('http://','')) if s3Endpoint is defined and s3Endpoint else omit}}
      AWS_DEFAULT_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
    {% if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - './conf/awscli/s3cfg.template:/root/.s3cfg.template'
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: tail -f /dev/null
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  AWSCLI_enable #}

{% if AZURECLI_enable | default(false) %}
  #  ================================== Azure CLI ========================================== #
  azurecli:
    image: mcr.microsoft.com/azure-cli:{{__AZURECLI_version}}
    container_name: azurecli
    hostname: azurecli
    labels:
      com.platys.name: "azurecli"
      com.platys.description: "Azure CLI"
    {% if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/azurecli:/root
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: tail -f /dev/null
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  AZURECLI_enable #}

{% if AZURE_STORAGE_EXPLORER_enable | default(false) %}
  #  ================================== Azure Storage Explorer ========================================== #
  azure-storage-explorer:
    image: sebagomez/azurestorageexplorer:{{__AZURE_STORAGE_EXPLORER_version}}
    container_name: azure-storage-explorer
    hostname: azure-storage-explorer
    labels:
      com.platys.name: "azure-storage-explorer"
      com.platys.description: "Azure Storage Explorer UI"
      com.platys.webui.title: "Azure Storage Explorer UI"
      com.platys.webui.url: "http://dataplatform:28279"
    ports:
      - 28279:80
    {% if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  AZURE_STORAGE_EXPLORER_enable #}

{% if LAKEFS_enable | default(false) %}
  #  ================================== LakeFS ========================================== #
  lakefs:
    image: treeverse/lakefs:{{__LAKEFS_version}}
    container_name: lakefs
    hostname: lakefs
    labels:
      com.platys.name: "lakefs"
      com.platys.description: "Git-Like Data Version Control"
      com.platys.webui.title: "LakeFS UI"
      com.platys.webui.url: "http://dataplatform:28220"
      com.platys.restapi.title: "LakeFS UI"
      com.platys.restapi.url: "http://dataplatform:28220/api/v1/repositories" 
    {%if not LAKEFS_database_type == 'local' %}
      com.platys.password.envvars: PLATYS_LAKEFS_DATABASE_PASSWORD
    {% endif -%}   {#  LAKEFS_database_type #}
    ports:
      - 28220:8000
    depends_on:
      {{'- postgresql' if LAKEFS_database_type is defined and LAKEFS_database_type == 'postgresql' else omit}}
    environment:
      # possible config settings can be found here: https://docs.lakefs.io/reference/configuration.html#reference
    {%if LAKEFS_database_type == 'local' %}
      LAKEFS_DATABASE_TYPE: local
      LAKEFS_DATABASE_LOCAL_PATH: '~/lakefs/metadata'
      LAKEFS_DATABASE_LOCAL_ENABLE_LOGGING: false
    {%elif LAKEFS_database_type == 'postgresql' %}
      LAKEFS_DATABASE_TYPE: postgres
      LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING: "postgres://{{LAKEFS_database_username}}:{{LAKEFS_database_password}}@postgresql:5432/{{LAKEFS_database_dbname}}"
    {% endif -%}   {#  LAKEFS_database_type #}
      LAKEFS_BLOCKSTORE_TYPE: {{LAKEFS_blockstore_type | default(local) }}
    {%if LAKEFS_blockstore_type == 'local' %}
      LAKEFS_BLOCKSTORE_LOCAL_PATH: '~/lakefs/data'
      LAKEFS_BLOCKSTORE_LOCAL_IMPORT_ENABLED: {{LAKEFS_blockstore_local_import_enabled | default(false)}}
    {%elif LAKEFS_blockstore_type == 's3' %}
      LAKEFS_BLOCKSTORE_S3_ENDPOINT: {{s3EndpointStorage if s3EndpointStorage is defined and s3EndpointStorage else omit}}
      LAKEFS_BLOCKSTORE_S3_REGION: {{s3DefaultRegion if s3DefaultRegion is defined and s3DefaultRegion else omit}}
      LAKEFS_BLOCKSTORE_S3_DISCOVER_BUCKET_REGION: {{LAKEFS_blockstore_s3_discover_bucket_region | default(false)}}
      LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE: '{{s3PathStyleAccess}}'
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_FILE: {{LAKEFS_blockstore_s3_credentials_file if LAKEFS_blockstore_s3_credentials_file is defined and LAKEFS_blockstore_s3_credentials_file != None and LAKEFS_blockstore_s3_credentials_file | length else "~/lakefs/data"}}
      LAKEFS_BLOCKSTORE_S3_CLIENT_LOG_REQUEST: {{LAKEFS_blockstore_s3_client_log_request | default(false) }}
      LAkEFS_BLOCKSTORE_S3_CLIENT_LOG_RETRIES: {{LAKEFS_blockstore_s3_client_log_retries | default(false) }}
    {% endif -%}   {#  LAKEFS_blockstore_type #}
      LAKEFS_UI_ENABLED: true
      LAKEFS_ACTIONS_ENABLED: {{LAKEFS_actions_enabled}}
      LAKEFS_ACTIONS_LUA_NET_HTTP_ENABLED: {{LAKEFS_actions_lua_net_http_enabled}}
      LAKEFS_ACTIONS_ENV_ENABLED: {{LAKEFS_actions_env_enabled | default(false) }}
      LAKEFS_ACTIONS_ENV_PREFIX: {{LAKEFS_actions_env_prefix if LAKEFS_actions_env_prefix is defined and LAKEFS_actions_env_prefix != None and LAKEFS_actions_env_prefix | length else 'LAKEFSACTION_'}}
      LAKEFS_AUTH_ENCRYPT_SECRET_KEY: {{LAKEFS_auth_encrypt_secret_key if LAKEFS_auth_encrypt_secret_key is defined and LAKEFS_auth_encrypt_secret_key != None and LAKEFS_auth_encrypt_secret_key | length else "some random secret string"}}
      LAKEFS_LOGGING_FORMAT: text
      LAKEFS_LOGGING_LEVEL: {{LAKEFS_logging_level | default(INFO) }}
      LAKEFS_LOGGING_AUDIT_LOG_LEVEL: {{LAKEFS_logging_audit_log_level | default(INFO) }}
      LAKEFS_STATS_ENABLED: true
      LAKEFS_USAGE_REPORT_ENABLED: false
      LAKEFS_COMMITTED_LOCAL_CACHE_DIR: {{LAKEFS_commited_local_cache_dir if LAKEFS_commited_local_cache_dir is defined and LAKEFS_commited_local_cache_dir != None and LAKEFS_commited_local_cache_dir | length else omit }}
    {%if LAKEFS_database_type == 'local' %}
      LAKEFS_INSTALLATION_USER_NAME: {{LAKEFS_user_name | default(lakefs) }}
      LAKEFS_INSTALLATION_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      LAKEFS_INSTALLATION_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
    {% endif -%}   {#  use_timezone #}
      LAKECTL_CREDENTIALS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      LAKECTL_SERVER_ENDPOINT_URL: http://${PUBLIC_IP}:28220
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if LAKEFS_blockstore_type == 'local' and LAKEFS_volume_map_data | default(false) %}
      - ./container-volume/lakefs/data:/home/lakefs/lakefs/data
      - ./container-volume/lakefs/metadata:/home/lakefs/lakefs/metadata
    {% endif -%} {# LAKEFS_blockstore_type == 'local' and LAKEFS_volume_map_data #}      
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: [ {{ '"/app/wait-for", "postgresql:5432", "--",' if LAKEFS_database_type == 'postgresql'}} "/bin/sh", "-c"]
    command:
        - |
          lakefs run --local-settings &
    {%if LAKEFS_database_type == 'local' and LAKEFS_quickstart_enabled | default(false) %}
          wait-for -t 60 lakefs:8000 -- echo "---- Creating repository ----" &&  \
              lakectl repo create lakefs://{{LAKEFS_quickstart_repo_name | default(demo)}} {{LAKEFS_blockstore_type}}://{{LAKEFS_quickstart_bucket_name | default('lakefs-demo-bucket')}}/{{LAKEFS_quickstart_repo_name | default(demo)}} --default-branch {{LAKEFS_quickstart_default_branch | default(main)}} {{'--sample-data' if LAKEFS_quickstart_with_demo_data | default(false)}} || true
    {% endif -%}   {#  LAKEFS_quickstart_enabled  #}
          echo "-------- Let's go and have axolotl fun! --------"
          wait
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  lakectl:
    image: treeverse/lakectl:{{__LAKEFS_version}}
    container_name: lakectl
    hostname: lakectl
    labels:
      com.platys.name: "lakefs"
      com.platys.description: "Git-Like Data Version Control"
    environment:
      LAKECTL_CREDENTIALS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
      LAKECTL_SERVER_ENDPOINT_URL: http://${PUBLIC_IP}:28220
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  LAKEFS_enable #}

{% if LAKEFS_HOOKS_enable | default(false) %}
  lakefs-hooks:
    image: trivadis/lakefs-hooks:{{__LAKEFS_HOOKS_version}}
    container_name: lakefs-hooks
    hostname: lakefs-hooks
    labels:
      com.platys.name: "lakefs-hooks"
      com.platys.description: "Git-Like Data Version Control"
    ports:
      - 28221:5000
    environment:
      LAKEFS_SERVER_ADDRESS: http://lakefs:8000
      LAKEFS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      LAKEFS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  LAKEFS_HOOKS_enable #}

{% if NESSIE_enable | default(false) %}
  #  ================================== Project Nessie ========================================== #
  nessie:
    image: ghcr.io/projectnessie/nessie:{{__NESSIE_version}}
    container_name: nessie
    hostname: nessie
    labels:
      com.platys.name: "nessie"
      com.platys.description: "Git-Like Data Version Control"
      com.platys.webui.title: "Nessie UI"
      com.platys.webui.url: "http://dataplatform:19120"
      com.platys.restapi.title: "Nessie REST API"
      com.platys.restapi.url: "http://dataplatform:19120/api/v2/trees"
    ports:
      # API port    
      - 19120:19120
      # Management port (metrics and health checks)
      - 19121:9000    
    environment:
      - QUARKUS_PROFILE=prod
      - QUARKUS_HTTP_PORT=19120
      - QUARKUS_LOG_CONSOLE_FORMAT=%d{yyyy-MM-dd HH:mm:ss} %-5p [%c{1.}] (%t) %s%e%n
      - QUARKUS_LOG_LEVEL=INFO      
      - NESSIE_SERVER_DEFAULT_BRANCH=main
    {%if NESSIE_store_type == 'in-memory' %}
      - NESSIE_VERSION_STORE_TYPE=IN_MEMORY
    {%elif NESSIE_store_type == 'rocksdb' %}
      - NESSIE_VERSION_STORE_TYPE=ROCKSDB
      - NESSIE_VERSION_STORE_PERSIST_ROCKS_DATABASE_PATH=/nessie/data
      - NESSIE_VERSION_STORE_PERSIST_CACHE_CAPACITY_MB=1024 
      - NESSIE_VERSION_STORE_PERSIST_COMMIT_TIMEOUT_MILLIS=10000
    {%elif NESSIE_store_type == 'mongodb' %}
      - NESSIE_VERSION_STORE_TYPE=MONGODB2
      - QUARKUS_MONGODB_DATABASE=nessie
      - QUARKUS_MONGODB_METRICS_ENABLED=true
      - QUARKUS_MONGODB_CONNECTION_STRING=mongodb://{{MONGO_root_username}}:{{MONGO_root_password}}@mongo:27017
      - QUARKUS_MONGODB_DEVSERVICES_ENABLED=true
    {%elif NESSIE_store_type == 'cassandra' %}
      - NESSIE_VERSION_STORE_TYPE=CASSANDRA2
      - QUARKUS_CASSANDRA_CONTACT_POINTS=cassandra-1:9042
      - QUARKUS_CASSANDRA_LOCAL_DATACENTER=se1
      - QUARKUS_CASSANDRA_KEYSPACE=nessie
    {%elif NESSIE_store_type == 'postgresql' %}
      - NESSIE_VERSION_STORE_TYPE=JDBC
      - NESSIE_VERSION_STORE_PERSIST_JDBC_DATASOURCE=postgresql
      - QUARKUS_DATASOURCE_POSTGRESQL_DB_KIND=postgresql
      - QUARKUS_DATASOURCE_POSTGRESQL_USERNAME={{POSTGRESQL_user}}
      - QUARKUS_DATASOURCE_POSTGRESQL_PASSWORD={{POSTGRESQL_password}}
      - QUARKUS_DATASOURCE_POSTGRESQL_JDBC_URL=jdbc:postgresql://postgresql:5432/{{POSTGRESQL_database}}
    {%elif NESSIE_store_type == 'mysql' %}
      - NESSIE_VERSION_STORE_TYPE=JDBC
      - NESSIE_VERSION_STORE_PERSIST_JDBC_DATASOURCE=mariadb
      - QUARKUS_DATASOURCE_MYSQL_DB_KIND=mariadb
      - QUARKUS_DATASOURCE_MYSQL_USERNAME={{MYSQL_user}}
      - QUARKUS_DATASOURCE_MYSQL_PASSWORD={{MYSQL_password}}
      - QUARKUS_DATASOURCE_MYSQL_JDBC_URL=jdbc:mysql://mysql:3306/{{MYSQL_database}}
    {% endif -%}   {#  NESSIE_store_type #}
    {%if MINIO_enable or external['S3_enable'] | default(false) %}
      # using MinIO as the object store.
      - NESSIE_CATALOG_DEFAULT_WAREHOUSE=warehouse
      - NESSIE_CATALOG_WAREHOUSES_WAREHOUSE_LOCATION={{NESSIE_warehouse_location}}
      - NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_REGION={{s3DefaultRegion | default(omit) }}
      - NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_PATH_STYLE_ACCESS=true
      - nessie.catalog.service.s3.default-options.access-key=urn:nessie-secret:quarkus:nessie.catalog.secrets.access-key
      - nessie.catalog.secrets.access-key.name={{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      - nessie.catalog.secrets.access-key.secret={{s3SecretAccessKey}}
      - NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_ENDPOINT={{s3Endpoint | default(omit) }}
      - NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_EXTERNAL_ENDPOINT=http://127.0.0.1:9002/
    {% endif %}  
    {%if KEYCLOAK_enable | default(false) %}
      - NESSIE_SERVER_AUTHENTICATION_ENABLED=true
      - QUARKUS_OIDC_ENABLED=true
      - QUARKUS_OIDC_AUTH_SERVER_URL=http://keycloak:8080/realms/iceberg
      - QUARKUS_OIDC_CLIENT_ID=client1
      - QUARKUS_OIDC_TOKEN_ISSUER=http://127.0.0.1:8080/realms/iceberg
    {% endif %}  
    {%if AUTHELIA_enable | default(false) %}
      - NESSIE_SERVER_AUTHENTICATION_ENABLED=true
      - QUARKUS_OIDC_ENABLED=true
      - QUARKUS_OIDC_AUTH_SERVER_URL=http://authelia:9091
      - QUARKUS_OIDC_CLIENT_ID=nessie_client
      - QUARKUS_OIDC_CREDENTIALS_SECRET=s3cr3t
    {% endif %}  
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if NESSIE_volume_map_data | default(false) %}
      - ./container-volume/nessie/data:/nessie/data
    {% endif -%} {# NESSIE_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: "exec 3<>/dev/tcp/localhost/9000 && echo -e 'GET /q/health HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | grep -q '200 OK'"
      interval: 5s
      timeout: 2s
      retries: 15    

  {% if NESSIE_CLI_enable | default(false) %}
  nessie-cli:
    image: ghcr.io/projectnessie/nessie-cli:{{__NESSIE_version}}
    hostname: nessie-cli
    container_name: nessie-cli    
    depends_on:
      nessie:
        condition: service_healthy
    stdin_open: true
    tty: true
    labels:
      com.platys.name: "nessie"
      com.platys.description: "Git-Like Data Version Control (CLI)"  
    command: [
        --uri, "http://nessie:19120/api/v2"
      ]
  {% endif %}   {#  NESSIE_CLI_enable #}
{% endif %}   {#  NESSIE_enable #}

{% if UNITY_CATALOG_enable | default(false) %}
  #  ================================== Unity Catalog ========================================== #
  unity-catalog:
    image: unitycatalog/unitycatalog:{{__UNITY_CATALOG_version}}
    container_name: unity-catalog
    hostname: unity-catalog
    labels:
      com.platys.name: "unity-catalog"
      com.platys.description: "Open, Multi-modal Catalog for Data & AI"
      com.platys.restapi.title: "Unity Catalog REST API"
      com.platys.restapi.url: "http://dataplatform:28392/api/2.1/unity-catalog/catalogs"
    ports:
      - 28392:8080
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if UNITY_CATALOG_UI_enable | default(false) %}
  unity-catalog-ui:
    image: ghcr.io/sdwbgn/unitycatalog-helm/docker/unitycatalog-ui:{{__UNITY_CATALOG_UI_version}}
    platform: linux/amd64
    container_name: unity-catalog-ui
    hostname: unity-catalog-ui
    labels:
      com.platys.name: "unity-catalog"
      com.platys.description: "Open, Multi-modal Catalog for Data & AI"
      com.platys.webui.title: "Unity Catalog UI"
      com.platys.webui.url: "http://dataplatform:28393"
    ports:
      - 28393:3000
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: 
      - sh
      - -c
      - |
        sed -i 's/server/unity-catalog/g' /ui/package.json
        yarn start            
    restart: {{container_restart_policy}}
  {% endif %}   {#  UNITY_CATALOG_UI_enable #}

{% endif %}   {#  UNITY_CATALOG_enable #}

{% if MOSQUITTO_enable | default(false) %}
  #  ================================== Mosquitto ========================================== #
    {% for num in range(MOSQUITTO_nodes | default(1) ) %}
      {% set external_port_1 = 1883 + loop.index - 1 %}
      {% set external_port_2 = 9101 + loop.index -1 %}
  mosquitto-{{loop.index}}:
    image: eclipse-mosquitto:{{__MOSQUITTO_version}}
    hostname: mosquitto-{{loop.index}}
    container_name: mosquitto-{{loop.index}}
    labels:
      com.platys.name: "mosquitto"
      com.platys.description: "MQTT Broker"
    ports:
      - "{{external_port_1}}:1883"
      - "{{external_port_2}}:9001"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf
    {% if MOSQUITTO_volume_map_data %}
      - "./container-volume/mosquitto/mosquitto-{{loop.index}}:/mosquitto/data/"
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    {% endfor %}
{% endif %}   {#  MOSQUITTO_enable #}

{% if HIVEMQ3_enable | default(false) %}
  #  ================================== HiveMQ v3 ========================================== #
  hivemq3-1:
    image: hivemq/hivemq3:{{__HIVEMQ3_version}}
    hostname: hivemq3-1
    container_name: hivemq3-1
    labels:
      com.platys.name: "hivemq3"
      com.platys.description: "MQTT Broker"
      com.platys.webui.title: "HiveMQ 3 WebUI"
      com.platys.webui.url: "http://dataplatform:28132"
    ports:
      - "1886:1883"
      - "28132:8080"
      - "28133:8000"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  HIVEMQ3_enable #}

{% if HIVEMQ4_enable | default(false) %}
  #  ================================== HiveMQ v4 ========================================== #
  hivemq4-1:
    image: hivemq/hivemq4:{{__HIVEMQ4_version}}
    hostname: hivemq4-1
    container_name: hivemq4-1
    labels:
      com.platys.name: "hivemq4"
      com.platys.description: "MQTT Broker"
      com.platys.webui.title: "HiveMQ 4 WebUI"
      com.platys.webui.url: "http://dataplatform:28134"
    ports:
      - "1888:1883"
      - "28134:8080"
      - "28135:8000"
    environment:
#      HIVEMQ_LICENSE:
      HIVEMQ_LOG_LEVEL: INFO
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  HIVEMQ4_enable #}

{% if EMQX_enable | default(false) %}
  #  ================================== EMQX ========================================== #
  emqx-1:
  {% if (EMQX_edition | lower) == 'oss' %}
    image: emqx/emqx:{{__EMQX_oss_version}}
  {% else -%}   {#  EMQX_edition #}
    image: emqx/emqx-ee:{{__EMQX_enterprise_version}}
  {% endif -%}   {#  EMQX_edition #}
    hostname: emqx-1
    container_name: emqx-1
    labels:
    labels:
      com.platys.name: "emqx"
      com.platys.description: "MQTT Broker"
      com.platys.webui.title: "EMQX WebUI"
      com.platys.webui.url: "http://dataplatform:28404"
      com.platys.restapi.title: "EMQX REST API"
      com.platys.restapi.url: "http://dataplatform:28404/api/v4/"
    ports:
      - "1891:1883"
      - "28400:8081"
      - "28401:8083"
      - "28402:8084"
      - "28403:8883"
      - "28404:18083"
    environment:
      EMQX_NAME: emqx-1
      EMQX_LOG__CONSOLE: console
      EMQX_ALLOW_ANONYMOUS: 'true'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  EMQX_enable #}

{% if MQTTX_CLI_enable | default(false) %}
  #  ================================== MQTTX Web ========================================== #
  mqttx-cli:
    image: emqx/mqttx-cli:{{__MQTTX_CLI_version}}
    container_name: mqttx-cli
    hostname: mqttx-cli
    labels:
      com.platys.name: "mqttx"
      com.platys.description: "a CLI for MQTT"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    restart: {{container_restart_policy}}
{% endif %}   {#  MQTTX_CLI_enable #}

{% if MQTTX_WEB_enable | default(false) %}
  #  ================================== MQTTX Web ========================================== #
  mqttx-web:
    image: emqx/mqttx-web:{{__MQTTX_WEB_version}}
    container_name: mqttx-web
    hostname: mqttx-web
    labels:
      com.platys.name: "mqttx"
      com.platys.description: "a Web UI for MQTT"
      com.platys.webui.title: "MQTTX-web UI"
      com.platys.webui.url: "http://dataplatform:28367"
    ports:
      - '28367:80'
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MQTTX_WEB_enable #}

{% if MQTT_UI_enable | default(false) %}
  #  ================================== MQTT UI ========================================== #
  mqtt-ui:
    image: vergissberlin/hivemq-mqtt-web-client:{{__HIVEMQ_MQTT_WEB_CLIENT_version}}
    container_name: mqtt-ui
    hostname: mqtt-ui
    labels:
      com.platys.name: "hivemq-ui"
      com.platys.description: "MQTT UI"
      com.platys.webui.title: "HiveMQ UI"
      com.platys.webui.url: "http://dataplatform:28136"
    ports:
      - '28136:80'
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MQTT_UI_enable #}

{% if CEDALO_MANAGEMENT_CENTER_enable | default(false) %}
  #  ================================== Cedalo Management Center ========================================== #
  cedalo-management-center:
    image: cedalo/management-center:{{__CEDALO_MANAGEMENT_CENTER_version}}
    container_name: cedalo-management-center
    hostname: cedalo-management-center
    labels:
      com.platys.name: "cedalo-mc"
      com.platys.description: "MQTT Management Server"
      com.platys.webui.title: "Cedalo Management Center UI"
#      com.platys.webui.url: "http://dataplatform:28175"
      com.platys.webui.url: "http://dataplatform:28175"
    expose:
      - 8088
    ports:
      - 28175:8088
    environment:
      CEDALO_MC_BROKER_ID: mosquitto-2.0
      CEDALO_MC_BROKER_NAME: Mosquitto 2.0
      CEDALO_MC_BROKER_URL: mqtt://mosquitto-1:1883
      CEDALO_MC_BROKER_USERNAME: default
      CEDALO_MC_BROKER_PASSWORD: ""
      CEDALO_MC_PROXY_CONFIG_DIR: /management-center/config/config.json
      CEDALO_MC_USERNAME: {{CEDALO_MANAGEMENT_CENTER_username | default(cedalo)}}
      CEDALO_MC_PASSWORD: {{CEDALO_MANAGEMENT_CENTER_password | default(secret)}}
    {% if (STREAMSHEETS_enable) | default(false) %}
      CEDALO_STREAMSHEETS_ID: streamsheets
      CEDALO_STREAMSHEETS_NAME: Streamsheets
      CEDALO_STREAMSHEETS_DESCRIPTION: Local Streamsheets
      CEDALO_STREAMSHEETS_URL: http://${PUBLIC_IP}:28175
    {% endif -%}   {#  STREAMSHEETS_enable #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
#     - ./config:/management-center/backend/config
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CEDALO_MANAGEMENT_CENTER_enable #}

{% if THINGSBOARD_enable | default(false) %}
  #  ================================== Cedalo Management Center ========================================== #
  thingsboard:
    image: thingsboard/tb-postgres:{{__THINGSBOARD_version}}
    container_name: thingsboard
    hostname: thingsboard
    labels:
      com.platys.name: "thingsboard"
      com.platys.description: "IoT Platform"
      com.platys.webui.title: "Thingsboard UI"
      com.platys.webui.url: "http://dataplatform:28200"
    ports:
      - "28200:9090"
      - "28201:1883"
      - "28202:5683/udp"
    environment:
      TB_QUEUE_TYPE: kafka
      TB_KAFKA_SERVERS: {{ns.bootstrapServers}}
    {% if ns.secureKafkaEnabled %}
      - TB_QUEUE_KAFKA_CONFLUENT_SECURITY_PROTOCOL='{{ns.securityProtocolBROKER}}'
      {% if ns.securityProtocolBROKER == 'SASL_PLAINTEXT' %}
      - TB_QUEUE_KAFKA_CONFLUENT_SASL_MECHANISM='{{ns.saslMechanismBROKER}}'
      - TB_QUEUE_KAFKA_CONFLUENT_SASL_JAAS_CONFIG='{{ns.loginModuleBROKER}} required username=\"{{ns.kafkaToolsDefaultUsername}}\" password=\"{{ns.kafkaToolsDefaultPassword}}\";'
      {% elif ns.securityProtocolBROKER == 'SSL' %}
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM='https'
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_TRUSTSTORE_LOCATION=/certs/client.ts.p12
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_TRUSTSTORE_PASSWORD={{KAFKA_client_truststore_password}}
        {% if KAFKA_ssl_client_authentication_enable | default(false) %}
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_KEYSTORE_TYPE="PKCS12"
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_KEYSTORE_LOCATION=/certs/client.ks.p12
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_KEYSTORE_PASSWORD={{KAFKA_client_keystore_password}}
      - TB_QUEUE_KAFKA_CONFLUENT_SSL_KEY_PASSWORD={{KAFKA_client_key_password}}
        {% endif -%}   {#  KAFKA_ssl_client_authentication_enable #}
      {% endif -%}   {#  ns.securityProtocol #}
    {% endif -%}   {#  secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if THINGSBOARD_volume_map_data %}
      - ./container-volume/thingsboard/data:/data
    {% endif -%}   {#  THINGSBOARD_volume_map_data #}
    {% if THINGSBOARD_volume_map_log %}
      - ./container-volume/thingsboard/log:/var/log/thingsboard
    {% endif -%}   {#  THINGSBOARD_volume_map_log #}
    {% if ns.secureKafkaEnabled and ns.securityProtocol == 'SSL' %}
      - ./security/kafka/client-certs:/certs
    {% endif -%} {#  ns.secureKafkaEnabled #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  THINGSBOARD_enable #}

{% if ACTIVEMQ_enable | default(false) %}
  #  ================================== Active MQ ========================================== #
  activemq:
  {% if (ACTIVEMQ_edition | lower) == 'classic' | default(false) %}
    image: active/activemq-classic:{{__ACTIVEMQ_CLASSIC_version}}
  {% elif (ACTIVEMQ_edition | lower) == 'artemis' | default(false) %}
    image: apache/activemq-artemis:{{__ACTIVEMQ_ARTEMIS_version}}
  {% endif -%}   {#  ACTIVEMQ_edition #}
    container_name: activemq
    labels:
      com.platys.name: "activemq"
      com.platys.description: "Message Broker"
      com.platys.webui.title: "ActiveMQ UI"
      com.platys.webui.url: "http://dataplatform:8161"
    ports:
      # mqtt
      - "1890:1883"
      # amqp
      - "5672:5672"
      # ui
      - "8161:8161"
      # stomp
      - "61613:61613"
      # ws
      - "61614:61614"
      # jms
      - "61616:61616"
    environment:
      DUMMY: make-it-valid
    {% if (ACTIVEMQ_edition | lower) == 'artemis' | default(false) %}
      ANONYMOUS_LOGIN: true
      ARTEMIS_USER: {{ACTIVEMQ_user | default(omit) }}
      ARTEMIS_PASSWORD: {{ACTIVEMQ_password | default(omit) }}
    {% endif -%}   {#  use_timezone #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if ACTIVEMQ_volume_map_data %}
      {% if (ACTIVEMQ_edition | lower) == 'classic' | default(false) %}
      - ./container-volume/activemq/data:/opt/activemq/data
      {% elif (ACTIVEMQ_edition | lower) == 'artemis' | default(false) %}
      - ./container-volume/activemq/data:/var/lib/artemis-instance
      {% endif -%}   {#  ACTIVEMQ_edition #}
    {% endif -%}   {#  ACTIVEMQ_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ACTIVEMQ_enable #}

{% if RABBITMQ_enable | default(false) %}
  #  ================================== RabbitMQ ========================================== #
  rabbitmq:
    image: rabbitmq:{{__RABBITMQ_version}}
    container_name: rabbitmq
    hostname: rabbitmq
    labels:
      com.platys.name: "rabbitmq"
      com.platys.description: "Message Broker"
      com.platys.webui.title: "RabbitMQ UI"
      com.platys.webui.url: "http://dataplatform:15672"
      com.platys.password.envvars: "PLATYS_RABBITMQ_PASSWORD"         
    ports:
      - 5673:5672
      - 15672:15672
    environment:
      RABBITMQ_ERLANG_COOKIE: "62cd4092-dbb3-4e7a-ac6d-ffd7cee850ee"
      RABBITMQ_DEFAULT_USER: rabbitmq
      RABBITMQ_DEFAULT_PASS: ${PLATYS_RABBITMQ_PASSWORD:-{{RABBITMQ_password}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if RABBITMQ_volume_map_data %}
      - ./container-volume/rabbitmq/data:/var/lib/rabbitmq/
    {% endif -%}   {#  RABBITMQ_volume_map_data #}
    {% if RABBITMQ_volume_map_logs %}
      - ./container-volume/rabbitmq/logs/:/var/log/rabbitmq/
    {% endif -%}   {#  RABBITMQ_volume_map_logs #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  RABBITMQ_enable #}

{% if SOLACE_PUBSUB_enable | default(false) %}
  #  ================================== Solace PubSub+ ========================================== #
  solace-pubsub:
    image: solace/solace-pubsub-standard:{{__SOLACE_PUBSUB_version}}
    container_name: solace-pubsub
    hostname: solace-pubsub
    labels:
      com.platys.name: "solace-pubsub"
      com.platys.description: "Solace Event Broker"
      com.platys.webui.title: "Solace UI"
      com.platys.webui.url: "http://dataplatform:28358"
      com.platys.restapi.title: "Solace Rest API"
      com.platys.restapi.url: "http://dataplatform:28359"
    shm_size: 1g
    ulimits:
      core: -1
      nofile:
        soft: 2448
        hard: 6592
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 1
    ports:
      - 28357:8008    # Web Transport
      - 1892:1883     # MQTT
      - 5674:5672     # AMQP
      - 28358:8080    # SEMP
      - 28359:9000    # REST
      - 28360:55555   # Solace Message Format (SMF)
    environment:
      username_admin_globalaccesslevel: {{SOLACE_PUBSUB_username}}
      username_admin_password: {{SOLACE_PUBSUB_password}}
      system_scaling_maxconnectioncount: 100
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if SOLACE_PUBSUB_volume_map_data %}
      - ./container-volume/solace/data:/var/lib/solace
    {% endif -%}   {#  SOLACE_PUBSUB_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SOLACE_PUBSUB_enable #}

{% if SOLACE_KAFKA_PROXY_enable | default(false) %}
  #  ================================== Solace Kafka Proxy ========================================== #
  solace-kafka-proxy:
    image: trivadis/pubsubplus-kafka-proxy:{{__SOLACE_KAFKA_PROXY_version}}
    container_name: solace-kafka-proxy
    hostname: solace-kafka-proxy
    labels:
      com.platys.name: "solace-kafka-proxy"
      com.platys.description: "Solace Kafka Proxy"
    ports:
      - 59092:59092
    environment:
      PROXY_LISTENERS: PLAINTEXT://solace-kafka-proxy:59092
      PROXY_ADVERTISED_LISTENERS: PLAINTEXT://${PUBLIC_IP}:59092
      SOLACE_HOST: tpc://solace-pubsub:55555
      SOLACE_VPN_NAME: {{SOLACE_KAFKA_PROXY_vpn_name}}
      SOLACE_SEPARATORS: {{SOLACE_KAFKA_PROXY_separators}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SOLACE_KAFKA_PROXY_enable #}

{% if NATS_enable | default(false) %}
  #  ================================== NATS ========================================== #
  nats-1:
    image: nats:{{__NATS_version}}
    container_name: nats-1
    hostname: nats-1
    labels:
      com.platys.name: "nats"
      com.platys.description: "Cloud and edge native messaging"
      com.platys.webui.title: "NATS Monitoring UI"
      com.platys.webui.url: "http://dataplatform:8222"
    ports:
      - 4222:4222
      - 8222:8222
      - 6222:6222
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  NATS_enable #}

{% if PURE_FTPD_enable | default(false) %}
  #  ================================== Pure FTPd Server ========================================== #
  pure-ftpd:
    image: stilliard/pure-ftpd:{{__PURE_FTPD_version}}
    container_name: pure-ftpd
    hostname: pure-ftpd
    labels:
      com.platys.name: "pure-ftp"
    environment:
#      - PUBLICHOST="192.168.73.86"
      - PUBLICHOST=pure-ftpd
      - FTP_USER_NAME={{PURE_FTPD_username}}
      - FTP_USER_PASS={{PURE_FTPD_password}}
      - FTP_USER_HOME={{PURE_FTPD_home}}
      - FTP_MAX_CLIENTS=9
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    ports:
      - "21:21"
      - "30000-30009:30000-30009"
    volumes:
      - ./data-transfer:/data-transfer
    {% if PURE_FTPD_volume_map_data %}
      - ./container-volume/pure-ftpd/data:{{PURE_FTPD_home}}
    {% elif PURE_FTPD_volume_map_data_transfer %}
      - ./data-transfer:{{PURE_FTPD_home}}
    {% endif -%}   {#  PURE_FTPD_volume_map_data or PURE_FTPD_volume_map_data_transfer #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  PURE_FTPD_enable #}

{% if SFTP_enable | default(false) %}
  #  ================================== SFTP Server ========================================== #
  sftp:
    image: atmoz/sftp:{{__SFTP_version}}
    container_name: sftp
    hostname: sftp
    labels:
      com.platys.name: "sftp"
      com.platys.description: "SFTP Server"
      com.platys.password.envvars: "PLATYS_SFTP_PASSWORD"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    ports:
      - "2222:22"
    volumes:
      - ./data-transfer:/data-transfer
    {% if SFTP_volume_map_data %}
      - ./container-volume/sftp/data:/home/{{SFTP_username}}/{{SFTP_home}}
    {% elif SFTP_volume_map_data_transfer %}
      - ./data-transfer:/home/{{SFTP_username}}/{{SFTP_home}}
    {% endif -%}   {#  SFTP_volume_map_data or SFTP_volume_map_data_transfer #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    command: {{SFTP_username}}:${PLATYS_SFTP_PASSWORD:-{{SFTP_password}}}:::{{SFTP_home}}
    restart: {{container_restart_policy}}
{% endif %}   {#  FTP_enable #}

{% if SFTPGO_enable | default(false) %}
  #  ================================== SFTPgo Server ========================================== #
  sftpgo:
    image: drakkan/sftpgo:{{__SFTPGO_version}}
    container_name: sftpgo
    hostname: sftpgo
    labels:
      com.platys.name: "sftpgo"
      com.platys.description: "SFTPgo Server"
      com.platys.webui.title: ""
      com.platys.webui.url: "http://dataplatform:28420"
      com.platys.password.envvars: "PLATYS_SFTPGO_ADMIN_PASSWORD"
    environment:
      - SFTPGO_DEFAULT_ADMIN_USERNAME={{SFTPGO_admin_username}}
      - SFTPGO_DEFAULT_ADMIN_PASSWORD=${PLATYS_SFTPGO_ADMIN_PASSWORD:-{{SFTPGO_admin_password}}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    ports:
      - "28420:8080"    
      - "2022:2022"
    volumes:
      - ./data-transfer:/data-transfer
    {% if SFTPGO_volume_map_data %}
      - ./container-volume/sftpgo/data:/srv/sftpgo/data/{{SFTPGO_home}}
    {% elif SFTPGO_volume_map_data_transfer %}
      - ./data-transfer:/srv/sftpgo/data/{{SFTPGO_home}}
    {% endif -%}   {#  SFTPGO_volume_map_data or SFTPGO_volume_map_data_transfer #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SFTPGO_enable #}

{% if FILEZILLA_enable | default(false) %}
  #  ================================== Filezilla ========================================== #
  filezilla:
    image: jlesage/filezilla:{{__FILEZILLA_version}}
    container_name: filezilla
    hostname: filezilla
    labels:
      com.platys.name: "filezilla"
      com.platys.description: "FTP UI"
      com.platys.webui.title: "Filezilla UI"
      com.platys.webui.url: "http://dataplatform:5800"
    ports:
      - "5800:5800"
      - "5900:5900"
    environment:
      - VNC_PASSWORD={{admin_password}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
#    volumes:
#      - "/docker/appdata/filezilla:/config:rw"
#      - "/files:/storage:rw"
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  FILEZILLA_enable #}

{% if MAILDEV_enable | default(false) %}
  #  ================================== MailDev ========================================== #
  maildev:
    image: maildev/maildev:{{__MAILDEV_version}}
    container_name: maildev
    hostname: maildev
    labels:
      com.platys.name: "maildev"
      com.platys.description: "SMTP Server + Web Interface for viewing and testing emails"
      com.platys.webui.title: "Maildev UI"
      com.platys.webui.url: "http://dataplatform:28316"
    ports:
      - "28316:1080"
      - "28317:{{MAILDEV_smtp_port}}"
    environment:
      - MAILDEV_WEB_PORT=1080
      - MAILDEV_SMTP_PORT={{MAILDEV_smtp_port}}
    {% if MAILDEV_web_disable | default(false) %}
      - MAILDEV_DISABLE_WEB=true
    {% endif -%}   {#  MAILDEV_web_disable #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MAILDEV_enable #}

{% if MAILPIT_enable | default(false) %}
  #  ================================== Mailpit ========================================== #
  mailpit:
    image: axllent/mailpit:{{__MAILPIT_version}}
    container_name: mailpit
    hostname: mailpit
    labels:
      com.platys.name: "mailpit"
      com.platys.description: "SMTP Server + Web Interface for viewing and testing emails"
      com.platys.webui.title: "Mailpit UI"
      com.platys.webui.url: "http://dataplatform:28318"
    ports:
      - "28318:8025"
      - "28319:{{MAILPIT_smtp_port}}"
    environment:
    {% if MAILPIT_volume_map_data %}
      - MP_DATA_FILE=/data/mailpit.db
    {% endif -%}   {#  MAILPIT_volume_map_data #}
      - MP_SMTP_BIND_ADDR=0.0.0.0:{{MAILPIT_smtp_port}}
      - MP_UI_BIND_ADDR=0.0.0.0:8025
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MAILPIT_volume_map_data %}
      - ./container-volume/mailpit/data:/data
    {% endif -%}   {#  MAILPIT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MAILPIT_enable #}

{% if MAILHOG_enable | default(false) %}
  #  ================================== MailHog ========================================== #
  mailhog:
    image: mailhog/mailhog:{{__MAILHOG_version}}
    container_name: mailhog
    hostname: mailhog
    labels:
      com.platys.name: "mailhog"
      com.platys.description: "SMTP Server + Web Interface for viewing and testing emails"
      com.platys.webui.title: "MailHog UI"
      com.platys.webui.url: "http://dataplatform:28325"
      com.platys.restapi.title: "MailHog REST API"
      com.platys.restapi.url: "http://dataplatform:28325/api/v2/messages"
    ports:
      - "28325:8025"
      - "28326:{{MAILHOG_smtp_port}}"
    environment:
      - MH_SMTP_BIND_ADDR=0.0.0.0:{{MAILHOG_smtp_port}}
      - MH_UI_BIND_ADDR=0.0.0.0:8025
      - MH_API_BIND_ADDR=0.0.0.0:8025
      - MH_STORAGE={{MAILHOG_storage_type}}
    {% if MAILHOG_storage_type | lower == 'maildir' and MAILHOG_volume_map_data %}
      - MH_MAILDIR_PATH=/data
    {% endif -%}   {#  MAILHOG_storage_type == maildir and MAILHOG_volume_map_data #}
    {% if MAILHOG_storage_type | lower == 'mongodb' and MAILHOG_volume_map_data %}
      - MH_MONGO_URI={{MAILHOG_mongo_uri}}
      - MH_MONGO_DB={{MAILHOG_mongo_db}}
      - MH_MONGO_COLLECTION={{MAILHOG_mongo_collection}}
    {% endif -%}   {#  MAILHOG_storage_type == mongodb #}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MAILHOG_volume_map_data %}
      - ./container-volume/mailhog/data:/data
    {% endif -%}   {#  MAILPIT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MAILHOG_enable #}

{% if CAMUNDA_BPM_PLATFORM_enable | default(false) %}
  #  ================================== Camunda BPM Platform ========================================== #
  bpm-platform:
    image: registry.camunda.cloud/cambpm-ee/camunda-bpm-platform-ee:{{__CAMUNDA_BPM_PLATFORM_version}}
    container_name: bpm-platform
    hostname: bpm-platform
    labels:
      com.platys.name: "camunda-bpm-platform"
      com.platys.description: "BPMN Engine"
      com.platys.webui.title: "Camunda BPM Platform UI"
      com.platys.webui.url: "http://dataplatform:28263/camunda-welcome/index.html"
      com.platys.restapi.title: "Camunda BPM Platform Rest-API"
      com.platys.restapi.url: "http://dataplatform:28263/engine-rest"
    ports:
      - 28263:8080
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    mem_limit: 1g
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CAMUNDA_BPM_PLATFORM_enable #}

{% if CAMUNDA_OPTIMIZE_enable | default(false) %}
  #  ================================== Camunda Optimize ========================================== #
  optimize:
    image: registry.camunda.cloud/optimize-ee/optimize:{{__CAMUNDA_OPTIMIZE_version}}
    container_name: optimize
    hostname: optimize
    labels:
      com.platys.name: "camunda-optimize"
      com.platys.description: "Integrated process intelligence platform and analytics"
      com.platys.webui.title: "Camunda Optimize UI"
      com.platys.webui.url: "http://dataplatform:28264"
    ports:
      - 28264:8090
    environment:
      JAVA_OPTS: "-Xms1g -Xmx1g -XX:MaxMetaspaceSize=256m"
      OPTIMIZE_CAMUNDABPM_REST_URL: "http://bpm-platform:8080/engine-rest"
      OPTIMIZE_ELASTICSEARCH_HOST: "elasticsearch-1"
      OPTIMIZE_CAMUNDA_BPM_EVENT_IMPORT_ENABLED: "true"
      OPTIMIZE_EVENT_BASED_PROCESSES_IMPORT_ENABLED: "true"
      OPTIMIZE_EVENT_BASED_PROCESSES_USER_IDS: "demo"
      OPTIMIZE_EVENT_INGESTION_ACCESS_TOKEN: "mytoken"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - ./conf/camunda-optimize/environment-config.yaml:/optimize/config/environment-config.yaml
      - ./custom-conf/camunda/CamundaLicense.txt:/optimize/config/OptimizeLicense.txt
    mem_limit: 2g
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  CAMUNDA_OPTIMIZE_enable #}

{% if CAMUNDA_ZEEBE_enable | default(false) %}
  #  ================================== Penthao Webspoon ========================================== #
  zeebe-1:
    image: camunda/zeebe:{{__CAMUNDA_ZEEBE_version}}
    container_name: zeebe-1
    hostname: zeebe-1
    labels:
      com.platys.name: "camunda-zeebe"
      com.platys.description: "Distributed workflow engine for microservices orchestration"
    ports:
      - '26500:26500'
      - '9600:9600'
      - '5705:5705'
    environment:
      ZEEBE_LOG_LEVEL: debug
      ZEEBE_INSECURE_CONNECTION: "true"
      ZEEBE_HAZELCAST_PORT: 5705
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if CAMUNDA_ZEEBE_volume_map_data %}
      - ./container-volume/camunda/zebee/data:/usr/local/zeebe/data
    {% endif -%}   {#  ZEEBE_volume_map_data #}
      - ./conf/camunda/zeebe/application.yaml:/usr/local/zeebe/config/application.yaml
#      - ./plugins/camunda/zeebe/zeebe-hazelcast-exporter-0.10.0.jar:/usr/local/zeebe/exporters/zeebe-hazelcast-exporter.jar
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if CAMUNDA_OPERATE_enable | default(false) %}
  operate:
    image: camunda/operate:{{__CAMUNDA_OPERATE_version}}
    container_name: operate
    hostname: operate
    labels:
      com.platys.name: "camunda-operate"
      com.platys.description: "Intuitive dashboards for process observability and troubleshooting"
      com.platys.webui.title: "Camunda Operate UI"
      com.platys.webui.url: "http://dataplatform:28207"
    ports:
      - '28207:8080'
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/camunda/operate/application.yaml:/usr/local/operate/config/application.yml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  CAMUNDA_OPERATE_enable #}

  {% if CAMUNDA_ZEEQS_enable | default(false) %}
  zeeqs:
    image: camunda/zeeqs:{{__CAMUNDA_ZEEQS_version}}
    container_name: zeeqs
    hostname: zeeqs
    labels:
      com.platys.name: "camunda-zeeqs"
      com.platys.description: "GraphQL API for Zeebe data"
      com.platys.webui.title: "Camunda Zeeqs GraphQL UI"
      com.platys.webui.url: "http://dataplatform:28208/graphiql"
    ports:
      - '28208:9000'
    environment:
      zeebe.client.worker.hazelcast.connection: 'zeebe-1:5705'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  CAMUNDA_ZEEQS_enable #}
{% endif %}   {#  CAMUNDA_ZEEBE_enable #}

{% if X4_SERVER_enable | default(false) %}
  #  ================================== Softproject X4 Server ========================================== #
  x4-server:
    image: softproject/x4_server:{{__X4_SERVER_version}}
    container_name: x4-server
    hostname: x4-server
    labels:
      com.platys.name: "x4-server"
      com.platys.description: "BPMN and ESB engine"
      com.platys.restapi.title: "X4 Server Rest-API"
      com.platys.restapi.url: "http://dataplatform:28361"
    ports:
      - 28361:8080
    environment:
      DUMMY: false
    {% if (X4_SERVER_db_type | lower) == 'postgres' %}
      DATABASE_MODE: postgresql
      DATABASE_USER: postgres
      DATABASE_PASSWORD: postgres
      DATABASE_HOST: postgressql
      DATABASE_PORT: 5432
    {% endif -%}   {#  X4_SERVER_db_type = postgres #}
    {% if (X4_SERVER_db_type | lower) == 'mssql' %}
      DATABASE_MODE: mssql
      DATABASE_USER: admin
      DATABASE_PASSWORD: abc123abc123!
      DATABASE_HOST: sqlserver
      DATABASE_PORT: 5432
    {% endif -%}   {#  X4_SERVER_db_type = mssql #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./licenses/x4-server/x4.license:/opt/X4/x4.license  # X4 license binding, you should place a valid license.
      - ./conf/x4-server:/opt/X4/configurations             # Configuration binding for keycloak.
#      - "./logs:/opt/X4/wildfly/standalone/log" # Logs binding.
#      - "./X4Config.xml:/opt/X4/X4config.xml" # X4config binding, you would need to place the file before starting.
#      - "./deployments:/opt/X4/deployments" # Deployments binding, in case you want to add x4apps there.
#      - "./X4DB/1:/opt/X4/X4DB/1" # Projects binding.
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  x4-keycloak:
    image: softproject/keycloak
    container_name: x4-keycloak
    hostname: x4-keycloak
    labels:
      com.platys.name: "keycloak"
      com.platys.description: "Identity and Access Management"
      com.platys.webui.title: "Keycloak UI"
      com.platys.webui.url: "https://dataplatform:28362"
    ports:
      - "28362:8085"
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=demo
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: "exec 3<>/dev/tcp/localhost/9000 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | grep -q '200 OK'"
      interval: 5s
      timeout: 2s
      retries: 15    
{% endif %}   {#  X4_SERVER_enable #}

{% if IOEVENT_COCKPIT_enable | default(false) %}
  #  ================================== IOEvent Cockpit ========================================== #
  ioeventui:
    image: ioevent/cockpit-ui:{{__IOEVENT_COCKPIT_version}}
    container_name: ioeventui
    hostname: ioeventui
    labels:
      com.platys.name: "ioevent-cockpit"
      com.platys.description: "Event-driven microservices Framework"
      com.platys.webui.title: "IOEvent Cockpit UI"
      com.platys.webui.url: "http://dataplatform:28327"
    depends_on:
      - ioeventapi
    ports:
      - 28327:3000
    environment:
      COCKPit_API_URL: 'http://${PUBLIC_IP}:8761/api'
      COCKPit_UI_REGISTRATION: "true#####"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  ioeventapi:
    image: ioevent/cockpit-api:{{__IOEVENT_COCKPIT_version}}
    container_name: ioeventapi
    hostname: ioeventapi
    labels:
      com.platys.name: "ioevent-cockpit"
      com.platys.description: "Event-driven microservices Framework"
      com.platys.restapi.title: "IOEvent Cockpit API"
      com.platys.restapi.url: "http://dataplatform:8761"
    depends_on:
      - kafka-1
      - elasticsearch-1
    ports:
      - 8761:8761
    environment:
      ES_URL: 'http://elasticsearch-1:9200'
      #ES_USERNAME:
      #ES_PASSWORD:
      KAFKA_BOOTSTRAP_SERVER: '{{ns.bootstrapServers}}'
    {% if ns.secureKafkaEnabled %}
      KAFKA_SASL_JAAS_USERNAME: {{ns.kafkaToolsDefaultUsername}}
      KAFKA_SASL_JAAS_PASSWORD: {{ns.kafkaToolsDefaultPassword}}
    {% endif -%}   {#  ns.secureKafkaEnabled #}
      ENABLE_MAIL_SERVICE: false
      DEFAULT_ENABLE_USERS: true
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  IOEVENT_COCKPIT_enable #}

{% if PENTHAO_enable | default(false) %}
  #  ================================== Penthao Webspoon ========================================== #
  penthao:
    image: hiromuhota/webspoon:{{__PENTHAO_version}}
    container_name: penthao
    hostname: penthao
    labels:
      com.platys.name: "penthao"
      com.platys.description: "Data Integration Engine"
      com.platys.webui.title: "Penthao UI"
      com.platys.webui.url: "http://dataplatform:28154"
    ports:
      - '28154:8080'
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  PENTHAO_enable #}

{% if DBT_enable | default(false) %}
  #  ================================== DBT (Data Build Tool) ========================================== #
  dbt:
    image: ghcr.io/dbt-labs/dbt-spark:{{__DBT_version}}
    container_name: dbt
    hostname: dbt
    labels:
      com.platys.name: "dbt"
      com.platys.description: "Data transformation builder"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./data-transfer/dbt:/usr/app
      - ./custom-conf/dbt/profiles.yml:/root/.dbt/
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done
    restart: {{container_restart_policy}}
{% endif %}   {#  DBT_enable #}

{% if CODE_SERVER_enable | default(false) %}
  #  ================================== Code-Server IDE (VS Code in Browser) ================= #
  code-server:
    image: trivadis/code-server:{{__CODE_SERVER_version}}
    container_name: code-server
    hostname: code-server
    labels:
      com.platys.name: "code-server"
      com.platys.description: "Code Editor (VS Code in Browser)"
      com.platys.webui.title: "Code-Server UI"
      com.platys.webui.url: "http://dataplatform:28140"
    user: 0:0
    ports:
      - 28140:8080
    environment:
      PASSWORD: 'abc123!'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
    {% if CODE_SERVER_volume_map_platform_root %}
      - ../:/home/coder/platform-root:Z
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# CODE_SERVER_enable #}

{% if TAIGA_enable | default(false) %}
  #  ================================== Taiga ================= #
  taiga-front:
    image: taigaio/taiga-front:{{__TAIGA_version}}
    container_name: taiga-front
    hostname: taiga-front
    labels:
      com.platys.name: "taiga"
      com.platys.description: "Project management for cross-functional agile teams"
      com.platys.webui.title: "Taiga UI"
      com.platys.webui.url: "http://dataplatform:28323"
    ports:
      - 28323:80
    environment:
      TAIGA_URL: "http://${PUBLIC_IP}:28324"
      TAIGA_WEBSOCKETS_URL: "ws://${PUBLIC_IP}:28324"
      TAIGA_SUBPATH: ""
      PUBLIC_REGISTER_ENABLED: false      
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-back:
    image: taigaio/taiga-back:{{__TAIGA_version}}
    container_name: taiga-back
    hostname: taiga-back
    labels:
      com.platys.name: "taiga"
      com.platys.description: "Project management for cross-functional agile teams"
    environment:
      # Taiga settings
      TAIGA_LANG: "en_us"
      TAIGA_SECRET_KEY: "{{TAIGA_secret_key}}"
      TAIGA_SITES_SCHEME: "http"
      TAIGA_SITES_DOMAIN: "${PUBLIC_IP}:28324"
      TAIGA_SUBPATH: ""
      # Email settings.
      EMAIL_BACKEND: {{TAIGA_email_backend}}
      DEFAULT_FROM_EMAIL: {{TAIGA_email_from}}
      EMAIL_USE_TLS: false
      EMAIL_USE_SSL: false
      EMAIL_HOST: {{TAIGA_email_host}}
      EMAIL_PORT: {{TAIGA_email_port}}
      EMAIL_HOST_USER: {{TAIGA_email_host_user}}
      EMAIL_HOST_PASSWORD: {{TAIGA_email_host_password}}
      # Database settings
      POSTGRES_HOST: taiga-db
      POSTGRES_DB: {{TAIGA_db_database}}
      POSTGRES_USER: {{TAIGA_db_username}}
      POSTGRES_PASSWORD: ${PLATYS_TAIGA_DB_PASSWORD:-{{TAIGA_db_password}}}
      # Rabbitmq settings
      RABBITMQ_USER: {{TAIGA_rabbitmq_user}} 
      RABBITMQ_PASS: ${PLATYS_TAIGA_RABBITMQ_PASSWORD:-{{TAIGA_rabbitmq_password}}}
      # Telemetry settings
      ENABLE_TELEMETRY: {{TAIGA_enable_telemetry}}
      PUBLIC_REGISTER_ENABLED: "False"      
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
      - ./data-transfer:/home/coder/data-transfer
      - ./container-volume/taiga/static-data:/taiga-back/static
      - ./container-volume/taiga/media-data:/taiga-back/media
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-async:
    image: taigaio/taiga-back:{{__TAIGA_version}}
    container_name: taiga-async
    hostname: taiga-async
    entrypoint: ["/taiga-back/docker/async_entrypoint.sh"]
    labels:
      com.platys.name: "taiga"
      com.platys.description: "Project management for cross-functional agile teams"
    environment:
      # Taiga settings
      TAIGA_LANG: "en_us"
      TAIGA_SECRET_KEY: "{{TAIGA_secret_key}}"
      TAIGA_SITES_SCHEME: "http"
      TAIGA_SITES_DOMAIN: "${PUBLIC_IP}:28324"
      TAIGA_SUBPATH: ""
      # Email settings.
      EMAIL_BACKEND: {{TAIGA_email_backend}}
      DEFAULT_FROM_EMAIL: {{TAIGA_email_from}}
      EMAIL_USE_TLS: false
      EMAIL_USE_SSL: false
      EMAIL_HOST: {{TAIGA_email_host}}
      EMAIL_PORT: {{TAIGA_email_port}}
      EMAIL_HOST_USER: {{TAIGA_email_host_user}}
      EMAIL_HOST_PASSWORD: {{TAIGA_email_host_password}}
      # Database settings
      POSTGRES_HOST: taiga-db
      POSTGRES_DB: {{TAIGA_db_database}}
      POSTGRES_USER: {{TAIGA_db_username}}
      POSTGRES_PASSWORD: ${PLATYS_TAIGA_DB_PASSWORD:-{{TAIGA_db_password}}}
      # Rabbitmq settings
      RABBITMQ_USER: {{TAIGA_rabbitmq_user}} 
      RABBITMQ_PASS: ${PLATYS_TAIGA_RABBITMQ_PASSWORD:-{{TAIGA_rabbitmq_password}}}
      # Telemetry settings
      ENABLE_TELEMETRY: {{TAIGA_enable_telemetry}}
      PUBLIC_REGISTER_ENABLED: "False"         
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
      - ./data-transfer:/home/coder/data-transfer
      - ./container-volume/taiga/static-data:/taiga-back/static
      - ./container-volume/taiga/media-data:/taiga-back/media
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-async-rabbitmq:
    image: rabbitmq:3.8-management-alpine
    container_name: taiga-async-rabbitmq
    hostname: taiga-async-rabbitmq    
    labels:
      com.platys.name: "rabbitmq"
      com.platys.description: "Taiga Message Broker"
      com.platys.password.envvars: "PLATYS_TAIGA_RABBITMQ_PASSWORD"    
    environment:
      RABBITMQ_ERLANG_COOKIE: "62cd4092-dbb3-4e7a-ac6d-ffd7cee850ee"
      RABBITMQ_DEFAULT_USER: {{TAIGA_rabbitmq_user}} 
      RABBITMQ_DEFAULT_PASS: ${PLATYS_TAIGA_RABBITMQ_PASSWORD:-{{TAIGA_rabbitmq_password}}}
      RABBITMQ_DEFAULT_VHOST: "taiga"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}      
    volumes:
      - ./data-transfer:/home/coder/data-transfer
      #- taiga-async-rabbitmq-data:/var/lib/rabbitmq
    {% if  TAIGA_volume_map_data | default(false) %}
      - ./container-volume/taiga/async-rabbitmq-data:/var/lib/rabbitmq
    {% endif -%}   {#  TAIGA_volume_map_data #}      
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-events:
    image: taigaio/taiga-events:{{__TAIGA_version}}
    container_name: taiga-events
    hostname: taiga-events
    labels:
      com.platys.name: "taiga"
      com.platys.description: ""    
    environment:
      RABBITMQ_USER: "taiga"
      RABBITMQ_PASS: "abc123!"
      TAIGA_SECRET_KEY: "{{TAIGA_secret_key}}"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}            
    volumes:
      - ./data-transfer:/home/coder/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-events-rabbitmq:
    image: rabbitmq:3.8-management-alpine
    container_name: taiga-events-rabbitmq
    hostname: taiga-events-rabbitmq
    labels:
      com.platys.name: "rabbitmq"
      com.platys.description: "Taiga Message Broker"
      com.platys.password.envvars: "PLATYS_TAIGA_RABBITMQ_PASSWORD"    
    environment:
      RABBITMQ_ERLANG_COOKIE: "62cd4092-dbb3-4e7a-ac6d-ffd7cee850ee"
      RABBITMQ_DEFAULT_USER: {{TAIGA_rabbitmq_user}} 
      RABBITMQ_DEFAULT_PASS: ${PLATYS_TAIGA_RABBITMQ_PASSWORD:-{{TAIGA_rabbitmq_password}}}
      RABBITMQ_DEFAULT_VHOST: "taiga"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}            
    volumes:
      - ./data-transfer:/home/coder/data-transfer    
    {% if  TAIGA_volume_map_data | default(false) %}
      - ./container-volume/taiga/events-rabbitmq-data:/var/lib/rabbitmq
    {% endif -%}   {#  TAIGA_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-protected:
    image: taigaio/taiga-protected:latest
    container_name: taiga-protected
    hostname: taiga-protected
    labels:
      com.platys.name: "taiga"
      com.platys.description: ""
    environment:
      MAX_AGE: "360"
      SECRET_KEY: "{{TAIGA_secret_key}}"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}            
    volumes:
      - ./data-transfer:/home/coder/data-transfer    
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-gateway:
    image: nginx:1.19-alpine
    container_name: taiga-gateway
    hostname: taiga-gateway
    labels:
      com.platys.name: "taiga"
      com.platys.description: "Taiga Gateway"
    ports:
      - "28324:80"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
      - ./conf/taiga/taiga-gateway/taiga.conf:/etc/nginx/conf.d/default.conf
      - ./container-volume/taiga/static-data:/taiga/static
      - ./container-volume/taiga/media-data:/taiga/media
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taiga-db:
    image: postgres:15
    container_name: taiga-db
    hostname: taiga-db
    labels:
      com.platys.name: "taiga"
      com.platys.description: "Relational Database"
      com.platys.password.envvars: "PLATYS_TAIGA_DB_PASSWORD"
    environment:
      POSTGRES_DB: "{{TAIGA_db_database}}"
      POSTGRES_USER: "{{TAIGA_db_username}}"
      POSTGRES_PASSWORD: ${PLATYS_TAIGA_DB_PASSWORD:-{{TAIGA_db_password}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
    {% if  TAIGA_volume_map_data | default(false) %}
      - ./container-volume/taiga/db-data:/var/lib/postgresql/data
    {% endif -%}   {#  TAIGA_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U {{TAIGA_db_username}} -d {{TAIGA_db_database}}"]
      interval: 2s
      timeout: 15s
      retries: 5

{% endif %}   {# TAIGA_enable #}

{% if TASKCAFE_enable | default(false) %}
  #  ================================== Taskcafe ========================================== #
  taskcafe:
    image: taskcafe/taskcafe:{{__TASKCAFE_version}}
    container_name: taskcafe
    hostname: taskcafe
    labels:
      com.platys.name: "taskcafe"
      com.platys.description: "Project management tool with Kanban boards "
      com.platys.webui.title: "Taskcafe UI"
      com.platys.webui.url: "http://dataplatform:3333"
    ports:
      - 3333:3333
    depends_on:
      - taskcafe-db
    environment:
      TASKCAFE_DATABASE_HOST: taskcafe-db
      TASKCAFE_DATABASE_USER: taskcafe
      TASKCAFE_DATABASE_PASSWORD: abc123!
      TASKCAFE_DATABASE_NAME: taskcafedb
      TASKCAFE_MIGRATE: "true"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  taskcafe-db:
    image: postgres:12.3-alpine
    container_name: taskcafe-db
    hostname: taskcafe-db
    labels:
      com.platys.name: "postgres"
      com.platys.description: "Relational Database"
    environment:
      POSTGRES_USER: taskcafe
      POSTGRES_PASSWORD: abc123!
      POSTGRES_DB: taskcafedb
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if  TASKCAFE_volume_map_db_data | default(false) %}
      - ./container-volume/taskcafe/db-data:/var/lib/postgresql/data
    {% endif -%}   {#  TASKCAFE_volume_map_db_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# MOCK_SERVER_enable #}

{% if FOCALBOARD_enable | default(false) %}
  #  ================================== Focalboard ========================================== #
  focalboard:
    image: mattermost/focalboard:{{__FOCALBOARD_version}}
    container_name: focalboard
    hostname: focalboard
    labels:
      com.platys.name: "focalboard"
      com.platys.description: "Project management tool"
      com.platys.webui.title: "Focalboard UI"
      com.platys.webui.url: "http://dataplatform:28413"
    ports:
      - 28413:8000
    environment:
      - VIRTUAL_HOST=focalboard.local
      - VIRTUAL_PORT=8000
      - FOCALBOARD_SINGLE_USER_TOKEN=abc123!
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if  FOCALBOARD_volume_map_db_data | default(false) %}
      - ./container-volume/focalboard/data:/opt/focalboard/data
    {% endif -%}   {#  FOCALBOARD_volume_map_db_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# FOCALBOARD_enable #}

{% if MOCK_SERVER_enable | default(false) %}
  #  ================================== MockServer ========================================== #
  mockserver:
    image: mockserver/mockserver:{{__MOCK_SERVER_version}}
    container_name: mockserver
    hostname: mockserver
    labels:
      com.platys.name: "mockserver"
      com.platys.description: "Easy mocking of any system you integrate with via HTTP or HTTPs"
      com.platys.webui.title: "MockServer UI"
      com.platys.webui.url: "http://dataplatform:28273/mockserver/dashboard"
    ports:
      - 28273:1090
    environment:
      MOCKSERVER_SERVER_PORT: 1090
      MOCKSERVER_LOG_LEVEL: {{MOCK_SERVER_log_level | default(DEBUG)}}
    {% if MOCK_SERVER_persist_expectations | default(false) %}
      MOCKSERVER_PERSIST_EXPECTATIONS: {{MOCK_SERVER_persist_expectations}}
      MOCKSERVER_PERSISTED_EXPECTATIONS_PATH: {{MOCK_SERVER_persisted_expecations_path}}
    {% endif -%}   {#  MOCK_SERVER_persist_expectations #}
    {% if MOCK_SERVER_initialization_json_file is defined and MOCK_SERVER_initialization_json_file|length %}
      MOCKSERVER_INITIALIZATION_JSON_PATH: /config/{{MOCK_SERVER_initialization_json_file}}
    {% endif -%}   {#  MOCK_SERVER_initialization_json_file #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock
    {% if MOCK_SERVER_initialization_json_file is defined and MOCK_SERVER_initialization_json_file|length %}
      - /scripts/mockserver/:/config
    {% endif -%}   {#  MOCK_SERVER_initialization_json_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: --admin-password '$$2y$$05$$NrPTXkUOIHTTbdHUqdAZVuSbncaZ9frWZYXDbA4v/WYqY0nAY1Sui'
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# MOCK_SERVER_enable #}

{% if EXCALIDRAW_enable | default(false) %}
  #  ================================== Excalidraw ================= #
  excalidraw:
    image: excalidraw/excalidraw:{{__EXCALIDRAW_version}}
    container_name: excalidraw
    hostname: excalidraw
    labels:
      com.platys.name: "excalidraw"
      com.platys.description: "Virtual whiteboard for sketching hand-drawn like diagrams"
      com.platys.webui.title: "Excalidraw UI"
      com.platys.webui.url: "http://dataplatform:28290"
    ports:
      - 28290:80
    stdin_open: true
    healthcheck:
      disable: true
    environment:
      NODE_ENV: 'development'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: unless-stopped
{% endif %}   {# EXCALIDRAW_enable #}

{% if DRAWIO_enable | default(false) %}
  #  ================================== draw.io ================= #
  drawio:
    image: jgraph/drawio:{{__DRAWIO_version}}
    container_name: drawio
    hostname: drawio
    labels:
      com.platys.name: "drawio"
      com.platys.description: "Client-side editor for general diagramming"
      com.platys.webui.title: "Draw.io UI"
      com.platys.webui.url: "http://dataplatform:28390"
    ports:
      - 28390:8080
      - 28391:8443
    environment:
      LETS_ENCRYPT_ENABLED: false
      PUBLIC_DNS: domain
      ORGANISATION_UNIT: unit
      ORGANISATION: org
      CITY: city
      STATE: state
      COUNTRY_CODE: country
      EXPORT_URL: http://drawio-export:8000/
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://drawio:8080 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 5
      start_period: 10s    

  drawio-export:
    image: jgraph/export-server
    container_name: drawio-export
    hostname: drawio-export
    labels:
      com.platys.name: "drawio"
      com.platys.description: "Client-side editor for general diagramming (exporter)"
    expose:
      - "8000"
    volumes:
      - ./scripts/drawio/fonts:/usr/share/fonts/drawio
    environment:
      DRAWIO_BASE_URL: ${PUBLIC_IP}      
{% endif %}   {# DRAWIO_enable #}

{% if FIREFOX_enable | default(false) %}
  #  ================================== Firefox Browser ================= #
  firefox:
    image: jlesage/firefox:{{__FIREFOX_version}}
    container_name: firefox
    hostname: firefox
    labels:
      com.platys.name: "firefox"
      com.platys.description: "Web-Browser"
      com.platys.webui.title: "Firefox UI"
      com.platys.webui.url: "http://dataplatform:{{80 if (FIREFOX_use_port_80 | default(true) )  else 5801 }}"
    ports:
      - "{{80 if (FIREFOX_use_port_80 | default(true) )  else 5801 }}:5800"
    shm_size: 2g
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/home/coder/data-transfer
      - /container-volume/firefox:/config:rw
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: unless-stopped
{% endif %}   {# CODE_SERVER_enable #}

{% if FILE_BROWSER_enable | default(false) %}
  #  ================================== File Browser ================= #
  filebrowser:
    image: hurlenko/filebrowser:{{__FILE_BROWSER_version}}
    container_name: filebrowser
    hostname: filebrowser
    labels:
      com.platys.name: "filebrowser"
      com.platys.description: "File-Browser"
      com.platys.webui.title: "File Browser UI"
      com.platys.webui.url: "http://dataplatform:28178/filebrowser"
      com.platys.password.envvars: "PLATYS_FILEBROWSER_PASSWORD_HASH10"
    ports:
      - 28178:8080
    environment:
      - FB_BASEURL=/filebrowser
      - FB_USERNAME={{FILE_BROWSER_user if FILE_BROWSER_user is defined and FILE_BROWSER_user and FILE_BROWSER_user | length else omit}}
      - FB_PASSWORD=${PLATYS_FILEBROWSER_PASSWORD_HASH10:-{{FILE_BROWSER_password if FILE_BROWSER_password is defined and FILE_BROWSER_password and FILE_BROWSER_password | length else omit}}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      # we map the data-transfer folder to /data so that Filebrowser "sits on top" of the data-transfer folder
      - ./data-transfer:/data
    {% if FILE_BROWSER_volume_map_data %}
      - ./container-volume/filebrowser/data:/config
    {% endif -%}   {#  FILE_BROWSER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    user: "{{uid | default(1000)}}:{{uid | default(1000)}}"
    restart: unless-stopped
{% endif %}   {# FILE_BROWSER_enable #}

{% if VAULT_enable | default(false) %}
  #  ================================== Vault ================= #
  vault:
    image: hashicorp/vault:{{__VAULT_version}}
    container_name: vault
    hostname: vault
    labels:
      com.platys.name: "vault"
      com.platys.description: "Secrets management, encryption as a service, and privileged access management"
      com.platys.webui.title: "Vault UI"
      com.platys.webui.url: "http://dataplatform:8200"
    ports:
      - 8200:8200
    environment:
      - VAULT_ADDR=http://0.0.0.0:8200
#      - VAULT_LOCAL_CONFIG='{"listener": [{"tcp":{"address": "0.0.0.0:8200","tls_disable":"0", "tls_cert_file":"/data/vault-volume/certificate.pem", "tls_key_file":"/data/vault-volume/key.pem"}}], "default_lease_ttl": "168h", "max_lease_ttl": "720h"}, "ui": true}'
      - VAULT_API_ADDR=http://0.0.0.0:8200
      - VAULT_ADDRESS=http://0.0.0.0:8200
    {% if VAULT_use_dev_mode | default(false) %}
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
      - VAULT_DEV_ROOT_TOKEN_ID={{VAULT_dev_mode_token}}
    {% endif -%}   {#  VAULT_use_dev_mode #}
      - VAULT_UI=true
      - VAULT_LOG_LEVEL=info
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/vault/config:/vault/config
    {% if VAULT_volume_map_data %}
      - "./container-volume/vault/data:/vault/data"
    {% endif -%}   {#  VAULT_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    cap_add:
      - IPC_LOCK
    command: vault server {{"-dev" if VAULT_use_dev_mode else "-config=/vault/config/vault.json"}}
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://vault:8200/v1/sys/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  {% if VAULT_username is defined and VAULT_username and VAULT_username | length | default(false) %}
  vault-init:
    image: hashicorp/vault:{{__VAULT_version}}
    container_name: vault-init
    hostname: vault-init
    labels:
      com.platys.password.envvars: "PLATYS_VAULT_PASSWORD"            
    depends_on:    
      vault:
        condition: service_healthy    
    environment:
      - VAULT_ADDR=http://vault:8200
      - VAULT_TOKEN={{VAULT_dev_mode_token}}   
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./security/vault/read-write-policy.hcl:/tmp/read-write-policy.hcl
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}    
    command: 
      - sh
      - -c
      - |
        vault policy write read-write-policy /tmp/read-write-policy.hcl
        vault auth disable userpass/
        vault auth enable userpass
        vault write auth/userpass/users/{{VAULT_username | default("admin")}} password=${PLATYS_VAULT_PASSWORD:-{{VAULT_password}}} policies=read-write-policy
    init: true
    restart: "no"    
  {% endif %}   {#  VAULT_enable #}
{% endif %}   {#  VAULT_enable #}

{% if KEYCLOAK_enable | default(false) %}
  #  ================================== Keycloak ================= #
  keycloak:
    image: keycloak/keycloak:{{__KEYCLOAK_version}}
    hostname: keycloak
    container_name: keycloak
    labels:
      com.platys.name: "keycloak"
      com.platys.description: "Identity and Access Management"
      com.platys.webui.title: "Keycloak UI"
      com.platys.webui.url: "https://dataplatform:28205"
    ports:
      - "28204:28204"
      - "28205:8443"
    environment:
      KEYCLOAK_ADMIN: "admin"
      KEYCLOAK_ADMIN_PASSWORD: "abc123!"
      KC_HTTP_ENABLED: 'true'
      KC_LOG_LEVEL: "INFO"
      KC_HEALTH_ENABLED: 'true'
      KC_METRICS_ENABLED: 'true'
      KC_FEATURES: '{{KEYCLOAK_features}}'
      KC_HTTPS_KEY_STORE_FILE: '/opt/keycloak/conf/server.keystore'
      KC_HTTPS_KEY_STORE_PASSWORD: 'password'
      KC_HOSTNAME_STRICT: 'false'
    {%if (KEYCLOAK_db_vendor | lower) == 'postgres' | default(false) %}
      KC_DB_URL: jdbc:postgresql://postgres/keycloakdb
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: abc123!
    {% endif -%}   {#  KEYCLOAK_db_vendor #}
    {%if (KEYCLOAK_db_vendor | lower) == 'mysql' | default(false) %}
      KC_DB_URL: jdbc:mysql://mysql/keycloakdb
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: abc123!
    {% endif -%}   {#  KEYCLOAK_db_vendor #}
    {%if (KEYCLOAK_db_vendor | lower) == 'mssql' | default(false) %}
      KC_DB_URL: jdbc:sqlserver://sqlserver;database=keycloak
      KC_DB_USERNAME: sa
      KC_DB_PASSWORD: abc123!
    {% endif -%}   {#  KEYCLOAK_db_vendor #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./security/keycloak/server.keystore:/opt/keycloak/conf/server.keystore
    {% if (SCHEMA_REGISTRY_enable and SCHEMA_REGISTRY_flavour | lower) == 'apicurio' | default(false) %}
      - ./security/apicurio-registry/keycloak/apicurio-realm{{"-with-users" if APICURIO_auth_import_default_users}}.json:/opt/keycloak/data/import/apicurio-realm.json
    {% endif -%}
    {% if KAFKA_enable and ns.secureKafkaEnabled %}
      - ./security/kafka/keycloak/kafka-authbearer.json:/opt/keycloak/data/import/kafka-realm.json
    {% endif -%}   {#  KEYCLOAK_db_vendor #}
    {% if DATAVERSE_enable %}
      - ./security/dataverse/keycloak/dataverse-realm.json:/opt/keycloak/data/import/dataverse-realm.json
    {% endif -%}   {#  KEYCLOAK_db_vendor #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    networks:
      default:
        aliases:
          - keycloak.platysplatform.io   #create a DNS alias within the network (add the same alias to your /etc/hosts to get a working OIDC flow)
    command:
      - start-dev
    {%if KEYCLOAK_import_realm_enable | default(false) %}
      - --import-realm
      - --http-port=28204   # change port to 8090, so within the network and external the same port is used
    {% endif -%}   {#  KEYCLOAK_import_realm_enable #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: "exec 3<>/dev/tcp/localhost/9000 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | grep -q '200 OK'"
      interval: 5s
      timeout: 2s
      retries: 15    
{% endif %}   {#  KEYCLOAK_enable #}

{% if AUTHELIA_enable | default(false) %}
  #  ================================== Authelia ================= #
  authelia:
    image: authelia/authelia:{{__AUTHELIA_version}}
    hostname: authelia
    container_name: authelia
    labels:
      com.platys.name: "authelia"
      com.platys.description: "Identity and Access Management"
      com.platys.webui.title: "Authelia UI"
      com.platys.webui.url: "https://dataplatform:28399"
    ports:
      - "28399:9091"
    environment:
      X_AUTHELIA_CONFIG_FILTERS: template
      X_AUTHELIA_HEALTHCHECK: "1"
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/authelia/configuration.yml:/config/configuration.yml
      - ./security/authelia/key.pem:/config/key.pem
      - ./security/authelia/users.yml:/config/users.yml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: "healthcheck.sh"
      interval: 5s
      timeout: 2s
      retries: 15    
{% endif %}   {#  AUTHELIA_enable #}

{% if POSTMAN_enable | default(false) %}
  #  ================================== Postman ================= #
  postman:
    image: quay.io/microcks/microcks-postman-runtime:{{__POSTMAN_version}}
    container_name: postman
    hostname: postman
    labels:
      com.platys.name: "postman"
      com.platys.description: "API platform for building and using APIs"
      com.platys.webui.title: "Postman UI"
      com.platys.webui.url: "http://dataplatform:3010"
    ports:
      - 3010:3000
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  POSTMAN_enable #}

{% if PACT_BROKER_enable | default(false) %}
  #  ================================== Postman ================= #
  pact-broker:
    image: pactfoundation/pact-broker:{{__PACT_BROKER_version}}
    container_name: pact-broker
    hostname: pact-broker
    labels:
      com.platys.name: "pact-broker"
      com.platys.description: "Consumer driven contracts workflow "
      com.platys.webui.title: "Pact Broker UI"
      com.platys.webui.url: "http://dataplatform:3010"
    ports:
      - 9292:9292
    environment:
      - PACT_BROKER_PORT='9292'
      - PACT_BROKER_BASIC_AUTH_USERNAME=pact
      - PACT_BROKER_BASIC_AUTH_PASSWORD=abc123!
      - PACT_BROKER_DATABASE_URL="postgres://postgres:password@postgres/postgres"
      - PACT_BROKER_LOG_LEVEL=INFO
      - PACT_BROKER_SQL_LOG_LEVEL=DEBUG
      - PACT_BROKER_DATABASE_CONNECT_MAX_RETRIES="5"
      - PACT_BROKER_BASE_URL='https://localhost http://localhost http://localhost:9292 http://pact-broker:9292 https://host.docker.internal http://host.docker.internal http://host.docker.internal:9292'
      - PACT_BROKER_PUBLIC_HEARTBEAT=true
      - PACT_BROKER_WEBHOOK_SCHEME_WHITELIST=http
      - PACT_BROKER_WEBHOOK_HOST_WHITELIST=host.docker.internal
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {#  PACT_BROKER_enable #}

{% if SWAGGER_EDITOR_enable | default(false) %}
  #  ================================== Swagger Editor ================= #
  swagger-editor:
    image: swaggerapi/swagger-editor:{{__SWAGGER_EDITOR_version}}
    container_name: swagger-editor
    hostname: swagger-editor
    labels:
      com.platys.name: "swagger-editor"
      com.platys.description: "OpenAPI API definitions editor"
      com.platys.webui.title: "Swagger Editor UI"
      com.platys.webui.url: "http://dataplatform:28156"
    ports:
      - 28156:8080
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SWAGGER_EDITOR_enable #}

{% if SWAGGER_UI_enable | default(false) %}
  #  ================================== Swagger UI ================= #
  swagger-ui:
    image: swaggerapi/swagger-ui:{{__SWAGGER_EDITOR_version}}
    container_name: swagger-ui
    hostname: swagger-ui
    labels:
      com.platys.name: "swagger-ui"
      com.platys.description: "Dynamically generate  documentation from a Swagger-compliant API"
      com.platys.webui.title: "Swagger UI"
      com.platys.webui.url: "http://dataplatform:28157"
    ports:
      - 28157:8080
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  SWAGGER_UI_enable #}

{% if ASYNCAPI_STUDIO_enable | default(false) %}
  #  ================================== AsyncAPI Studio ================= #
  asyncapi-studio:
    image: asyncapi/studio:{{__ASYNCAPI_STUDIO_version}}
    container_name: asyncapi-studio
    hostname: asyncapi-studio
    labels:
      com.platys.name: "asyncapi-studio"
      com.platys.description: "Visually design  AsyncAPI files and event-driven architecture"
      com.platys.webui.title: "AsyncAPI Studio UI"
      com.platys.webui.url: "http://dataplatform:28322"
    ports:
      - 28322:80
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  ASYNCAPI_STUDIO_enable #}

{% if DATA_MESH_MANAGER_enable | default(false) %}
  #  ================================== Data Mesh Manager ================= #
  data-mesh-manager:
    image: datameshmanager/datamesh-manager-ce:{{__DATA_MESH_MANAGER_version}}
    container_name: data-mesh-manager
    hostname: data-mesh-manager
    labels:
      com.platys.name: "data-mesh-manager"
      com.platys.description: "Data Mesh Manager"
      com.platys.webui.title: "Data Mesh Manager UI"
      com.platys.webui.url: "http://dataplatform:28381"      
    ports:
      - 28381:8080
    environment:
      - APPLICATION_HOST_WEB=http://${PUBLIC_IP}:28381
      - SPRING_DATASOURCE_URL=jdbc:postgresql://data-mesh-manager-postgresql:5432/data_mesh_mgr_db
      - SPRING_DATASOURCE_USERNAME={{DATA_MESH_MANAGER_postgres_username}}
      - SPRING_DATASOURCE_PASSWORD=${PLATYS_DATA_MESH_MANAGER_POSTGRESQL_PASSWORD:-{{DATA_MESH_MANAGER_postgres_password}}}
      - SPRING_MAIL_HOST={{DATA_MESH_MANAGER_mail_host}}
      - SPRING_MAIL_PORT={{DATA_MESH_MANAGER_mail_port}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  data-mesh-manager-postgresql:
    image: pgvector/pgvector:pg16
    container_name: data-mesh-manager-postgresql
    hostname: data-mesh-manager-postgresql
    labels:
      com.platys.name: "postgresql"
      com.platys.description: "Data Mesh Manager Postgresql"
      com.platys.password.envvars: "PLATYS_DATA_MESH_MANAGER_POSTGRESQL_PASSWORD"    
    environment:
      - POSTGRES_DB=data_mesh_mgr_db
      - POSTGRES_USER={{DATA_MESH_MANAGER_postgres_username}}
      - POSTGRES_PASSWORD=${PLATYS_DATA_MESH_MANAGER_POSTGRESQL_PASSWORD:-{{DATA_MESH_MANAGER_postgres_password}}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer    
    {% if DATA_MESH_MANAGER_volume_map_data %}      
      - ./container-volume/data-mesh-manager/postgresql-data:/var/lib/postgresql/data
    {% endif -%}   {#  DATA_MESH_MANAGER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DATA_MESH_MANAGER_enable #}

{% if DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_enable | default(false) %}
  #  ================================== Data Mesh Manager Connector Databricks ================= #
  datamesh-manager-connector-databricks:
    image: datameshmanager/datamesh-manager-connector-databricks:{{__DATA_MESH_MANAGER_version}}
    container_name: datamesh-manager-connector-databricks
    hostname: datamesh-manager-connector-databricks
    labels:
      com.platys.name: "data-mesh-manager"
      com.platys.description: "Data Mesh Manager Connector Databricks"
      com.platys.password.envvars: "PLATYS_DATA_MESH_MANAGER_POSTGRESQL_PASSWORD"    
    environment:
      - DATAMESHMANAGER_CLIENT_HOST=http://data-mesh-manager:8080
      - DATAMESHMANAGER_CLIENT_APIKEY=
      - DATAMESHMANAGER_CLIENT_DATABRICKS_WORKSPACE_HOST={{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_workspace_host}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_WORKSPACE_CLIENTID={{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_workspace_clientid}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_WORKSPACE_CLIENTSECRET=${PLATYS_DATA_MESH_MANAGER_POSTGRESQL_PASSWORD:-{{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_workspace_clientsecret}}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_ACCOUNT_HOST={{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_account_host}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_ACCOUNT_ACCOUNTID={{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_account_accountid}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_ACCOUNT_CLIENTID={{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_account_clientid}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_ACCOUNT_CLIENTSECRET=${PLATYS_DATA_MESH_MANAGER_POSTGRESQL_PASSWORD:-{{DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_account_clientsecret}}}
      - DATAMESHMANAGER_CLIENT_DATABRICKS_ASSETS_POLLINTERVAL=PT10M
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer    
    {% if DATA_MESH_MANAGER_volume_map_data %}      
      - ./container-volume/data-mesh-manager/postgresql-data:/var/lib/postgresql/data
    {% endif -%}   {#  DATA_MESH_MANAGER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}    
{% endif %}   {#  DATA_MESH_MANAGER_CONNECTOR_DATABRICKS_enable #}

{% if DATA_CONTRACT_MANAGER_enable | default(false) %}
  #  ================================== Data Contract Manager ================= #
  data-contract-manager:
    image: datacontractmanager/datacontract-manager-ce:{{__DATA_CONTRACT_MANAGER_version}}
    container_name: data-contract-manager
    hostname: data-contract-manager
    labels:
      com.platys.name: "data-contract-manager"
      com.platys.description: "Data Contract Manager"
      com.platys.webui.title: "Data Contract Manager UI"
      com.platys.webui.url: "http://dataplatform:28382"      
    ports:
      - 28382:8080
    environment:
      - APPLICATION_HOST_WEB=http://${PUBLIC_IP}:28382
      - SPRING_DATASOURCE_URL=jdbc:postgresql://data-contract-manager-postgresql:5432/data_contract_mgr_db
      - SPRING_DATASOURCE_USERNAME={{DATA_CONTRACT_MANAGER_postgres_username}}
      - SPRING_DATASOURCE_PASSWORD=${PLATYS_DATA_CONTRACT_MANAGER_POSTGRESQL_PASSWORD:-{{DATA_CONTRACT_MANAGER_postgres_password}}}
      - SPRING_MAIL_HOST={{DATA_CONTRACT_MANAGER_mail_host}}
      - SPRING_MAIL_PORT={{DATA_CONTRACT_MANAGER_mail_port}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DATA_CONTRACT_MANAGER_volume_map_data %}      
      - {{DATA_CONTRACT_CLI_home_dir}}:/home/datacontract
    {% endif -%}   {#  DATA_CONTRACT_MANAGER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  data-contract-manager-postgresql:
    image: pgvector/pgvector:pg16
    container_name: data-contract-manager-postgresql
    hostname: data-contract-manager-postgresql
    labels:
      com.platys.name: "postgresql"
      com.platys.description: "Data Contract Manager Postgresql"
      com.platys.password.envvars: "PLATYS_DATA_CONTRACT_MANAGER_POSTGRESQL_PASSWORD"    
    environment:
      - POSTGRES_DB=data_contract_mgr_db
      - POSTGRES_USER={{DATA_CONTRACT_MANAGER_postgres_username}}
      - POSTGRES_PASSWORD=${PLATYS_DATA_CONTRACT_MANAGER_POSTGRESQL_PASSWORD:-{{DATA_CONTRACT_MANAGER_postgres_password}}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer    
    {% if DATA_CONTRACT_MANAGER_volume_map_data %}      
      - ./container-volume/data-contract-manager/postgresql-data:/var/lib/postgresql/data
    {% endif -%}   {#  DATA_CONTRACT_MANAGER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DATA_CONTRACT_MANAGER_enable #}

{% if DATA_CONTRACT_CLI_enable | default(false) %}
  #  ================================== AsyncAPI Studio ================= #
  data-contract-cli:
    image: datacontract/cli:{{__DATA_CONTRACT_CLI_version}}
    container_name: data-contract-cli
    hostname: data-contract-cli
    labels:
      com.platys.name: "data-contract-cli"
      com.platys.description: "Data Contract tooling"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - {{DATA_CONTRACT_CLI_home_dir}}:/home/datacontract
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint:
      - /bin/sh
      - -c
      - |
        while [ 1 -eq 1 ];do sleep 60;done    
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  DATA_CONTRACT_CLI_enable #}

{% if MICROCKS_enable | default(false) %}
  #  ================================== Microck ================= #
  microcks:
    image: quay.io/microcks/microcks:{{__MICROCKS_version}}
    container_name: microcks
    hostname: microcks
    labels:
      com.platys.name: "microcks"
      com.platys.description: "API Mocking and Testing"
      com.platys.webui.title: "Microcks UI"
      com.platys.webui.url: "http://dataplatform:28203"
    ports:
      - 28203:8080
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATA_MONGODB_URI=mongodb://mongo:27017
      - SPRING_DATA_MONGODB_DATABASE=microcks
      - POSTMAN_RUNNER_URL=http://postman:3000
      - TEST_CALLBACK_URL=http://microcks:8080
      - SERVICES_UPDATE_INTERVAL=0 0 0/2 * * *
      - KEYCLOAK_URL=http://keycloak:8080/auth
      - KEYCLOAK_PUBLIC_URL=http://localhost:18080/auth
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  MICROCKS_enable #}

{% if OTEL_COLLECTOR_enable | default(false) %}
  #  ======================= OpenTelemetry Collector ================= #
  otel-collector:
    image: otel/opentelemetry-collector-contrib-dev:{{__OTEL_COLLECTOR_version}}
    container_name: otel-collector
    hostname: otel-collector
    labels:
      com.platys.name: "otel-collector"
      com.platys.description: "vendor-agnostic implementation on how to receive, process and export telemetry data"
    ports:
      - "4317:4317"        # OTLP gRPC receiver
      - "4318:4318"        # OTLP gRPC receiver
#      - "1888:1888"   # pprof extension
      - "28261:8888"   # Prometheus metrics exposed by the collector
      - "28262:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
#      - "55670:55679" # zpages extension
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./{{"custom-" if OTEL_COLLECTOR_use_custom_conf}}conf/otel-collector/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ["--config=/etc/otel-collector-config.yaml"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {# OTEL_COLLECTOR_enable #}

{% if ZIPKIN_enable | default(false) %}
  #  ================================== Zipkin ========================================== #
  zipkin:
    {%if ZIPKIN_collect_kafka and KAFKA_enable | default(false) %}
    image: ghcr.io/openzipkin/zipkin:{{__ZIPKIN_version}}
    {% else %}
    image: ghcr.io/openzipkin/zipkin-slim:{{__ZIPKIN_version}}
    {% endif -%}   {#  KAFKA_enable #}
    container_name: zipkin
    hostname: zipkin
    labels:
      com.platys.name: "zipkin"
      com.platys.description: "Distributed tracing system"
      com.platys.webui.title: "Zipkin UI"
      com.platys.webui.url: "http://dataplatform:9411"
      com.platys.restapi.title: "Zipkin API"
      com.platys.restapi.url: "http://dataplatform:9411/api/v2/spans"
    ports:
      - 9411:9411
    environment:
      STORAGE_TYPE: {{ZIPKIN_storage_type | lower}}
    {%if (ZIPKIN_storage_type | lower) == 'mysql' | default(false) %}
      MYSQL_HOST: mysql
    {% endif -%}   {#  ZIPKIN_storage_type #}
    {%if (ZIPKIN_storage_type | lower) == 'elasticsearch' | default(false) %}
      ES_HOST: elasticsearch
    {% endif -%}   {#  ZIPKIN_storage_type #}
    {%if (ZIPKIN_storage_type | lower) == 'cassandra3' | default(false) %}
      CASSANDRA_ENSURE_SCHEMA: false
      CASSANDRA_CONTACT_POINTS: cassandra
      # CASSANDRA_USERNAME: cassandra
      # CASSANDRA_PASSWORD: cassandra
    {% endif -%}   {#  ZIPKIN_storage_type #}
    {%if ZIPKIN_collect_kafka and KAFKA_enable | default(false) %}
      KAFKA_BOOTSTRAP_SERVERS: "{{ns.bootstrapServers}}"
    {% endif -%}   {#  KAFKA_enable #}
      #SELF_TRACING_ENABLED: true
      #JAVA_OPTS: '-Xms128m -Xmx128m -XX:+ExitOnOutOfMemoryError'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if ZIPKIN_debug | default(false) %}
    command: --logging.level.zipkin2=DEBUG
    {% endif -%}   {#  ZIPKIN_debug #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  zipkin-dependencies:
    image: ghcr.io/openzipkin/zipkin-dependencies
    container_name: zipkin-dependencies
    hostname: zipkin-dependencies
    labels:
      com.platys.name: "zipkin-dependencies"
      com.platys.description: "Distributed tracing system"
      com.platys.webui.title: "Zipkin Dependencies UI"
      com.platys.webui.url: "http://dataplatform:9411/dependency"
    environment:
      STORAGE_TYPE: {{ZIPKIN_storage_type | lower}}
    {%if (ZIPKIN_storage_type | lower) == 'mysql' | default(false) %}
      MYSQL_HOST: mysql
    {% endif -%}   {#  ZIPKIN_storage_type #}
    {%if (ZIPKIN_storage_type | lower) == 'elasticsearch' | default(false) %}
      ES_HOST: elasticsearch
    {% endif -%}   {#  ZIPKIN_storage_type #}
    {%if (ZIPKIN_storage_type | lower) == 'cassandra3' | default(false) %}
      CASSANDRA_ENSURE_SCHEMA: false
      CASSANDRA_CONTACT_POINTS: cassandra
      # CASSANDRA_USERNAME: cassandra
      # CASSANDRA_PASSWORD: cassandra
    {% endif -%}   {#  ZIPKIN_storage_type #}
    {%if ZIPKIN_debug | default(false) %}
      ZIPKIN_LOG_LEVEL: DEBUG
    {% endif -%}   {#  ZIPKIN_debug #}
      # Uncomment to adjust memory used by the dependencies job
      JAVA_OPTS: '-verbose:gc -Xms1G -Xmx1G'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: crond -f
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

{% endif %}   {# ZIPKIN_enable #}

{% if JAEGER_enable | default(false) %}
  #  ================================== Jaeger ========================================== #
  jaeger:
    image: jaegertracing/all-in-one:{{__JAEGER_version}}
    container_name: jaeger
    hostname: jaeger
    labels:
      com.platys.name: "jaeger"
      com.platys.description: "Distributed tracing system"
      com.platys.webui.title: "Jaeger UI"
      com.platys.webui.url: "http://dataplatform:16686"
      com.platys.restapi.title: "Jaeger Zipkin API"
      com.platys.restapi.url: "http://dataplatform:{{JAEGER_zipkin_port}}/api/v2/spans"
    ports:
      - 16686:16686
      - 14271:14271
      - 5778:5778
      - 14250:14250
      - 6831:6831/udp
      - 6832:6832/udp
      - {{JAEGER_zipkin_port}}:{{JAEGER_zipkin_port}}
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: ":{{JAEGER_zipkin_port}}"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# JAEGER_enable #}

{% if PITCHFORK_enable | default(false) %}
  #  =================== Pitchfork ================= #
  pitchfork:
    image: hotelsdotcom/pitchfork:{{__PITCHFORK_version}}
    container_name: pitchfork
    hostname: pitchfork
    labels:
      com.platys.name: "pitchfork"
      com.platys.description: "Convert tracing data between Zipkin and Haystack formats"
      com.platys.restapi.title: "Pitchfork API"
      com.platys.restapi.url: "http://dataplatform:{{PITCHFORK_server_port}}/api/v2/spans"
    ports:
      - {{PITCHFORK_server_port}}:{{PITCHFORK_server_port}}
    environment:
      SERVER_PORT: {{PITCHFORK_server_port}}
      PITCHFORK_FORWARDERS_LOGGING_ENABLED: "{{PITCHFORK_use_logging}}"
      PITCHFORK_FORWARDERS_LOGGING_LOG_FULL_SPAN: "true"
      PITCHFORK_FORWARDERS_ZIPKIN_HTTP_ENABLED: "{{PITCHFORK_use_zipkin_http}}"
      PITCHFORK_FORWARDERS_ZIPKIN_HTTP_ENDPOINT: "http://zipkin:9411/api/v2/spans"
      PITCHFORK_FORWARDERS_HAYSTACK_KAFKA_ENABLED: "{{PITCHFORK_use_haystack_kafka}}"
      PITCHFORK_FORWARDERS_HAYSTACK_KAFKA_BOOTSTRAP_SERVERS: "{{ns.bootstrapServers}}"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  PITCHFORK_enable #}


{% if PROMTAIL_enable | default(false) %}
  #  ================================== Promtail ========================================== #
  promtail:
    image: grafana/promtail:{{__PROMTAIL_version}}
    hostname: promtail
    container_name: promtail
    labels:
      com.platys.name: "promtail"
      com.platys.description: ""
    volumes:
      - ./data-transfer:/data-transfer
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./conf/promtail/promtail-config.yaml:/etc/promtail/config.yaml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: -config.file=/etc/promtail/config.yaml
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {#  PROMTAIL_enable  #}

{% if LOKI_enable | default(false) %}
  #  ================================== Grafana ========================================== #
  loki:
    image: grafana/loki:{{__LOKI_version}}
    hostname: loki
    container_name: loki
    labels:
      com.platys.name: "loki"
      com.platys.decription: "Log-File Server"
      com.platys.restapi.title: "Loki REST API"
      com.platys.restapi.url: "http://dataplatform:3100/loki/api/v1/status/buildinfo"
    expose:
      - 3100
    ports:
      - "3100:3100"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: -config.file=/etc/loki/local-config.yaml
    restart: {{container_restart_policy}}
{% endif %}   {#  LOKI #}

{% if TEMPO_enable | default(false) %}
  #  ================================== Tempo ========================================== #
  tempo:
    image: grafana/tempo:{{__TEMPO_version}}
    hostname: tempo
    container_name: tempo
    labels:
      com.platys.name: "tempo"
      com.platys.description: "Distributed tracing backend"
    ports:
      - "3200:3200" # tempo
      - "14268"     # jaeger ingest
      - "55680"     # otlp grpc
      - "55681"     # otlp http
      - "9411"      # zipkin
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./{{"custom-" if TEMPO_use_custom_conf}}conf/tempo/tempo.yaml:/etc/tempo.yaml
      - ./{{"custom-" if TEMPO_use_custom_conf}}conf/tempo/overrides.yaml:/etc/overrides.yaml
    {% if TEMPO_volume_map_data %}
      - ./container-volume/tempo/data:/tmp/tempo
    {% endif -%}   {#  TEMPO_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: ["--config.file=/etc/tempo.yaml", "--search.enabled=true"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  {% if TEMPO_with_tempo_query | default(false) %}
  tempo-query:
    image: grafana/tempo-query:{{__TEMPO_version}}
    hostname: tempo-query
    container_name: tempo-query
    labels:
      com.platys.name: "tempo"
      com.platys.description: "Distributed tracing backend"
      com.platys.webui.title: "Tempo Jaeger Query UI"
      com.platys.webui.url: "http://dataplatform:28266"
    ports:
      - "28266:16686"  # jaeger-ui
    environment:
      BACKEND: tempo:80
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
#      - ./conf/tempo/tempo-query.yaml:/etc/tempo-query.yaml
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    logging:
      driver: loki
      options:
        loki-url: 'http://host.docker.internal:3100/loki/api/v1/push'
    #command: ["--grpc-storage-plugin.configuration-file=/etc/tempo-query.yaml"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
  {% endif %}   {#  TEMPO_with_tempo_query #}
{% endif %}   {#  TEMPO_enable #}

{% if GRAFANA_enable | default(false) %}
  #  ================================== Grafana ========================================== #
  grafana:
    image: grafana/grafana:{{__GRAFANA_version}}
    hostname: grafana
    container_name: grafana
    labels:
      com.platys.name: "grafana"
      com.platys.description: "Data visualization and dashboarding"
      com.platys.webui.title: "Grafana UI"
      com.platys.webui.url: "http://dataplatform:3000"
      com.platys.restapi.title: "Grafana API"
      com.platys.restapi.url: "http://dataplatform:3000/api/org"
    expose:
      - 3000
    ports:
      - "3000:3000"
    environment:
      - GF_SERVER_HTTP_ADDR=0.0.0.0
      - GF_SERVER_HTTP_PORT=3000
      - GF_INSTALL_PLUGINS={{GRAFANA_install_plugins if GRAFANA_install_plugins is defined and GRAFANA_install_plugins and GRAFANA_install_plugins | length else omit}}
      - GF_AUTH_ANONYMOUS_ENABLED={{GRAFANA_auth_anonymous_enabled}}
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD={{admin_password}}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_FEATURE_TOGGLES_ENABLE={{GRAFANA_feature_toggles if GRAFANA_feature_toggles is defined and GRAFANA_feature_toggles and GRAFANA_feature_toggles | length else omit}}
    {% if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/grafana/dashboards/dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml
      - ./scripts/grafana/dashboards/docker/monitor_services.json:/etc/grafana/provisioning/dashboards/docker/monitor_services.json
    {% if CADVISOR_enable | default(false) %}
      - ./scripts/grafana/dashboards/docker/docker_host.json:/etc/grafana/provisioning/dashboards/docker/docker_host.json
      - ./scripts/grafana/dashboards/docker/docker_containers.json:/etc/grafana/provisioning/dashboards/docker/docker_containers.json
    {% endif -%}   {#  (CADVISOR_enable #}
    {% if (KAFKA_enable or external['KAFKA_enable']) | default(false) %}
      - ./scripts/grafana/dashboards/kafka/confluent-platform{{'-zkless' if KAFKA_use_kraft_mode}}.json:/etc/grafana/provisioning/dashboards/kafka/confluent-platform{{'-zkless' if KAFKA_use_kraft_mode}}.json
      - ./scripts/grafana/dashboards/kafka/kafka-cluster.json:/etc/grafana/provisioning/dashboards/kafka/kafka-cluster.json
      - ./scripts/grafana/dashboards/kafka/kraft.json:/etc/grafana/provisioning/dashboards/kafka/kraft.json
      - ./scripts/grafana/dashboards/kafka/cluster-linking.json:/etc/grafana/provisioning/dashboards/kafka/cluster-linking.json
      - ./scripts/grafana/dashboards/kafka-transaction-coordinator.json:/etc/grafana/provisioning/dashboards/kafka/kafka-transaction-coordinator.json
      - ./scripts/grafana/dashboards/kafka/kafka-connect-cluster.json:/etc/grafana/provisioning/dashboards/kafka/kafka-connect-cluster.json
      - ./scripts/grafana/dashboards/kafka/confluent-oracle-cdc.json:/etc/grafana/provisioning/dashboards/kafka/confluent-oracle-cdc.json
      - ./scripts/grafana/dashboards/kafka/ksqldb-cluster.json:/etc/grafana/provisioning/dashboards/kafka/ksqldb-cluster.json
      - ./scripts/grafana/dashboards/kafka/schema-registry-cluster.json:/etc/grafana/provisioning/dashboards/kafka/schema-registry-cluster.json
      {% if KAFKA_RESTPROXY_enable | default(false) %}
      - ./scripts/grafana/dashboards/kafka/rest-proxy.json:/etc/grafana/provisioning/dashboards/kafka/rest-proxy.json
      {% endif -%}   {# KAFKA_RESTPROXY_enable #}
      - ./scripts/grafana/dashboards/kafka/kafka-topics.json:/etc/grafana/provisioning/dashboards/kafka/kafka-topics.json
      - ./scripts/grafana/dashboards/kafka/kafka-lag-exporter.json:/etc/grafana/provisioning/dashboards/kafka/kafka-lag-exporter.json
      - ./scripts/grafana/dashboards/kafka/kafka-consumer.json:/etc/grafana/provisioning/dashboards/kafka/kafka-consumer.json
      - ./scripts/grafana/dashboards/kafka/kafka-producer.json:/etc/grafana/provisioning/dashboards/kafka/kafka-producer.json
      - ./scripts/grafana/dashboards/kafka/kafka-quotas.json:/etc/grafana/provisioning/dashboards/kafka/kafka-quotas.json
      - ./scripts/grafana/dashboards/kafka/kafka-streams.json:/etc/grafana/provisioning/dashboards/kafka/kafka-streams.json
      - ./scripts/grafana/dashboards/kafka/kafka-streams-rocksdb.json:/etc/grafana/provisioning/dashboards/kafka/kafka-streams-rocksdb.json
    {% endif -%}   {#  (KAFKA_enable or external['KAFKA_enable']) #}
    {% if NIFI_enable | default(false) %}
      - ./scripts/grafana/dashboards/nifi:/etc/grafana/provisioning/dashboards/nifi
    {% endif -%}   {#  NIFI_enable #}
    {% if MINIO_enable | default(false) %}
      - ./scripts/grafana/dashboards/minio:/etc/grafana/provisioning/dashboards/minio
    {% endif -%}   {#  MINIO_enable #}
    {% if RISINGWAVE_enable | default(false) %}
      - ./scripts/grafana/dashboards/risingwave:/etc/grafana/provisioning/dashboards/risingwave
    {% endif -%}   {#  RISINGWAVE_enable #}
    {% if PROMETHEUS_enable | default(false) %}
      - ./scripts/grafana/datasources/prometheus-datasource.yml:/etc/grafana/provisioning/datasources/prometheus-datasource.yml
    {% endif -%}   {#  PROMETHEUS_enable #}
    {% if INFLUXDB_enable or INFLUXDB2_enable | default(false) %}
      - ./scripts/grafana/datasources/influx-datasource.yml:/etc/grafana/provisioning/datasources/influx-datasource.yml
    {% endif -%}   {#  INFLUXDB_enable or INFLUXDB2_enable #}
    {% if TEMPO_enable | default(false) %}
      - ./scripts/grafana/datasources/tempo-datasource.yml:/etc/grafana/provisioning/datasources/tempo-datasource.yml
    {% endif -%}   {#  TEMPO_enable #}
    {% if LOKI_enable | default(false) %}
      - ./scripts/grafana/datasources/loki-datasource.yml:/etc/grafana/provisioning/datasources/loki-datasource.yml
    {% endif -%}   {#  LOKI_enable #}
    {% if GRAFANA_install_plugins is defined and GRAFANA_install_plugins %}
      - ./plugins/grafana/plugins:/var/lib/grafana/plugins
    {% endif -%}   {#  GRAFANA_install_plugins #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/3000; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
{% endif %}   {#  GRAFANA_enable #}

{% if ETCD_enable | default(false) %}
  #  ================================== etcd ========================================== #
  etcd-1:
    image: quay.io/coreos/etcd:{{__ETCD_version}}
    container_name: etcd-1
    hostname: etcd-1
    labels:
      com.platys.name: "etcd"
      com.platys.description: "Distributed reliable key-value store"
    expose:
      - "2379"
    ports:
      - "2379:2379"
      - "2380:2380"
      - "2381:2381"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    entrypoint: /usr/local/bin/etcd
    command:
            - '--name=etcd-1'
            - '--initial-advertise-peer-urls=http://etcd-1:2380'
            - '--listen-peer-urls=http://0.0.0.0:2380'
            - '--listen-client-urls=http://0.0.0.0:2379'
            - '--advertise-client-urls=http://etcd-1:2379'
            - '--listen-metrics-urls=http://0.0.0.0:2381'
            - '--max-txn-ops=999999'
            - '--max-request-bytes=10485760'
            - '--auto-compaction-mode=periodic'
            - '--auto-compaction-retention=1m'
            - '--snapshot-count=10000'
            - '--data-dir=/etcd-data'
#            - '--initial-cluster=etcd-1=http://etcd-1:2380,etcd-2=http://etcd-2:2380,etcd-3=http://etcd-3:2380'
#            - '--initial-cluster-state=new'
#            - '--initial-cluster-token=mys3cr3ttok3n'
#            - '--heartbeat-interval=250'
#            - '--election-timeout=1250'
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
    healthcheck:
      test:
        - CMD
        - etcdctl
        - --endpoints=http://localhost:2379
        - endpoint
        - health
      interval: 1s
      timeout: 5s
      retries: 5
{% endif %}   {# ETCD_enable #}

{% if ETCD_BROWSER_enable | default(false) %}
  #  ================================== etcd-browser ========================================== #
  etcd-browser:
    image: rustyx/etcdv3-browser:{{__ETCD_BROWSER_version}}
    hostname: etcd-browser
    container_name: etcd-browser
    labels:
      com.platys.name: "etcd-browser"
      com.platys.description: "Distributed reliable key-value store"
      com.platys.webui.title: "etcd UI"
      com.platys.webui.url: "http://dataplatform:28310"
    ports:
      - "28310:8081"
    depends_on:
      - etcd-1
    environment:
      - HTTP_PORT=8081
      - ETCD=etcd-1:2379
      - EDITABLE={{1 if ETCD_BROWSER_allow_edit else omit}}
      - CORS=http://localhost:28310,http://localhost:8081,http://localhost:8080,http://${PUBLIC_IP}:28310,http://${DOCKER_HOST_IP}:28310
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# ETCD_E3W_enable #}

{% if PORTAINER_enable | default(false) %}
  #  ================================== Portainer ========================================== #
  portainer:
    image: portainer/portainer-ce:{{__PORTAINER_version}}
    container_name: portainer
    hostname: portainer
    labels:
      com.platys.name: "portainer"
      com.platys.description: "Easy Docker and Kubernetes management"
      com.platys.webui.title: "Portainer UI"
      com.platys.webui.url: "http://dataplatform:28137"
    ports:
      - 28137:9000
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: --admin-password '$$2y$$05$$Qf7g15yr4ohg8UyxBkVR1OPXVigCcldXoCxS.HgR9xzSALeiv4H.K'
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# PORTAINER_enable #}

{% if DOCKER_EXEC_WEBCONSOLE_enable | default(false) %}
  #  ================================== Portainer ========================================== #
  docker-exec-webconsole:
    image: bitbull/docker-exec-web-console:{{__DOCKER_EXEC_WEBCONSOLE_version}}
    container_name: docker-exec-webconsole
    hostname: docker-exec-webconsole
    labels:
      com.platys.name: "docker-exec-webconsole"
      com.platys.description: "Docker Exec Web Console"
      com.platys.webui.title: "Docker Exec Web Console"
      com.platys.webui.url: "http://dataplatform:28375{{DOCKER_EXEC_WEBCONSOLE_context_path if DOCKER_EXEC_WEBCONSOLE_context_path is defined and DOCKER_EXEC_WEBCONSOLE_context_path and DOCKER_EXEC_WEBCONSOLE_context_path | length}}"
    ports:
      - 28375:8888
    environment:
      DUMMY: 1
      CONTEXT_PATH: {{DOCKER_EXEC_WEBCONSOLE_context_path if DOCKER_EXEC_WEBCONSOLE_context_path is defined and DOCKER_EXEC_WEBCONSOLE_context_path and DOCKER_EXEC_WEBCONSOLE_context_path | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# DOCKER_EXEC_WEBCONSOLE_enable #}

{% if RANCHER_enable | default(false) %}
  #  ================================== Rancher ========================================== #
  rancher:
    image: rancher/rancher:{{__RANCHER_version}}
    container_name: rancher
    hostname: rancher
    labels:
      com.platys.name: "rancher"
      com.platys.description: "Kubernetes cluster management"
      com.platys.webui.title: "Rancher UI"
      com.platys.webui.url: "https://dataplatform:28371"
    ports:
      - 28370:80
      - 28371:443
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if RANCHER_volume_map_data %}
      - ./container-volume/rancher/data:/var/lib/rancher
    {% endif -%}   {#  RANCHER_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    privileged: true
    restart: {{container_restart_policy}}
{% endif %}   {# RANCHER_enable #}

{% if CETUSGUARD_enable | default(false) %}
  #  ================================== Cetusguard ========================================== #
  cetusguard:
    image: docker.io/hectorm/cetusguard:{{__CETUSGUARD_version}}
    container_name: cetusguard
    hostname: cetusguard
    labels:
      com.platys.name: "cetusguard"
      com.platys.description: "protects the Docker daemon socket by filtering calls"
      com.platys.restapi.title: "Cetusguard Docker Socket Proxy URL"
      com.platys.restapi.url: "tcp://cetusguard:2375"
    environment:
      CETUSGUARD_BACKEND_ADDR: "{{CETUSGUARD_docker_daemon_socket if CETUSGUARD_docker_daemon_socket is defined and CETUSGUARD_docker_daemon_socket and CETUSGUARD_docker_daemon_socket | length else 'unix:///var/run/docker.sock'}}"
      CETUSGUARD_FRONTEND_ADDR: "tcp://:{{CETUSGUARD_port if CETUSGUARD_port is defined and CETUSGUARD_port else omit}}"
      CETUSGUARD_RULES_FILE: {{CETUSGUARD_rules_file if CETUSGUARD_rules_file is defined and CETUSGUARD_rules_file and CETUSGUARD_rules_file | length else omit}}
    {% if CETUSGUARD_rules is defined and CETUSGUARD_rules and CETUSGUARD_rules | length %}
      CETUSGUARD_RULES: |
      {% for rule in CETUSGUARD_rules.split(",") %}
        {{rule}}
      {% endfor %}
    {% endif -%}   {#  CETUSGUARD_rules #}
      CETUSGUARD_LOG_LEVEL: "{{CETUSGUARD_log_level if CETUSGUARD_log_level is defined and CETUSGUARD_log_level else omit}}"
      CETUSGUARD_NO_BUILTIN_RULES: {{'true' if CETUSGUARD_no_builtin_rules else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    read_only: true
    restart: {{container_restart_policy}}
{% endif %}   {# CETUSGUARD_enable #}

{% if NGROK_enable | default(false) %}
  #  ================================== ngrok ========================================== #
  ngrok:
    image: ngrok/ngrok:{{__NGROK_version}}
    container_name: ngrok
    hostname: ngrok
    labels:
      com.platys.name: "ngrok"
      com.platys.description: "Globally distributed reverse proxy"
      com.platys.webui.title: "ngrok UI"
      com.platys.webui.url: "https://dataplatform:28247"
      com.platys.password.envvars: "PLATYS_NGROK_AUTH_TOKEN"
    ports:
      - 28247:4040
    environment:
      NGROK_PORTS: {{NGROK_expose_ports}}
      NGROK_AUTHTOKEN: ${PLATYS_NGROK_AUTH_TOKEN:-{{NGROK_auth_token}}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    {% if NGROK_expose_ports is defined and NGROK_expose_ports and NGROK_expose_ports | length %}
    entrypoint:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        printf "version: 2\n\n" > /tmp/ngrok.yml
        printf "tunnels:\n" >> /tmp/ngrok.yml
        for port in $$(echo {{NGROK_expose_ports}} | sed "s/,/ /g")
        do
          printf "  app-$${port}:\n    proto: http\n    addr: host.docker.internal:%s\n" $$port >> /tmp/ngrok.yml;
        done
        cat /tmp/ngrok.yml
        ngrok start --config /tmp/ngrok.yml --all
    {% endif -%}   {#  NGROK_expose_ports #}
    restart: {{container_restart_policy}}
{% endif %}   {# NGROK_enable #}

{% if CADVISOR_enable | default(false) %}
  #  ================================== cAdvisor ========================================== #
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:{{__CADVISOR_version}}
    container_name: cadvisor
    hostname: cadvisor
    labels:
      com.platys.name: "cadvisor"
      com.platys.description: "Analyzes resource usage and performance of containers"
      com.platys.webui.title: "cAdvisor UI"
      com.platys.webui.url: "http://dataplatform:28138"
    ports:
      - 28138:8080
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    devices:
      - "/dev/kmsg"
    tty: true
    privileged: true
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# CADVISOR_enable #}

{% if GLANCES_enable | default(false) %}
  #  ================================== Glances ========================================== #
  glances:
    image: nicolargo/glances:{{__GLANCES_version}}
    container_name: glances
    hostname: glances
    labels:
      com.platys.name: "glances"
      com.platys.description: "A top/htop alternative"
      com.platys.webui.title: "Glances UI"
      com.platys.webui.url: "http://dataplatform:28138"
    ports:
      - 28138-28139:61208-61209
    environment:
      GLANCES_OPT: "-C /glances/conf/glances.conf -w"
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/run/user/1000/podman/podman.sock:/run/user/1000/podman/podman.sock:ro"
      - "./conf/glances/glances.conf:/glances/conf/glances.conf"      
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    pid: "host"
    privileged: true
    # network_mode: "host" 
    {% if GLANCES_with_gpu | default(false) %}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu] 
    {% endif -%}   {#  GLANCES_with_gpu #}                    
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# GLANCES_enable #}

{% if DOCKER_REGISTRY_enable | default(false) %}
  #  ================================== Docker Registry ========================================== #
  docker-registry:
    image: registry:{{__DOCKER_REGISTRY_version}}
    container_name: docker-registry
    hostname: docker-registry
    labels:
      com.platys.name: "docker-registry"
      com.platys.description: "Docker Registry"
      com.platys.restapi.title: 'Docker Registry REST API'
      com.platys.restapi.url: "http://dataplatform:5020/v2"
    ports:
      - 5020:5000
    environment:
      REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /var/lib/registry
      REGISTRY_HTTP_ADDR: 0.0.0.0:5000
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin: '["http://${PUBLIC_IP}:28307"]'
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods: '[HEAD,GET,OPTIONS,DELETE]'
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials: '[true]'
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers: '[Authorization,Accept,Cache-Control]'
      REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers: '[Docker-Content-Digest]'
      REGISTRY_STORAGE_DELETE_ENABLED: 'true'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if DOCKER_REGISTRY_volume_map_custom_config %}
      - ./custom-conf/docker-registry/config.yml:/etc/docker/registry/config.yml
    {% endif -%}   {#  DOCKER_REGISTRY_volume_map_custom_config #}
    {% if DOCKER_REGISTRY_volume_map_data %}
      - ./container-volume/docker-registry/data:/var/lib/registry
    {% endif -%}   {#  DOCKER_REGISTRY_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  docker-registry-ui:
    image: joxit/docker-registry-ui:{{__DOCKER_REGISTRY_UI_version}}
    container_name: docker-registry-ui
    hostname: docker-registry-ui
    labels:
      com.platys.name: "docker-registry-ui"
      com.platys.description: "Docker Registry"
      com.platys.webui.title: "Docker Registry UI"
      com.platys.webui.url: "http://dataplatform:28307"
    ports:
      - 28307:80
    depends_on:
     - docker-registry
    environment:
      SINGLE_REGISTRY: true
      REGISTRY_TITLE: "Private Docker Registry"
      REGISTRY_URL: http://${PUBLIC_IP}:5020
      DELETE_IMAGES: true
      SHOW_CONTENT_DIGEST: true
      REGISTRY_SECURED: false
      CATALOG_ELEMENTS_LIMIT: 1000
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# DOCKER_REGISTRY_enable #}

{% if GITWEB_enable | default(false) %}
  #  ================================== GitWeb ========================================== #
  gitweb:
    image: rockstorm/gitweb:{{__GITWEB_version}}
    container_name: gitweb
    hostname: gitweb
    labels:
      com.platys.name: "gitweb"
      com.platys.description: "Visualize and serve git repositories over HTTP "
      com.platys.webui.title: "GitWeb UI"
      com.platys.webui.url: "http://dataplatform:28250"
    ports:
      - "28250:80"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - {{GITWEB_path_to_repo}}:/srv/git:ro
    {% if GITWEB_volume_map_custom_config %}
      - ./custom-conf/gitweb/gitweb.conf:/etc/gitweb.conf:ro
    {% endif %}
    {% if GITWEB_volume_map_theme %}
      - ./plugins/gitweb/theme:/usr/share/gitweb/theme:ro
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# GITWEB_enable #}

{% if HAWTIO_enable | default(false) %}
  #  ================================== Hawtio ========================================== #
  hawtio:
    image: erikwramner/hawtio:{{__HAWTIO_version}}
    container_name: hawtio
    hostname: hawtio
    labels:
      com.platys.name: "hawtio"
      com.platys.description: "Lightweight and modular Web console for managing your Java stuff"
      com.platys.webui.title: "Hawtio UI"
      com.platys.webui.url: "http://dataplatform:28155"
    ports:
      - "28155:8080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# HAWTIO_enable #}

{% if SPRING_BOOT_ADMIN_enable | default(false) %}
  #  ================================== Spring Boot Admin Server ========================================== #
  spring-boot-admin:
    image: codecentric/spring-boot-admin:{{__SPRING_BOOT_ADMIN_version}}
    container_name: spring-boot-admin
    hostname: spring-boot-admin
    labels:
      com.platys.name: "spring-boot-admin"
      com.platys.description: "SpringBoot Admin UI"
      com.platys.webui.title: "Spring Boot Admin UI"
      com.platys.webui.url: "http://dataplatform:28293"
    ports:
      - "28293:8080"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# SPRING_BOOT_ADMIN_enable #}

{% if WETTY_enable | default(false) %}
  #  ================================== Wetty ========================================== #
  {% set externalPort = 3001 + dataCenterId %}
  wetty{{dataCenter}}:
    image: wettyoss/wetty:{{__WETTY_version}}
    container_name: wetty{{dataCenter}}
    hostname: wetty{{dataCenter}}
    labels:
      com.platys.name: "wetty"
      com.platys.description: "A terminal window in Web-Browser"
      com.platys.webui.title: "WeTTY UI"
      com.platys.webui.url: "http://dataplatform:{{externalPort}}"
    ports:
      - "{{externalPort}}:3000"
    environment:
      - SSHHOST=${DOCKER_HOST_IP}
      - SSHPORT=22
      - SSHUSER=
      - SSHAUTH=password
      - PORT=3000
      - BASE=/
    {%if use_timezone | default(false) %}
      - TZ='{{use_timezone}}'
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# WETTY_enable #}

{% if PYTHON_enable | default(false) %}
  #  ================================== python ========================================== #
  python-1:
    image: {{PYTHON_image}}:{{__PYTHON_version}}
    container_name: python-1
    hostname: python-1
    labels:
      com.platys.name: "python"
      com.platys.description: "Python script executor"
    environment:
      DUMMY: make-it-valid
    {%if MLFLOW_SERVER_enable | default(false) %}
      MLFLOW_TRACKING_URI: http://mlflow-tracking-server:5000
    {% endif %}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/python/run_pip_and_python.sh:/scripts/run_pip_and_python.sh:ro
    {% if PYTHON_artefacts_folder is defined and PYTHON_artefacts_folder|length %}
      - {{PYTHON_artefacts_folder}}:/app:ro
    {% endif %}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - sh
      - -c
      - |
    {% if PYTHON_python_packages is defined and PYTHON_python_packages|length  %}
        pip install {{PYTHON_python_packages}}
    {% endif -%}   {#  PYTHON_python_packages #}
    {% if PYTHON_requirements_file is defined and PYTHON_requirements_file | length %}
        /scripts/run_pip_and_python.sh --requirement-file /app/{{PYTHON_requirements_file}} {{'--python-script /app/' ~ PYTHON_script_to_run if PYTHON_script_to_run is defined and PYTHON_script_to_run and PYTHON_script_to_run | length}} 
    {% else %}
        /scripts/run_pip_and_python.sh {{'--python-script /app/' ~ PYTHON_script_to_run if PYTHON_script_to_run is defined and PYTHON_script_to_run and PYTHON_script_to_run | length}} 
    {% endif -%}   {#  PYTHON_requirements_file #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# PYTHON_enable #}

{% if RANETO_enable | default(false) %}
  #  ================================== Raneto ========================================== #
  raneto:
    image: lscr.io/linuxserver/raneto:{{__RANETO_version}}
    container_name: raneto
    hostname: raneto
    labels:
      com.platys.name: "raneto"
      com.platys.description: "Markdown powered Knowledgebase Wiki"
      com.platys.webui.title: "Raneto UI"
      com.platys.webui.url: "http://dataplatform:28311"
    ports:
      - "28311:3000"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if RANETO_volume_map_config | default(false) %}
      - ./container-volume/raneto/config:/config
    {% endif -%}   {#  RANETO_volume_map_data #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    restart: {{container_restart_policy}}
{% endif %}   {# RANETO_enable #}

{% if MARKDOWN_MADNESS_enable | default(false) %}
  #  ================================== Markdown Madness Server ========================================== #
  markdown-madness:
    image: {{MARKDOWN_MADNESS_custom_image_name if MARKDOWN_MADNESS_custom_image_name is defined and MARKDOWN_MADNESS_custom_image_name | length else 'dannyben/madness'}}:{{__MARKDOWN_MADNESS_version}}
    container_name: markdown-madness
    hostname: markdown-madness
    labels:
      com.platys.name: "markdown-madness"
      com.platys.description: {{MARKDOWN_MADNESS_description | default("Markdown Viewer") }}
      com.platys.webui.title: "Markdown Madness Server UI"
      com.platys.webui.url: "http://dataplatform:28312"
      com.centurylinklabs.watchtower.enable: {{true if MARKDOWN_MADNESS_watchtower_enable else omit}}
    ports:
      - "28312:3000"
    {%if use_timezone | default(false) %}
    environment:
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {% if MARKDOWN_MADNESS_volume_map_docs | default(false) %}
      - ./container-volume/markdown-madness/docs:/docs
    {% else %}
      - ./markdown:/docs
    {% endif -%}   {#  MARKDOWN_MADNESS_volume_map_docs #}
    {% if MARKDOWN_MADNESS_volume_map_custom_config_file | default(false) %}
      - ./custom-conf/markdown-madness/markdown-madness.yml:/docs/.madness.yml
    {% else %}
      - ./conf/markdown-madness/markdown-madness.yml:/docs/.madness.yml
    {% endif -%}   {#  MARKDOWN_MADNESS_volume_map_config_file #}
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    command: server
    restart: {{container_restart_policy}}
{% endif %}   {# MARKDOWN_MADNESS_enable #}

{% if MARKDOWN_VIEWER_enable | default(false) %}
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer{{dataCenter}}:
  {% if MARKDOWN_VIEWER_edition == 'markdown-madness' | default(false) %}
    image: dannyben/madness:{{__MARKDOWN_MADNESS_version}}
  {% else -%}
    image: trivadis/markdown-web:{{__MARKDOWN_VIEWER_version}}
  {% endif -%}   {# MARKDOWN_VIEWER_use_markdown_madness_impl #}
    container_name: markdown-viewer{{dataCenter}}
    hostname: markdown-viewer{{dataCenter}}
    labels:
      com.platys.name: "markdown-viewer"
      com.platys.description: "Platys Platform homepage viewer"
      com.platys.webui.title: "Markdown Viewer UI"
      com.platys.webui.url: "http://dataplatform:{{80 + (dataCenterId * 1) if (MARKDOWN_VIEWER_use_port_80 | default(true) )  else 8008 }}"
    ports:
      - "{{80 + (dataCenterId * 1) if (MARKDOWN_VIEWER_use_port_80 | default(true) )  else 8008 }}:{{3000 if MARKDOWN_VIEWER_edition == 'markdown-madness' else 80}}"
    volumes:
      - ./artefacts:{{"/docs" if MARKDOWN_VIEWER_edition == 'markdown-madness' else "/home/python/markdown"}}
  {% if MARKDOWN_VIEWER_edition == 'markdown-madness' | default(false) %}
      - ./conf/markdown-viewer/markdown-madness.yml:/docs/.madness.yml
  {% endif -%}   {# MARKDOWN_VIEWER_use_markdown_madness_impl #}
      - ./data-transfer:/data-transfer
  {% if MARKDOWN_VIEWER_edition == 'markdown-madness' | default(false) %}
    command: server
  {% endif -%}   {# MARKDOWN_VIEWER_use_markdown_madness_impl #}
    restart: {{container_restart_policy}}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://markdown-viewer:3000 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 1m

  markdown-renderer{{dataCenter}}:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer{{dataCenter}}
    hostname: markdown-renderer{{dataCenter}}
    labels:
      com.platys.name: "markdown-renderer"
      com.platys.description: "Platys Platform homepage rendering"
    environment:
      USE_PUBLIC_IP: "{{MARKDOWN_VIEWER_use_public_ip | default(true) }}"
      PUBLIC_IP: '${PUBLIC_IP}'
      DOCKER_HOST_IP: '${DOCKER_HOST_IP}'
      DATAPLATFORM_HOME: '${DATAPLATFORM_HOME}'
      PLATYS_PLATFORM_NAME: {{platys['platform-name']}}
      PLATYS_PLATFORM_STACK: {{platys['platform-stack']}}
      PLATYS_PLATFORM_STACK_VERSION: {{platys['platform-stack-version']}}
      PLATYS_COPY_COOKBOOK_DATA: '{{copy_cookbook_data_folder}}'
      SERVICE_LIST_VERSION: {{MARKDOWN_VIEWER_services_list_version}}
  {% if MARKDOWN_MADNESS_enable | default(false) %}
      PLATYS_ADDL_DOCUMENTATION: 'http://${PUBLIC_IP}:28312'
  {% endif -%}   {# MARKDOWN_MADNESS_enable #}
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
    init: true
{% endif %}   {# MARKDOWN_VIEWER_enable #}

{% if LOG4BRAINS_enable | default(false) %}
  #  ================================== markdown-viewer ========================================== #
  log4brains:
    image: {{LOG4BRAINS_repository_name if LOG4BRAINS_repository_name is defined and LOG4BRAINS_repository_name | length else 'thomvaill'}}/{{LOG4BRAINS_image_name | default(log4brains) }}:{{__LOG4BRAINS_version}}
    container_name: log4brains
    hostname: log4brains
    labels:
      com.platys.name: "log4brains"
      com.platys.description: "Log and publish architecture decisions (ADR)"
      com.platys.webui.title: "log4brains UI (ADR Viewer)"
      com.platys.webui.url: "http://dataplatform:4004"
    ports:
      - "4004:4004"
    volumes:
    {% if LOG4BRAINS_adr_source_dir is defined and LOG4BRAINS_adr_source_dir | length %}
      - {{LOG4BRAINS_adr_source_dir}}:/workdir
    {% endif -%}   {# LOG4BRAINS_adr_source_dir is defined #}
      - ./data-transfer:/data-transfer
    command: ["{{LOG4BRAINS_command | default(preview) }}"]
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
{% endif %}   {# LOG4BRAINS_enable #}

{% if WATCHTOWER_enable | default(false) %}
  #  ================================== watchtower ========================================== #
  watchtower:
    image: containrrr/watchtower:{{__WATCHTOWER_version}}
    container_name: watchtower
    hostname: watchtower
    labels:
      com.platys.name: "watchtower"
      com.platys.description: "Automating Docker container base image updates"
    {% if WATCHTOWER_http_api_token | default(false) %}
      com.platys.restapi.title: "Watchtower API"
      com.platys.restapi.url: "http://dataplatform:28314/v1/update"
    ports:
      - 28314:8080
    {% endif -%}   {#  WATCHTOWER_http_api_token #}
    environment:
      - DUMMY=true
      - WATCHTOWER_POLL_INTERVAL={{WATCHTOWER_poll_interval if WATCHTOWER_poll_interval is defined and WATCHTOWER_poll_interval else omit}}
      - WATCHTOWER_SCHEDULE={{WATCHTOWER_schedule if WATCHTOWER_schedule is defined and WATCHTOWER_schedule and WATCHTOWER_schedule | length else omit}}
      - WATCHTOWER_CLEANUP={{true if WATCHTOWER_cleanup_enable else omit}}
      - WATCHTOWER_DEBUG={{true if WATCHTOWER_debug_enable else omit}}
      - WATCHTOWER_TRACE={{true if WATCHTOWER_trace_enable else omit}}
      - WATCHTOWER_MONITOR_ONLY={{true if WATCHTOWER_monitor_only_enable else omit}}
      - WATCHTOWER_NO_RESTART={{true if WATCHTOWER_no_restart_enable else omit}}
      - WATCHTOWER_ROLLING_RESTART={{true if WATCHTOWER_rolling_restart_enable else omit}}
      - WATCHTOWER_LABEL_ENABLE={{true if WATCHTOWER_label_enable else omit}}
      - WATCHTOWER_SCOPE={{WATCHTOWER_scope if WATCHTOWER_scope is defined and WATCHTOWER_scope and WATCHTOWER_scope | length else omit}}
      - WATCHTOWER_HTTP_API_UPDATE={{true if WATCHTOWER_http_api_update_enable else omit}}
      - WATCHTOWER_HTTP_API_TOKEN={{WATCHTOWER_http_api_token if WATCHTOWER_http_api_token is defined and WATCHTOWER_http_api_token and WATCHTOWER_http_api_token | length else omit}}
      - WATCHTOWER_HTTP_API_PERIODIC_POLLS={{true if WATCHTOWER_http_api_period_polls_enable else omit}}
      - WATCHTOWER_HTTP_API_METRICS={{true if WATCHTOWER_http_api_metrics_enable else omit}}
      - WATCHTOWER_TIMEOUT={WATCHTOWER_timeout if WATCHTOWER_timeout is defined and WATCHTOWER_timeout else omit}}
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock
    {%if WATCHTOWER_map_config_json %}
      - ./custom-conf/watchtower/config.json:/config.json
    {% endif -%}
    {%if WATCHTOWER_watch_containers is defined and WATCHTOWER_watch_containers and WATCHTOWER_watch_containers | length %}
    command: {{WATCHTOWER_watch_containers}}
    {% endif -%} {#  WATCHTOWER_watch_containers #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# WATCHTOWER_enable #}

{% if S3FS_enable and (MINIO_enable or external['S3_enable']) | default(false) %}
  #  ================================== S3FS (S3 Bucket mapping) ========================================== #
  s3fs:
    image: efrecon/s3fs:{{__S3FS_version}}
    container_name: s3fs
    hostname: s3fs
    labels:
      com.platys.name: "s3fs"
      com.platys.description: "FUSE-based file system backed by Amazon S3"
    cap_add:
      - SYS_ADMIN
    security_opt:
      - 'apparmor:unconfined'
    devices:
      - /dev/fuse
    environment:
      AWS_S3_BUCKET: '{{S3FS_bucket_name}}'
      AWS_S3_ACCESS_KEY_ID: '{{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}'
      AWS_S3_SECRET_ACCESS_KEY: '{{s3SecretAccessKey}}'
#      AWS_S3_AUTHFILE: '${AWS_S3_AUTHFILE}'
#      AWS_S3_ACCESS_KEY_ID_FILE: '${AWS_S3_ACCESS_KEY_ID_FILE}'
#      AWS_S3_SECRET_ACCESS_KEY_FILE: '${AWS_S3_SECRET_ACCESS_KEY_FILE}'
      AWS_S3_URL: '{{s3Endpoint | default(omit) }}'
      AWS_S3_MOUNT: '/opt/s3fs/bucket'
      S3FS_ARGS: {{'use_path_request_style,' if s3PathStyleAccess == 'true' else omit}}allow_others,default_acl=public-read
      S3FS_DEBUG: 1
      UID: 1000
      GID: 1000
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - './s3-bucket:/opt/s3fs/bucket:rshared'
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# S3FS_enable #}

{% if WEB_PROTEGE_enable | default(false) %}
  #  ================================== Web Protégé ========================================== #
  web-protege:
    image: protegeproject/webprotege:{{__WEB_PROTEGE_version}}
    container_name: web-protege
    hostname: web-protege
    labels:
      com.platys.name: "web-protege"
      com.platys.description: "Ontology Editor"
      com.platys.webui.title: "Web Protégé UI"
      com.platys.webui.url: "http://dataplatform:28368"
    ports:
      - "28368:8080"
    environment:
      - webprotege.mongodb.host=web-protege-mongo
    {%if use_timezone | default(false) %}
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if WEB_PROTEGE_volume_map_data %}
      - ./container-volume/web-protege/data:/srv/webprotege
    {% endif -%}   {#  WEB_PROTEGE_volume_map_data #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}

  web-protege-mongo:
    image: mongo:4.1-bionic
    container_name: web-protege-mongo
    hostname: web-protege-mongo
    labels:
      com.platys.name: "mongo"
      com.platys.description: "MongoDB for Web Protégé"
    {%if use_timezone | default(false) %}
    environment:
      - TZ={{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if WEB_PROTEGE_volume_map_data %}
      - ./container-volume/web-protege/mongodb-data:/data/db
    {% endif -%}   {#  WEB_PROTEGE_volume_map_data #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# WEB_PROTEGE_enable #}

{% if HAPI_FHIR_enable | default(false) %}
  #  ================================== Hapi FHIR Server ========================================== #
  hapi-fhir:
    image: docker.io/hapiproject/hapi:{{__HAPI_FHIR_version}}
    container_name: blaze-fhir
    hostname: blaze-fhir
    labels:
      com.platys.name: "hapi-fhir"
      com.platys.description: "FHIR server"
      com.platys.restapi.title: "HAPI FHIR REST API"
      com.platys.restapi.url: "http://dataplatform:28336/fhir"
    ports:
      - "28336:8080"
    environment:
      DUMMY: false
#      SPRING_CONFIG_LOCATION
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# HAPI_FHIR_enable #}

{% if BLAZE_FHIR_enable | default(false) %}
  #  ================================== Blaze FHIR Server ========================================== #
  blaze-fhir:
    image: samply/blaze:{{__BLAZE_FHIR_version}}
    container_name: blaze-fhir
    hostname: blaze-fhir
    labels:
      com.platys.name: "blaze-fhir"
      com.platys.description: "FHIR server"
      com.platys.webui.title: "Blaze UI"
      com.platys.webui.url: "http://dataplatform:28331"
      com.platys.restapi.title: "Blaze REST API"
      com.platys.restapi.url: "http://dataplatform:28331/fhir"
    ports:
      - "28331:8080"
    environment:
      BASE_URL: 'http://localhost:8080'
      JAVA_TOOL_OPTIONS: '-Xmx2g'
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# BLAZE_FHIR_enable #}

{% if LFH_FHIR_enable | default(false) %}
  #  ================================== LinuxForHealth (IBM) FHIR Server ========================================== #
  lfh-fhir:
    image: ghcr.io/linuxforhealth/fhir-server:{{__LFH_FHIR_version}}
    container_name: lfh-fhir
    hostname: lfh-fhir
    labels:
      com.platys.name: "lfh-fhir"
      com.platys.description: "FHIR server"
      com.platys.restapi.title: "LinuxForHealth FHIR REST API"
      com.platys.restapi.url: "http://dataplatform:9443/fhir-server/api/v4/$healthcheck"
    ports:
      - "9443:9443"
    environment:
      BOOTSTRAP_DB: true
      DISABLED_OPERATIONS: ''
      FHIR_USER_PASSWORD: {{LFH_FHIR_user_password if LFH_FHIR_user_password is defined and LFH_FHIR_user_password and LFH_FHIR_user_password | length else omit}}
      FHIR_ADMIN_PASSWORD: {{LFH_FHIR_admin_password if LFH_FHIR_admin_password is defined and LFH_FHIR_admin_password and LFH_FHIR_admin_password | length else omit}}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# LFH_FHIR_enable #}

{% if FHIR_GATEWAY_enable | default(false) %}
  #  ================================== fhir-gateway ========================================== #
  fhir-gateway:
    image: ghcr.io/miracum/fhir-gateway:{{__FHIR_GATEWAY_version}}
    container_name: fhir-gateway
    hostname: fhir-gateway
    labels:
      com.platys.name: "fhir-gateway"
      com.platys.description: "FHIR Gateway"
      com.platys.restapi.title: "Miracom FHIR Gateway"
      com.platys.restapi.url: "http://dataplatform:28335/fhir"
    ports:
      - "28335:8080"
#    depends_on:
#      - loinc-converter
#      - fhir-db
#      - fhir-pseudonymizer
    cap_drop:
      - ALL
    ipc: none
    security_opt:
      - "no-new-privileges:true"
    read_only: true
    tmpfs:
      - /tmp
    privileged: false
    environment:
    {%if FHIR_GATEWAY_postgresql_enabled | default(false) %}
      SERVICES_PSQL_ENABLED: true
      SPRING_DATASOURCE_URL: jdbc:postgresql://fhir-db:5432/fhir
      SPRING_DATASOURCE_USERNAME: postgres
      # kics-scan ignore-line
      SPRING_DATASOURCE_PASSWORD: abc123!
    {% else %}
      SERVICES_PSQL_ENABLED: false
      SPRING_DATASOURCE_URL:
      SPRING_SQL_INIT_MODE: never
    {% endif -%}   {#  FHIR_GATEWAY_postgresql_enabled #}
      SERVICES_PSEUDONYMIZER_ENABLED: {{FHIR_GATEWAY_pseudonymizer_enabled | default(false) }}
      SERVICES_PSEUDONYMIZER_URL: http://fhir-pseudonymizer:8080/fhir
      SERVICES_LOINC_CONVERSIONS_URL: http://loinc-converter:8080/api/v1
      SERVICES_FHIRSERVER_ENABLED: {{FHIR_GATEWAY_fhir_server_enabled | default(false) }}
      SERVICES_FHIRSERVER_URL: {{FHIR_GATEWAY_fhir_server_urls | default(omit) }}
      SERVICES_FHIRSERVER_AUTH_BASIC_USERNAME: {{FHIR_GATEWAY_fhir_server_username | default(omit) }}
      SERVICES_FHIRSERVER_AUTH_BASIC_PASSWORD: {{FHIR_GATEWAY_fhir_server_password | default(omit) }}
      SERVICES_KAFKA_ENABLED: {{FHIR_GATEWAY_kafka_enabled | default(false) }}
    {%if FHIR_GATEWAY_kafka_enabled | default(false) %}
      BOOTSTRAP_SERVERS: {{ns.bootstrapServers}}
      SERVICES_KAFKA_GENERATE_OUTPUT_TOPIC_MATCH_EXPRESSION:
    {% endif -%}   {#  FHIR_GATEWAY_kafka_enabled #}
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    restart: {{container_restart_policy}}
{% endif %}   {# FHIR_GATEWAY_enable #}

{% if PROVISIONING_DATA_enable | default(false) %}
  #  ================================== data-provisioning ========================================== #
  data-provisioning:
    image: trivadis/platys-modern-data-platform-data:{{__PROVISIONING_DATA_version}}
    container_name: data-provisioning
    hostname: data-provisioning
    labels:
      com.platys.name: "data-provisioning"
      com.platys.description: "Provisioning sample data"
    volumes:
      - ./data-transfer:/data-transfer
    init: true
{% endif %}   {# PROVISIONING_DATA_enable #}


{%if PLATYS_MDP_INIT_enable | default(false) %}
  #  ================================== Platys Modern Data Platform Init Service ========================================== #
  platys-mdp-init:
    image: trivadis/platys-mdp-init:latest
    container_name: platys-mdp-init
    hostname: platys-mdp-init
    {%if PLATYS_MDP_INIT_use_env_file | default(false) %}
    env_file: {{PLATYS_MDP_INIT_env_filename if PLATYS_MDP_INIT_env_filename is defined and PLATYS_MDP_INIT_env_filename and PLATYS_MDP_INIT_env_filename | length else ".env"}}
    {% endif -%}   {#  NIFI2_enable #}   
    environment:
    {% if MINIO_enable | default(false) %}    
      MC_HOST_minio-1: http://{{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}:{{s3SecretAccessKey}}@minio-1:9000    
      MINIO_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      MINIO_SECRET_ACCESS_KEY: {{s3SecretAccessKey}}
      MINIO_URL: {{s3Endpoint}}
    {% endif -%}   {#  NIFI2_enable #}   
    {% if LAKEFS_enable | default(false) %}
      LAKEFS_URL: http://lakefs:8000
      LAKEFS_USER: {{LAKEFS_user_name | default(lakefs) }}
      LAKEFS_CREDENTIALS_ACCESS_KEY_ID: {{s3AccessKey | mandatory('s3AccessKey must be defined, if using minIO use the MINIO_access_key in config.yml') }}
      LAKEFS_CREDENTIALS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-{{MINIO_secret_key}}}
    {% endif -%}   {#  LAKEFS_enable #}   
    {% if NIFI2_enable | default(false) %}
      NIFI2_URL: https://nifi2-1:18083
      NIFI2_VERSION: '2.0.0'
      NIFI2_USERNAME: '{{NIFI2_username}}'
      NIFI2_PASSWORD: '${PLATYS_NIFI2_PASSWORD:-{{NIFI2_password}}}'
    {% endif -%}   {#  NIFI2_enable #}   
    {% if AIRFLOW_enable | default(false) %}
      AIRFLOW_URL: http://airflow:8080
      AIRFLOW_USERNAME: '{{AIRFLOW_admin_username}}'
      AIRFLOW_PASSWORD: '${PLATYS_AIRFLOW_ADMIN_PASSWORD:-{{AIRFLOW_admin_password}}}'
    {% endif -%}   {#  AIRFLOW_enable #}   
    {% if VAULT_enable | default(false) %}
      VAULT_URL: http://vault:8200
      VAULT_USERNAME: '{{VAULT_username}}'
      VAULT_PASSWORD: '${PLATYS_VAULT_PASSWORD:-{{VAULT_password}}}'
    {% endif -%}   {#  VAULT_enable #}   
    {% if PLATYS_MDP_INIT_env_variables is defined and PLATYS_MDP_INIT_env_variables and PLATYS_MDP_INIT_env_variables | length | default(false) %}
      {% for envvar in PLATYS_MDP_INIT_env_variables.split(",") %}
      {{envvar.split('=')[0]}}: "{{envvar.split('=')[1]}}"
      {% endfor %}
    {% endif -%}   {#  PLATYS_MDP_INIT_env_variables #}   
    {%if use_timezone | default(false) %}
      TZ: {{use_timezone}}
    {% endif -%}   {#  use_timezone #}
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/platys-mdp-init:/docker-entrypoint-init.d/
    {%if NIFI2_enable | default(false) %}
      - nifi-config-vol:/opt/nifi/conf/
    {% endif -%}      
    {%if use_timezone | default(false) %}
      - "./etc/timezone:/etc/timezone:ro"
      - "./etc/localtime:/etc/localtime:ro"
    {% endif -%}   {#  use_timezone #}
    {%if logging_driver is defined and logging_driver and logging_driver in ('fluentd','loki','syslog','splunk') | default(false) %}
    <<: *logging
    {% endif -%}   {#  logging_driver is defined ... #}
    init: true
    command: sleep 30
    restart: "no"
{% endif %}   {#  PLATYS_MDP_INIT_enable  #}

volumes:
  data-transfer-vol:
    name: data_transfer_vol
{%if FLINK_enable %}
  flink-vol:
{% endif %}
{%if MINIO_enable or external['S3_enable'] | default(false) %}
  aws-credentials-vol:
{% endif %}
{%if NIFI2_enable | default(false) %}
  nifi-config-vol:
{% endif %}
{%if SPARK_enable %}
  spark-{{__SPARK_version | replace('.','-')}}-vol:
{% endif %}
{%if AIRBYTE_volume_map_data %}
  airbyte-workspace-vol:
    name: airbyte_workspace_vol
  airbyte-db-vol:
    name: airbyte_db_vol
{% endif %}
